% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\title{Functional Data Analysis, Regression, Classification and Clustering using fda.usc package}
\author{Manuel Oviedo de la Fuente}
\date{May 11, 2021}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Functional Data Analysis, Regression, Classification and Clustering using fda.usc package},
  pdfauthor={Manuel Oviedo de la Fuente},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\ifxetex
  \usepackage{polyglossia}
  \setmainlanguage{spanish}
  % Tabla en lugar de cuadro
  \gappto\captionsspanish{\renewcommand{\tablename}{Table}  
          \renewcommand{\listtablename}{Index of tables}}

\else
  \usepackage[spanish,es-tabla]{babel}
\fi
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

This vignette describes the usage of fda.usc in R.
fda.usc package carries out exploratory and descriptive analysis of functional data exploring its most important features such as:

\begin{itemize}
\item
  Functional Data Representation
\item
  Functional Regression
\item
  Functional Classification
\item
  Functional ANOVA
\end{itemize}

The co-author of fda.usc is \emph{Manuel Febrero-Bande}, the contributors are \textbf{Pedro Galeano}, \textbf{Alicia Nieto} and \textbf{Eduardo Garcia-Portugues}.

\hypertarget{cran-task-view}{%
\section*{CRAN Task View}\label{cran-task-view}}
\addcontentsline{toc}{section}{CRAN Task View}

R task view devoted to FDA: \url{https://cran.r-project.org/web/views/FunctionalData.html}

*\textbf{fda.usc} is included in the set of the 7 recommended core packages.

Dependencies:

\begin{itemize}
\item
  Imports, dependent: MASS, fda, mgcv, nlme, rpart (depecrated)
\item
  Reverse depends: GPFDA, ILS, MFHD, NITPicker, qcr
\item
  Reverse imports: classiFunc, PCRedux, mlr
\item
  Versions (by year): 5 (2012), 6 (2013), 2 (2014), 1 (2015), 2 (2016), 1 (2018), 1 (2019)
\end{itemize}

fda.usc package download by month:

\begin{verbatim}
## Warning: package 'ggplot2' was built under R version 4.1.1
\end{verbatim}

\includegraphics{bookdown_intro_files/figure-latex/unnamed-chunk-3-1.pdf}

\hypertarget{functional-data-analysis-in-r}{%
\section*{Functional Data Analysis in R}\label{functional-data-analysis-in-r}}
\addcontentsline{toc}{section}{Functional Data Analysis in R}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  {fda} It is a basic reference to work in R with functional data, see \citep{Ramsay2005}, \url{http://ego.psych.mcgill.ca/misc/fda/}
\item
  Ferray and Vieu, (2006) processed FD from a nonparametric point of view (normed or semi--normed functional spaces).
  These authors are part of the French group STAPH maintaining the page
  \url{http://www.lsp.ups-tlse.fr/staph/}
\item
  {Core packages: }
\end{enumerate}

\begin{itemize}
\tightlist
\item
  {FDboost}: Boosting Functional Regression Models
\item
  {fds}: Functional Data Sets
\item
  {ftsa}: Functional Time Series Analysis
\item
  {fdasrvf}: Elastic Functional Data Analysis
\item
  {refund}: Regression with Functional Data
\item
  {fdapace}: Functional Data Analysis and Empirical Dynamics
\end{itemize}

4.{Interactive tools: }
+ {StatFda}: exploratory analysis and functional regression models,\url{http://www.statfda.com} forand refund.shiny package
for interactive plotting.
+ {refund.shiny}: Interactive Plotting for Functional Data Analyses
+ {tidyfun}: makes data wrangling and exploratory analysis of functional data easier, \url{https://fabian-s.github.io/tidyfun}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  {Other packages: }
\end{enumerate}

\begin{itemize}
\tightlist
\item
  {GPFDA}: Use Functional regression as the mean structure and Gaussian Process as the covariance structure.
\item
  {MFHD}: Multivariate Functional Halfspace Depth
\item
  {rainbow}: Bagplots, Boxplots and Rainbow Plots for Functional Data
\item
  {NITPicker}: Finds the Best Subset of Points to Sample
\item
  {fdatest}: Interval Testing Procedure for Functional Data
\item
  {fdakma}: Functional Data Analysis: K-Mean Alignment
\item
  {fdaMixed}: Functional data analysis in a mixed model framework
\item
  {geofd}: Spatial Prediction for Function Value Data
\item
  {goffda}: Goodness-of-Fit Tests for Functional Data
\item
  {mlr}: Machine Learning in R
\end{itemize}

\hypertarget{installation}{%
\section*{Installation}\label{installation}}
\addcontentsline{toc}{section}{Installation}

Like many other R packages, the simplest way to obtain \texttt{fda.usc} is to install it directly from CRAN. Type the following command in R console:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"fda.usc"}\NormalTok{, }\AttributeTok{repos =} \StringTok{"http://cran.us.r{-}project.org"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Users may change the \texttt{repos} options depending on their locations and preferences. Other options such as the directories where to install the packages can be altered in the command. For more details, see \texttt{help(install.packages)}.

Here the R package has been downloaded and installed to the default directories.

Alternatively, users can download the package source at \url{http://cran.r-project.org/web/packages/fda.usc/index.html} and type Unix commands to install it to the desired location.

\hypertarget{quick-start}{%
\section*{Quick Start}\label{quick-start}}
\addcontentsline{toc}{section}{Quick Start}

The purpose of this section is to give users a general sense of the package, including the components, what they do and some basic usage. We will briefly go over the main functions, see the basic operations and have a look at the outputs. Users may have a better idea after this section what functions are available, which one to choose, or at least where to seek help. More details are given in later sections.

First, we load the \texttt{fda.usc} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fda.usc)}
\end{Highlighting}
\end{Shaded}

\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrc}[1]{\left[#1\right]}
\newcommand{\lrb}[1]{\left\{#1\right\}}

\hypertarget{definition}{%
\chapter{Functional Data: Definition, Representation and Manipulation}\label{definition}}

\hypertarget{some-definitions-of-functional-data}{%
\section{Some definitions of Functional Data}\label{some-definitions-of-functional-data}}

Functional data analysis is a branch of statistics that analyzes data providing information about curves, surfaces or anything else varying over a continuum. The continuum is often time, but may also be spatial location, wavelength, probability, etc.

Functional data analysis is a branch of statistics concerned with analysing data in the form of functions.

\begin{itemize}
\item
  Definition 1: A random variable \(\mathcal{X}\) is called a functional variable if it takes values in a functional space \(\mathcal{E}\) --complete normed (or seminormed) space--, \citep{FV2006}
\item
  Definition~2: A functional dataset \(\{\mathcal{X}_1,\ldots,\mathcal{X}_n\}\) is the observation of \emph{n} functional variables \(\mathcal{X}_1,\ldots,\mathcal{X}_n\) identically distributed as \(\mathcal{X}\), \citep{FV2006}.
\end{itemize}

A research stream focuses on the computational treatment that is used to working with functional data as an extension of multivariate data. Thus, the following definition of functional data proposed by \citep{mu2008functional} is also common in practice.

\begin{itemize}
\tightlist
\item
  Definition~3. Functional data is multivariate data with an ordering on the dimensions, so that \(a=t_1<t_2,\ldots,t_{m-1}< b=t_m\).
\end{itemize}

\hypertarget{in-fda.usc-the-data-are-curves}{%
\section{In fda.usc: ``The data are curves'\,'}\label{in-fda.usc-the-data-are-curves}}

\(X_i(t)\) represents the mean temperature (averaged over 1980-2009 years) at the \emph{i}th weather station in Spain, and at time time \(t\) during the year.

Temperature curves (right) located in Spanish airports (left),

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-7-1} \end{center}

But what code has been used to obtain the graph?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(aemet)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{CanaryIslands }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{latitude }\SpecialCharTok{\textless{}} \DecValTok{31}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{df[,}\FunctionTok{c}\NormalTok{(}\StringTok{"longitude"}\NormalTok{,}\StringTok{"latitude"}\NormalTok{)], }\AttributeTok{col =}\NormalTok{ CanaryIslands, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(temp, }\AttributeTok{col =}\NormalTok{ CanaryIslands, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{definition-of-fdata-class-in-r}{%
\subsection{Definition of --fdata-- class in R}\label{definition-of-fdata-class-in-r}}

Definition of fdata class object: An object called \texttt{fdata} as a list of the following components:

\begin{itemize}
\tightlist
\item
  \texttt{data}: typically a matrix of \emph{(n x m)} dimension which contains a set of \emph{n} curves discretized in \emph{m} points or \texttt{argvals}.
\item
  \texttt{argvals}: locations of the discretization points, by default: \({{t}_1=1,\ldots,{t}_m=m}\}\).
\item
  \texttt{rangeval}: rangeval of discretization points.
\item
  \texttt{names}: (optional) list with three components: \texttt{main}, an overall title, \texttt{xlab}, a title for the x axis and \texttt{ylab}, a title for the y axis.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lapply}\NormalTok{(aemet,class)}
\FunctionTok{names}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{df)}
\FunctionTok{lapply}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{temp,class)}
\NormalTok{temp }\OtherTok{\textless{}{-}}\NormalTok{ aemet}\SpecialCharTok{$}\NormalTok{temp}
\FunctionTok{names}\NormalTok{(temp)}
\FunctionTok{dim}\NormalTok{(temp)}
\FunctionTok{length}\NormalTok{(}\FunctionTok{argvals}\NormalTok{(temp))}
\FunctionTok{rangeval}\NormalTok{(temp)}
\NormalTok{temp}\SpecialCharTok{$}\NormalTok{names}
\end{Highlighting}
\end{Shaded}

\hypertarget{some-utilities-of-fda.usc-package}{%
\subsection{Some utilities of fda.usc package}\label{some-utilities-of-fda.usc-package}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Basic operations for fdata class objects:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Group Math: \texttt{abs}, \texttt{sqrt}, \texttt{floor}, \texttt{ceiling,}semimetric.basis()\texttt{,}trunc\texttt{,}round\texttt{,}signif\texttt{,}exp\texttt{,}log\texttt{,}cos\texttt{,}sin\texttt{,}tan`.
\item
  Other operations: \texttt{{[}{]}}, \texttt{is.fdata()}, \texttt{c()}, \texttt{dim()}, \texttt{ncol()}, \texttt{nrow()}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.fdata}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{temp)}
\FunctionTok{is.fdata}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{df)}
\FunctionTok{dim}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{df)}
\FunctionTok{dim}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{temp)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Convert the class
\end{enumerate}

\begin{itemize}
\item
  The class \textbf{fdata} only uses the evaluations at the discretization points.
\item
  The \texttt{fdata2fd()} converts \textbf{fdata} object to \textbf{fd} object (using the basis representation).
\item
  Inversely, the \texttt{fdata()} converts object of class: \textbf{fd}, \textbf{fds}, \textbf{fts}, \textbf{sfts}, \textbf{vector}, \textbf{matrix}, \textbf{data.frame} to an object of class \textbf{fdata}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp.fd}\OtherTok{=}\FunctionTok{fdata2fd}\NormalTok{(temp,}\AttributeTok{type.basis=}\StringTok{"fourier"}\NormalTok{,}\AttributeTok{nbasis=}\DecValTok{15}\NormalTok{)}
\NormalTok{temp.fdata}\OtherTok{=}\FunctionTok{fdata}\NormalTok{(temp.fd) }\CommentTok{\#back to fdata}
\FunctionTok{class}\NormalTok{(temp.fd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "fd"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(temp.fdata)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "fdata"
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{split.fdata()} and \texttt{unlist()}: A wrapper for the split and unlist function for fdata object
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Canary Islands in red vs Iberian Peninsula in blue}
\NormalTok{l1 }\OtherTok{\textless{}{-}}\NormalTok{ fda.usc}\SpecialCharTok{:::}\FunctionTok{split.fdata}\NormalTok{(temp,CanaryIslands)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(l1[[}\DecValTok{1}\NormalTok{]],}\AttributeTok{col=}\DecValTok{2}\NormalTok{,}\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{30}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(l1[[}\DecValTok{2}\NormalTok{]],}\AttributeTok{col=}\DecValTok{4}\NormalTok{,}\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{30}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-12-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(l1[[}\DecValTok{1}\NormalTok{]]);}\FunctionTok{dim}\NormalTok{(l1[[}\DecValTok{2}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   9 365
\end{verbatim}

\begin{verbatim}
## [1]  64 365
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Group Ops: + -, *, /, \^{}, \%\%, \%/\%, \&, \textbar, !, ==, !=, \textless, \textless=, \textgreater=, \textgreater.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l1[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{==}\NormalTok{l1[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{order.fdata()} A wrapper for the order function. The funcional data is ordered w.r.t the sample order of the values of vector.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp2 }\OtherTok{\textless{}{-}} \FunctionTok{order.fdata}\NormalTok{(}\FunctionTok{order}\NormalTok{(CanaryIslands),temp)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Generate random process of fdata class.
\end{itemize}

\texttt{rproc2fdata()} function generates Functional data from: Ornstein Uhlenbeck process, Brownian process, Gaussian process or Exponential variogram process.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{lent }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{tt }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\AttributeTok{len=}\NormalTok{lent)}
\NormalTok{xgen1 }\OtherTok{\textless{}{-}} \FunctionTok{rproc2fdata}\NormalTok{(}\DecValTok{200}\NormalTok{,}\AttributeTok{t=}\NormalTok{tt,}\AttributeTok{sigma=}\StringTok{"OU"}\NormalTok{,}\AttributeTok{par.list=}\FunctionTok{list}\NormalTok{(}\StringTok{"scale"}\OtherTok{=}\DecValTok{1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(xgen1)}
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{fdata}\NormalTok{(}\FunctionTok{sin}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{tt),tt)}
\NormalTok{xgen2 }\OtherTok{\textless{}{-}} \FunctionTok{rproc2fdata}\NormalTok{(}\DecValTok{200}\NormalTok{,}\AttributeTok{mu=}\NormalTok{mu,}\AttributeTok{sigma=}\StringTok{"OU"}\NormalTok{,}\AttributeTok{par.list=}\FunctionTok{list}\NormalTok{(}\StringTok{"scale"}\OtherTok{=}\DecValTok{1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(xgen2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/rproc2-1} \end{center}

\hypertarget{definition-of-ldata-class-in-r}{%
\subsection{Definition of --ldata-- class in R}\label{definition-of-ldata-class-in-r}}

ldata is a list with two type of objects:

\begin{itemize}
\item
  \texttt{df} is a data frame with the multivariate data with \emph{n} rows.
\item
  \texttt{...} objects of class \texttt{fdata} with \emph{n} rows.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ldat }\OtherTok{\textless{}{-}} \FunctionTok{ldata}\NormalTok{(}\AttributeTok{df=}\NormalTok{aemet}\SpecialCharTok{$}\NormalTok{df,}\AttributeTok{temp=}\NormalTok{aemet}\SpecialCharTok{$}\NormalTok{temp, }\AttributeTok{logprec=}\NormalTok{aemet}\SpecialCharTok{$}\NormalTok{logprec )}
\FunctionTok{plot}\NormalTok{(ldat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/ldata-1} \end{center}

\hypertarget{resume-by-smoothing}{%
\section{Resume by smoothing}\label{resume-by-smoothing}}

If we supposed that our functional data \(Y(t)\) is observed through the model \(Y(t_i)=X(t_i)+\varepsilon(t_i)\)
where the residuals \(\varepsilon(t)\) are independent with \(X(t)\).

We can to get back the original signal \(X(t)\) using a linear smoother:
\[\hat{X}(t_i)=\sum_{i=1}^{n} s_{i}(t_j)Y(t_i)  \Rightarrow \mathbf{\hat{X}}=\mathbf{S}\mathbf{Y} \]
where \(s_{i}(t_j)\) is the weight that the point \(t_j\) gives to the point \(t_i\).

We use two methods to estimate the smoothing matrix \(S\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Finite representation in a fixed basis \citep{Ramsay2005}:
\end{enumerate}

Let \(X(t)\in \mathcal{L}_2\),
\[ X(t)= \sum_{k\in\mathbb{N}}{c_k\phi_k(t)}\approx\sum_{k=1}^K c_k\phi_k(t)=c^{\top}\Phi\]

The smoothing matrix is given by: \(\mathbf{S}=\Phi(\Phi^{\top}W\Phi+\lambda R)^{-1}\Phi^{\top}W\) where \(\lambda\) is the penalty parameter.

Type of basis:

\begin{itemize}
\item
  Fourier: Design to represent periodic functions. Orthonormal
\item
  BSplines: Set of polynomials (of order m) defined in subintervals constructed in such a way that in the border of the subintervals the polynomials coincide (up to \(m - 2\) derivative). Banded
\item
  Wavelets: Powerful especially when the grid is a power of 2. Orthonormal
\item
  Polynomials: Adjust a polynomial to the whole curve.
\end{itemize}

Example: raw (left) and smoothed temperature curves with 111 (center) and 11 (right) elements of a B-spline base:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bsp11 }\OtherTok{\textless{}{-}} \FunctionTok{create.bspline.basis}\NormalTok{(temp}\SpecialCharTok{$}\NormalTok{rangeval,}\AttributeTok{nbasis=}\DecValTok{11}\NormalTok{)}
\NormalTok{bsp111 }\OtherTok{\textless{}{-}} \FunctionTok{create.bspline.basis}\NormalTok{(temp}\SpecialCharTok{$}\NormalTok{rangeval,}\AttributeTok{nbasis=}\DecValTok{111}\NormalTok{)}
\NormalTok{S.bsp11 }\OtherTok{\textless{}{-}}  \FunctionTok{S.basis}\NormalTok{(temp}\SpecialCharTok{$}\NormalTok{argvals, bsp11)}
\NormalTok{S.bsp111 }\OtherTok{\textless{}{-}} \FunctionTok{S.basis}\NormalTok{(temp}\SpecialCharTok{$}\NormalTok{argvals, bsp111)}
\NormalTok{temp.bsp11 }\OtherTok{\textless{}{-}}\NormalTok{ temp.bsp111}\OtherTok{\textless{}{-}}\NormalTok{temp}
\NormalTok{temp.bsp11}\SpecialCharTok{$}\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ temp}\SpecialCharTok{$}\NormalTok{data}\SpecialCharTok{\%*\%}\NormalTok{S.bsp11}
\NormalTok{temp.bsp111}\SpecialCharTok{$}\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ temp}\SpecialCharTok{$}\NormalTok{data}\SpecialCharTok{\%*\%}\NormalTok{S.bsp111 }
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(temp)}
\FunctionTok{plot}\NormalTok{(temp.bsp111, }\AttributeTok{main=}\StringTok{"111 Bspline basis elements"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(temp.bsp11, }\AttributeTok{main=}\StringTok{"11 Bspline basis elements"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/smoothbsp-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Another way }
\NormalTok{out1}\OtherTok{\textless{}{-}}\FunctionTok{optim.basis}\NormalTok{(temp, }\AttributeTok{type.CV=}\NormalTok{CV.S)}
\FunctionTok{names}\NormalTok{(out1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "gcv"          "numbasis"     "lambda"       "fdataobj"     "fdata.est"   
##  [6] "gcv.opt"      "numbasis.opt" "lambda.opt"   "S.opt"        "base.opt"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out1}\SpecialCharTok{$}\NormalTok{numbasis.opt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 76
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Kernel Smoothing \citep{FV2006}
\end{enumerate}

The problem is to estimate the smoothing parameter or bandwidth \(\nu=h\) that better represents the functional data using kernel smoothing.
Now,the nonparametric smoothing of functional data is given by the smoothing matrix \(S\):
\[s_{ij}=\frac{1}{h}K\left(\frac{t_i-t_j}{h}\right)\]

\[S(h)=(s_j(t_i))=\frac{K\left(\frac{t_i-t_j}{h}\right)}{\sum_{k=1}^{\top}K\left(\frac{t_k-t_j}{h}\right)}\]
where \(h\) is the bandwidth and \(K()\) the Kernel function.

Different types of kernels \(K()\) are defined in the package, see \texttt{Kernel} function.

Example: three examples of non-parametric smoothing with different values of the bandwidth parameter (\(h = 1\), left; \(h = 10\), center and \(h\) selected by the GCV criteria, right).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S }\OtherTok{\textless{}{-}} \FunctionTok{S.NW}\NormalTok{(temp}\SpecialCharTok{$}\NormalTok{argvals,}\AttributeTok{h=}\DecValTok{1}\NormalTok{)}
\NormalTok{temp }\OtherTok{\textless{}{-}}\NormalTok{ aemet}\SpecialCharTok{$}\NormalTok{temp}
\NormalTok{temp.hat }\OtherTok{\textless{}{-}}\NormalTok{ temp}
\NormalTok{temp.hat}\SpecialCharTok{$}\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ temp}\SpecialCharTok{$}\NormalTok{data}\SpecialCharTok{\%*\%}\NormalTok{S}
\FunctionTok{args}\NormalTok{(optim.np)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (fdataobj, h = NULL, W = NULL, Ker = Ker.norm, type.CV = GCV.S, 
##     type.S = S.NW, par.CV = list(trim = 0, draw = FALSE), par.S = list(), 
##     correl = TRUE, verbose = FALSE, ...) 
## NULL
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Another way }
\NormalTok{temp.est }\OtherTok{\textless{}{-}} \FunctionTok{optim.np}\NormalTok{(temp,}\AttributeTok{h=}\DecValTok{1}\NormalTok{)}\SpecialCharTok{$}\NormalTok{fdata.est}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(temp.est,}\AttributeTok{main=}\StringTok{"Kernel smooth {-} h=1"}\NormalTok{)}
\NormalTok{temp.est }\SpecialCharTok{==}\NormalTok{ temp.hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(temp.hat, }\AttributeTok{main=}\StringTok{"Kernel smooth {-} h=10"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{optim.np}\NormalTok{(temp)}\SpecialCharTok{$}\NormalTok{fdata.est, }\AttributeTok{main=}\StringTok{"Kernel smooth{-} h{-}GCV"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/smoothNW-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Finite representation in a Data-driven basis \citep{Cardot_1999}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Functional Principal Components (FPC)
  Functional Principal Components Analysis (FPCA) and the Functional PLS (FPLS) allow display the functional in a few components.
\end{itemize}

The functional data can be rewritten as a decomposition in an orthonormal PC basis: maximizing the variance of \(X(t)\):

\[
\hat{X}_{i}(t)=\sum_{i=1}^{K}f_{ik}\xi_{k}(t)
\]

where \(f_{i,k}\) is the score of the principal component PC, \(\xi_k\).

Example: Functional principal component analysis (FPCA)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rowSums}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{logprec}\SpecialCharTok{$}\NormalTok{data)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{temp)}
\FunctionTok{plot}\NormalTok{((pc }\OtherTok{\textless{}{-}} \FunctionTok{create.pc.basis}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{temp,}\AttributeTok{l=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{))}\SpecialCharTok{$}\NormalTok{fdata.est,}\AttributeTok{main=}\StringTok{"Temperature, 2 FPC"}\NormalTok{)}
\NormalTok{pc2 }\OtherTok{\textless{}{-}} \FunctionTok{fdata2pc}\NormalTok{(aemet}\SpecialCharTok{$}\NormalTok{temp)}
\FunctionTok{plot}\NormalTok{(pc2}\SpecialCharTok{$}\NormalTok{rotation, }\AttributeTok{main=}\StringTok{"2 FPC basis"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"day"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/smoothPC-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pc2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
## 
##      - SUMMARY:   fdata2pc  object   -
## 
## -With 2  components are explained  98.78 %
##  of the variability of explicative variables.
##  
## -Variability for each component (%):
## [1] 85.57 13.21
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/smoothPC-2} \end{center}

\begin{itemize}
\tightlist
\item
  Functional Partial Least Squares (FPLS) \citep{kramer}
  The PLS components seek to maximize the covariance of \(X(t)\) and \(y\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{args}\NormalTok{(create.pls.basis)}
\FunctionTok{args}\NormalTok{(fdata2pls)}
\end{Highlighting}
\end{Shaded}

An integrated version of these functions is coming.

\begin{quote}
Example: Exploratory analysis for spectrometric curves
\end{quote}

Tecator dataset: 215 spectrometric curves of meat samples also with Fat, Water and Protein contents obtained by analytic procedures.

Tecator Goal: Explain the fat content through spectrometric curves.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(tecator)}
\FunctionTok{names}\NormalTok{(tecator)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "absorp.fdata" "y"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Fat"     "Water"   "Protein"
\end{verbatim}

As shown in tecator Figure, the plot of \(X(t)\) against \(t\) is not necessarily the most informative and other semi-metric can allow to extract much information from functional variables.

The information about fat seems to be contained in the shape of the curves, so, the semimetric of derivatives could be preferred to \(\mathcal{L}_2\). It is difficult to find the best plot given a particular functional dataset because the shape of graphics depends strongly on the proximity measure.

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/tec2-1} \end{center}

\hypertarget{derivatives}{%
\subsection{Derivatives}\label{derivatives}}

To compute derivatives there are several options (see \texttt{fdata.deriv} function):

\begin{itemize}
\tightlist
\item
  Raw Diferentiation: Only interesting when the number of discretization points are dense.
\item
  Basis representation: Dierentiate a finite representation in a basis.
\item
  Spline Interpolation: Perform a spline interpolation of given data points and use this interpolation for computing derivatives.
\item
  Nonparametric Estimation: Use a Local Polynomial Estimator of the functional data.
\end{itemize}

Examle:

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/deriv1-1} \end{center}

\hypertarget{computing-distances-norms-and-inner-products}{%
\subsection{Computing distances, norms and inner products}\label{computing-distances-norms-and-inner-products}}

Utilities for computing distances, norm and inner products are included in the package. Below are described those but of course, many others can be built with the only restriction that the first two arguments correspond to class --fdata--. In addition, the procedures of the package \textbf{fda.usc} that contains the argument --metric-- allow the use of metric or semi-metric functions as shown in the following sections.

\begin{itemize}
\item
  Distance between functional elements, \texttt{metric.lp}: \(d(X(t),Y(t))=\left\|X(t)-Y(t)\right\|\) with a norm \(\left\| \cdot \right\|\)
\item
  Norm \texttt{norm.fdata}:
  \(\left\|X(t)\right\|=\sqrt{\left\langle X(t),X(t)\right\rangle}\)
\item
  Inner product \texttt{inprod.fdata}:
  \(\left\langle x,y \right\rangle=(1/4)(\left\|x+y\right\|^2-\left\|x-y\right\|^2)\).
\end{itemize}

\textbf{fda.usc} package collects several metric and semi-metric functions which allow to extract as much information possible from the functional variable.

Option 1: If the data is in a Hilbert space, represent your data in a basis, and compute all the things accordingly. From a practical point of way, the representation of a functional datum
must be approximated by a finite number of terms.

\begin{itemize}
\tightlist
\item
  \texttt{semimetric.basis()}
\end{itemize}

Option 2: Approximates all quantities using numerical approximations (Valid for Hilbert and non-Hilbert spaces). This usually involves numerical approximations of integrals, derivatives and so on. A collection of semi-metrics proposed by \citep{FV2006} are also included in the package.

\begin{itemize}
\tightlist
\item
  \texttt{semimetric.pca()}, based on the Principal Components.
\item
  \texttt{semimetric.mplsr()}, based on the Partial Least Squares.\\
\item
  \texttt{semimetric.deriv()}, based on B-spline representation.
\item
  \texttt{semimetric.hshift()}, measure the horizontal shift effect.
\item
  \texttt{semimetric.fourier()}, based on ther Fourier representation
\end{itemize}

\begin{quote}
Example of computing norms
\end{quote}

Consider \(X(t) = t^2; t\in[0, 1]\). In this case \(||X_1|| =\sqrt{1/5}=0.4472136\) and \(||X_2|| =1/3\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{len =} \DecValTok{101}\NormalTok{)}
\NormalTok{x2 }\OtherTok{=}\NormalTok{ t}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{x2 }\OtherTok{=} \FunctionTok{fdata}\NormalTok{(x2, t)}
\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{norm.fdata}\NormalTok{(x2), }\FunctionTok{norm.fdata}\NormalTok{(x2, }\AttributeTok{lp =} \DecValTok{1}\NormalTok{))) }\CommentTok{\# L2, L1 norm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4472509 0.3333500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f1 }\OtherTok{=} \FunctionTok{fdata}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{length}\NormalTok{(t)), t)}
\NormalTok{f2 }\OtherTok{=} \FunctionTok{fdata}\NormalTok{(}\FunctionTok{sin}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ pi }\SpecialCharTok{*}\NormalTok{ t), t) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{f3 }\OtherTok{=} \FunctionTok{fdata}\NormalTok{(}\FunctionTok{cos}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ pi }\SpecialCharTok{*}\NormalTok{ t), t) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{f4 }\OtherTok{=} \FunctionTok{fdata}\NormalTok{(}\FunctionTok{sin}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*}\NormalTok{ pi }\SpecialCharTok{*}\NormalTok{ t), t) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{f5 }\OtherTok{=} \FunctionTok{fdata}\NormalTok{(}\FunctionTok{cos}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*}\NormalTok{ pi }\SpecialCharTok{*}\NormalTok{ t), t) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{fou5 }\OtherTok{=} \FunctionTok{c}\NormalTok{(f1, f2, f3, f4, f5) }\CommentTok{\# Fourier basis is Orthonormal}
\FunctionTok{round}\NormalTok{(}\FunctionTok{inprod.fdata}\NormalTok{(fou5), }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    0    0    0    0
## [2,]    0    1    0    0    0
## [3,]    0    0    1    0    0
## [4,]    0    0    0    1    0
## [5,]    0    0    0    0    1
\end{verbatim}

\begin{quote}
Example of computing distances
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)}
\NormalTok{p}\OtherTok{\textless{}{-}}\DecValTok{1001}
\NormalTok{r}\OtherTok{\textless{}{-}}\FunctionTok{rnorm}\NormalTok{(p,}\AttributeTok{sd=}\NormalTok{.}\DecValTok{1}\NormalTok{)}
\NormalTok{x}\OtherTok{\textless{}{-}}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi,}\AttributeTok{length=}\NormalTok{p)}
\NormalTok{fx}\OtherTok{\textless{}{-}}\FunctionTok{fdata}\NormalTok{((}\FunctionTok{sin}\NormalTok{(x)}\SpecialCharTok{+}\NormalTok{r)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(pi),x)}
\NormalTok{fx0}\OtherTok{\textless{}{-}}\FunctionTok{fdata}\NormalTok{(r,x)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{c}\NormalTok{(fx,fx0))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/semi1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{metric.lp}\NormalTok{(fx,fx0)[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.003144
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{semimetric.basis}\NormalTok{(fx,fx0,}\AttributeTok{nbasis1=}\DecValTok{5}\NormalTok{,}\AttributeTok{nbasis2=}\DecValTok{5}\NormalTok{)[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9927386
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{semimetric.basis}\NormalTok{(fx,fx0,}\AttributeTok{nbasis1=}\DecValTok{11}\NormalTok{,}\AttributeTok{nbasis2=}\DecValTok{111}\NormalTok{)[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9992571
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{semimetric.fourier}\NormalTok{(fx,fx0,}\AttributeTok{nbasis=}\DecValTok{5}\NormalTok{)[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9972634
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{semimetric.fourier}\NormalTok{(fx,fx0,}\AttributeTok{nbasis=}\DecValTok{11}\NormalTok{)[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9992104
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{semimetric.deriv}\NormalTok{(fx,fx0,}\AttributeTok{nderiv=}\DecValTok{0}\NormalTok{,}\AttributeTok{nknot=}\DecValTok{5}\NormalTok{)[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9934345
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{semimetric.deriv}\NormalTok{(fx,fx0,}\AttributeTok{nderiv=}\DecValTok{0}\NormalTok{,}\AttributeTok{nknot=}\DecValTok{11}\NormalTok{)[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9984524
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{integrate}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x)\{(}\FunctionTok{sin}\NormalTok{(x)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(pi))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{\},}\DecValTok{0}\NormalTok{,}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1 with absolute error < 7.3e-10
\end{verbatim}

\begin{quote}
Example of semi-metric as classification rule
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(tecator)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)}
\CommentTok{\#ind\textless{}{-}sample(215,30)}
\CommentTok{\#xx\textless{}{-}tecator$absorp[ind]}

\NormalTok{yy}\OtherTok{\textless{}{-}}\FunctionTok{ifelse}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{$}\NormalTok{Fat}\SpecialCharTok{\textless{}}\DecValTok{20}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{ind}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\FunctionTok{which}\NormalTok{(yy}\SpecialCharTok{==}\DecValTok{0}\NormalTok{),}\DecValTok{10}\NormalTok{),}\FunctionTok{sample}\NormalTok{(}\FunctionTok{which}\NormalTok{(yy}\SpecialCharTok{==}\DecValTok{1}\NormalTok{),}\DecValTok{10}\NormalTok{))}
\NormalTok{xx}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{absorp[ind])}
\NormalTok{yy}\OtherTok{\textless{}{-}}\NormalTok{yy[ind]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{d1}\OtherTok{\textless{}{-}}\FunctionTok{as.dist}\NormalTok{(}\FunctionTok{metric.lp}\NormalTok{(xx),T,T)}
\NormalTok{c1}\OtherTok{\textless{}{-}}\FunctionTok{hclust}\NormalTok{(d1)}
\NormalTok{c1}\SpecialCharTok{$}\NormalTok{labels}\OtherTok{=}\NormalTok{yy}
\FunctionTok{plot}\NormalTok{(c1,}\AttributeTok{main=}\StringTok{"Raw data {-}{-} metric.lp"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"Class of eah leaf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-14-1} \end{center}

Try with the derivative of the curves:

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-15-1} \end{center}

Extension: Combining the information of two components using a \(p\)-dimensional metric such as, for example, the Euclidean:

\[m\left(\left(x_0^1,\ldots,x_0^p\right),\left(x_i^1,\ldots,x_i^p\right)\right):=\sqrt{m_1\left(x_0^1,x_i^1\right)^2+\cdots+m_p\left(x_0^p,x_i^p\right)^2}\]
where \(m_{i}\) denotes the metric in the \(i\)-component of the product space, see \texttt{fda.usc:::metric.ldata} code.

It is important here to ensure that the different metrics of the spaces have similar scales to avoid one single component dominating the overall distance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{range}\NormalTok{(d1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.005531143 0.091797359
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{range}\NormalTok{(d2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0003143458 0.0105466510
\end{verbatim}

\begin{quote}
NOTE: Dependent data in tecator dataset?
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ar}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{$}\NormalTok{Fat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## ar(x = tecator$y$Fat)
## 
## Coefficients:
##      1       2  
## 0.6552  0.1161  
## 
## Order selected 2  sigma^2 estimated as  72.82
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y,}\AttributeTok{col=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\FunctionTok{ts.plot}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{$}\NormalTok{Fat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-17-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#barplot(t(tecator$y))}
\end{Highlighting}
\end{Shaded}

\hypertarget{correlation-distances}{%
\section{Correlation Distances}\label{correlation-distances}}

The correlation Distances characterizes independence between vectors of arbitrary finite dimensions. Recently, in \citep{Szekely2013}, a bias-corrected version is considered and a test of independence developed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(tecator)}
\NormalTok{xx}\OtherTok{\textless{}{-}}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{absorp}
\NormalTok{xx.d2}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(xx,}\AttributeTok{nderiv=}\DecValTok{2}\NormalTok{)}
\FunctionTok{dcor.xy}\NormalTok{(xx,xx.d2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  dcor t-test of independence
## 
## data:  D1 and D2
## T = 35.933, df = 22789, p-value < 2.2e-16
## sample estimates:
## Bias corrected dcor 
##           0.2315581
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d1}\OtherTok{\textless{}{-}}\FunctionTok{metric.lp}\NormalTok{(xx)}
\NormalTok{d2}\OtherTok{\textless{}{-}}\FunctionTok{metric.lp}\NormalTok{(xx.d2)}
\FunctionTok{bcdcor.dist}\NormalTok{(d1,d2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2315581
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fat }\OtherTok{\textless{}{-}}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{y[,}\StringTok{"Fat"}\NormalTok{,drop}\OtherTok{=}\ConstantTok{FALSE}\NormalTok{]}
\NormalTok{d3}\OtherTok{\textless{}{-}}\FunctionTok{metric.dist}\NormalTok{(Fat)}
\FunctionTok{bcdcor.dist}\NormalTok{(d1,d3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1963254
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bcdcor.dist}\NormalTok{(d2,d3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9147076
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Functional / Factor}
\NormalTok{Fat.cut}\OtherTok{\textless{}{-}}\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{cut}\NormalTok{(Fat[,}\DecValTok{1}\NormalTok{],}\AttributeTok{labels=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}\AttributeTok{breaks=}\DecValTok{4}\NormalTok{))}
\FunctionTok{dcor.xy}\NormalTok{(Fat.cut,xx.d2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  dcor t-test of independence
## 
## data:  D1 and D2
## T = 252.57, df = 22789, p-value < 2.2e-16
## sample estimates:
## Bias corrected dcor 
##           0.8583661
\end{verbatim}

\hypertarget{depth-for-functional-data}{%
\subsection{Depth for functional data}\label{depth-for-functional-data}}

Depth (in univariate or multivariate context) is a statistical tool that provides a center-outward ordering of data points. This ordering can be employed to define location measures (and by oposition outliers). Also, some measures can be defined as maximizers of a particular depth.

\begin{itemize}
\tightlist
\item
  Mahalanobis Depth (Mean) \texttt{mdepth.MhD}
\item
  Halfspace Depth, also known as Tukey Depth (Median) \texttt{mdepth.HS}
\item
  Convex Hull Peeling Depth (Mode)
\item
  Oja Depth
\item
  Simplicial Depth \texttt{mdepth.SD}
\item
  Likelihood Depth (Mode) \texttt{mdepth.LD}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y12}\OtherTok{\textless{}{-}}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{y[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\NormalTok{dep}\OtherTok{\textless{}{-}}\FunctionTok{mdepth.LD}\NormalTok{(}\AttributeTok{x=}\NormalTok{y12,}\AttributeTok{scale=}\NormalTok{T)}\SpecialCharTok{$}\NormalTok{dep}
\FunctionTok{plot}\NormalTok{(y12,}\AttributeTok{col=}\FunctionTok{grey}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{dep))}
\FunctionTok{points}\NormalTok{(y12[}\FunctionTok{which.max}\NormalTok{(dep),],}\AttributeTok{col=}\DecValTok{4}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-19-1} \end{center}

Many of the preceding depth functions defined in a multivariate contex cannot be applied to functional data. In any case, given a depth for functional data.

\begin{itemize}
\item
  The deepest point is a location measure and based on the depth function can have a dierent interpretation: Mean, Median, Mode \ldots{}
\item
  Computing the depths of the whole sample and ordering them (in decreasing order) gives a rank from the most central point (\(x_{[1]}\)) to the most outlying one (\(x_{[n]}\)). So, those points with larger rank are the candidates to be outliers w.r.t. the data cloud (in the sense of the depth function).
\item
  Summarizing the \((1-\alpha)\) deepest points could lead also to a robust location measure.
\item
  Fraiman-Muniz Depth \citep{FM2001} \texttt{depth.FM}
\item
  Modal Depth \citep{Cuevas2007} \texttt{depth.mode}
\item
  Random Projection Depth \citep{Cuevas2007} \texttt{depth.RP}
\item
  Random Tukey Depth \citep{Cuesta2008} \texttt{depth.RT}\\
\item
  Band Depth \citep{Lopez2009} \texttt{depth.MBD} (private function)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OtherTok{\textless{}{-}}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{absorp}
\FunctionTok{depth.mode}\NormalTok{(x,}\AttributeTok{draw=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-20-1} \end{center}

\hypertarget{depth-and-distances-for-multivariate-functional-data-cuesta2017}{%
\subsection{\texorpdfstring{Depth (and distances) for multivariate functional data \citep{cuesta2017}}{Depth (and distances) for multivariate functional data {[}@cuesta2017{]}}}\label{depth-and-distances-for-multivariate-functional-data-cuesta2017}}

Modify the procedure to incorporate the extended information.

\begin{itemize}
\tightlist
\item
  Fraiman--Muniz: Compute a multivariate depth marginally. \texttt{depth.FMp}
\item
  Modal depth: Use a new distance between data (for derivatives, for example, the Sobolev metric). \texttt{depth.FMp}
\item
  Random Projection: Consider a multivariate depth to be applied to the dierent projections (also the random projection method could be applied twice). \texttt{depth.RPp}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x.d1}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(x)}
\FunctionTok{depth.modep}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{x.d1=}\NormalTok{x.d1),}\AttributeTok{draw=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-21-1} \end{center}

\begin{quote}
Example poblenou dataset
\end{quote}

Hourly levels of nitrogen oxides in Poblenou (Barcelona). This dataset has 127 daily records (2005/01/06-2005/06/26).

Objective: Explain the diferences in NOx levels as a function of day.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(poblenou)}
\NormalTok{dayw }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{day.week }\SpecialCharTok{==} \DecValTok{7} \SpecialCharTok{|}\NormalTok{ poblenou}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{day.festive }\SpecialCharTok{==}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{day.week }\SpecialCharTok{==} \DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox,}\AttributeTok{col=}\NormalTok{dayw)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-22-1} \end{center}

The \(\mathcal{L}_2\) space (distance is area between curves) seems appropriate although other possibilities could be take into account (\(\mathcal{L}_1\), for example).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fmd }\OtherTok{=} \FunctionTok{depth.FM}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox,}\AttributeTok{draw=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-23-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md }\OtherTok{=}  \FunctionTok{depth.mode}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox)}
\NormalTok{rpd }\OtherTok{=} \FunctionTok{depth.RP}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox, }\AttributeTok{nproj =} \DecValTok{50}\NormalTok{)}
\NormalTok{rtd }\OtherTok{=} \FunctionTok{depth.RT}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox)}
\FunctionTok{print}\NormalTok{(cur }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(fmd}\SpecialCharTok{$}\NormalTok{lmed, md}\SpecialCharTok{$}\NormalTok{lmed, rpd}\SpecialCharTok{$}\NormalTok{lmed, rtd}\SpecialCharTok{$}\NormalTok{lmed))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 2005-05-06 2005-05-06 2005-05-06 2005-03-04 
##         63         63         63         10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox,}\AttributeTok{col=}\StringTok{"grey"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox[cur], }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{col =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"FMD"}\NormalTok{, }\StringTok{"MD"}\NormalTok{, }\StringTok{"RPD"}\NormalTok{, }\StringTok{"RTD"}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}\AttributeTok{col =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-23-2} \end{center}

\hypertarget{outliers-detection}{%
\subsection{Outliers detection}\label{outliers-detection}}

There is no general accepted definition of outliers in Functional data so, we define outlier as a datum generated from a dierent process than the rest of the sample with the following characteristics
Its number in the sample is unknown but probably low.

An outlier will have low depth and it will be an outlier in the sense of the depth used.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md1 }\OtherTok{=} \FunctionTok{depth.mode}\NormalTok{(lab }\OtherTok{\textless{}{-}}\NormalTok{ poblenou}\SpecialCharTok{$}\NormalTok{nox[dayw }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]) }\CommentTok{\#Labour days}
\NormalTok{md2 }\OtherTok{=} \FunctionTok{depth.mode}\NormalTok{(sat }\OtherTok{\textless{}{-}}\NormalTok{ poblenou}\SpecialCharTok{$}\NormalTok{nox[dayw }\SpecialCharTok{==} \DecValTok{2}\NormalTok{]) }\CommentTok{\#Saturdays}
\NormalTok{md3 }\OtherTok{=} \FunctionTok{depth.mode}\NormalTok{(sun }\OtherTok{\textless{}{-}}\NormalTok{ poblenou}\SpecialCharTok{$}\NormalTok{nox[dayw }\SpecialCharTok{==} \DecValTok{3}\NormalTok{]) }\CommentTok{\#Sundays/Festive}
\FunctionTok{rbind}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{df[dayw }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, ][}\FunctionTok{which.min}\NormalTok{(md1}\SpecialCharTok{$}\NormalTok{dep), ], poblenou}\SpecialCharTok{$}\NormalTok{df[dayw }\SpecialCharTok{==}
\DecValTok{2}\NormalTok{, ][}\FunctionTok{which.min}\NormalTok{(md2}\SpecialCharTok{$}\NormalTok{dep), ], poblenou}\SpecialCharTok{$}\NormalTok{df[dayw }\SpecialCharTok{==} \DecValTok{3}\NormalTok{, ][}\FunctionTok{which.min}\NormalTok{(md3}\SpecialCharTok{$}\NormalTok{dep),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          date day.week day.festive
## 22 2005-03-18        5           0
## 57 2005-04-30        6           0
## 58 2005-05-01        7           0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox,}\AttributeTok{col=}\StringTok{"grey"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(poblenou}\SpecialCharTok{$}\NormalTok{nox[}\FunctionTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{,}\DecValTok{57}\NormalTok{,}\DecValTok{58}\NormalTok{),], }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{col =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Method for detecting outliers, see Febrero{-}Bande, 2008}
\CommentTok{\#out1\textless{}{-}outliers.depth.trim(poblenou$nox[dayw == 1],nb=100)$outliers}
\end{Highlighting}
\end{Shaded}

Two procedures for detecting outliers are implemented in the package. \citep{Febrero2008}

\begin{itemize}
\item
  Detecting outliers based on trimming: \texttt{outliers.depth.trim()}
\item
  Detecting outliers based on weighting: \texttt{outliers.depth.pond()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Method for detecting outliers, see Febrero{-}Bande, 2008}
\CommentTok{\#out1\textless{}{-}outliers.depth.trim(poblenou$nox[dayw == 1],nb=100)$outliers}
\CommentTok{\#out2\textless{}{-}outliers.depth.pond(poblenou$nox[dayw == 1],nb=100)$outliers}
\end{Highlighting}
\end{Shaded}

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{regression}{%
\chapter{Functional Regression Model}\label{regression}}

Regression models are those techniques for modeling and analyzing the relationship between a dependent variable and one or more independent variables. When one of the variables have a functional nature, we have functional regression models.

This section is devoted to all the functional regression models where the response variable is scalar and at least, there is one functional covariate.

For illustration, we will use the Tecator dataset to predict the fat contents from
The explanatory variables to introduce in the models are:p The curves of absorbance \(X(t)\) as functional data or one of its two first derivatives (\(X.d1,X.d2\)) and/or Water content as real variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fda.usc)}
\FunctionTok{data}\NormalTok{(tecator)}
\NormalTok{absorp}\OtherTok{\textless{}{-}}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{absorp}
\NormalTok{ind}\OtherTok{\textless{}{-}}\FunctionTok{sample}\NormalTok{(}\DecValTok{215}\NormalTok{,}\DecValTok{129}\NormalTok{) }\CommentTok{\#ind = 1:129 }
\NormalTok{tt }\OtherTok{=}\NormalTok{ absorp[[}\StringTok{"argvals"}\NormalTok{]]}
\NormalTok{y }\OtherTok{=}\NormalTok{ tecator[[}\StringTok{"y"}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{Fat[ind]}
\NormalTok{X }\OtherTok{=}\NormalTok{ absorp[ind, ]}
\NormalTok{X.d1 }\OtherTok{=} \FunctionTok{fdata.deriv}\NormalTok{(X, }\AttributeTok{nbasis =} \DecValTok{19}\NormalTok{, }\AttributeTok{nderiv =} \DecValTok{1}\NormalTok{)}
\NormalTok{X.d2 }\OtherTok{=} \FunctionTok{fdata.deriv}\NormalTok{(X, }\AttributeTok{nbasis =} \DecValTok{19}\NormalTok{, }\AttributeTok{nderiv =} \DecValTok{2}\NormalTok{)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(X)}
\FunctionTok{plot}\NormalTok{(X.d1)}
\FunctionTok{plot}\NormalTok{(X.d2)}
\FunctionTok{boxplot}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/warning==F-1} \end{center}

In the following sections, regression methods implemented --fda.usc-- pacakge in the package are presented one by one and illustrated with examples for estimating the \textbf{Fat} content of the Tecator dataset.

\hypertarget{functional-linear-model-flr-with-basis-representation}{%
\section{Functional linear model (FLR) with basis representation}\label{functional-linear-model-flr-with-basis-representation}}

Supose that \(\mathcal{X} \in \mathcal{L}_{2}(T)\) and \(y \in \mathbb{R}\). Assume also that
\(\mathbb{E}[\mathcal{X}(t)]=0, \forall t \in [0,T]\) and \(\mathbb{E}[y]=0\).

The FLM states that
\[y= \left\langle  \mathcal{X},\beta \right\rangle +\varepsilon=\int_{T}X(t)\beta(t)dt+\varepsilon\]
where \(\beta \in \mathcal{L}_{2}(T)\) and \(\varepsilon\) is the errror term.

One way of estimating \(\beta\), it is representing the parametmer (and \(\mathcal{X}\)) in a \(\mathcal{L}_2\)-basis in the following way:

\[\beta(t)=\sum_k \beta_k \theta_k(t), \mathbf{X}(t)=\sum_k c_i \psi_k(t)\]

\begin{itemize}
\tightlist
\item
  \texttt{fregre.basis()} fucntion uses fixed basis: B--spline, Fourier, etc. \citet{RS2005}, \citet{Cardot_1999})
\end{itemize}

The next code illustrates how to estimate the fat contents using a sample of absorbances curves.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rangett }\OtherTok{\textless{}{-}}\NormalTok{ X}\SpecialCharTok{$}\NormalTok{rangeval}
\NormalTok{basis1 }\OtherTok{=} \FunctionTok{create.bspline.basis}\NormalTok{(}\AttributeTok{rangeval =}\NormalTok{ rangett, }\AttributeTok{nbasis =} \DecValTok{17}\NormalTok{)}
\NormalTok{basis2 }\OtherTok{=} \FunctionTok{create.bspline.basis}\NormalTok{(}\AttributeTok{rangeval =}\NormalTok{ rangett, }\AttributeTok{nbasis =} \DecValTok{7}\NormalTok{)}
\NormalTok{res.basis0 }\OtherTok{=} \FunctionTok{fregre.basis}\NormalTok{(X, y, }\AttributeTok{basis.x =}\NormalTok{ basis1, }\AttributeTok{basis.b =}\NormalTok{ basis2)}
\NormalTok{res.basis1 }\OtherTok{=} \FunctionTok{fregre.basis}\NormalTok{(X.d1, y, }\AttributeTok{basis.x =}\NormalTok{ basis1, }\AttributeTok{basis.b =}\NormalTok{ basis2)}
\NormalTok{res.basis2 }\OtherTok{=} \FunctionTok{fregre.basis}\NormalTok{(X.d2, y, }\AttributeTok{basis.x =}\NormalTok{ basis1, }\AttributeTok{basis.b =}\NormalTok{ basis2)}
\NormalTok{res.basis0}\SpecialCharTok{$}\NormalTok{r2;res.basis1}\SpecialCharTok{$}\NormalTok{r2;res.basis2}\SpecialCharTok{$}\NormalTok{r2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9385496
\end{verbatim}

\begin{verbatim}
## [1] 0.9360606
\end{verbatim}

\begin{verbatim}
## [1] 0.9518397
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(res.basis2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  *** Summary Functional Data Regression with representation in Basis *** 
## 
## Call:
## fregre.basis(fdataobj = X.d2, y = y, basis.x = basis1, basis.b = basis2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.9498 -1.5962 -0.2428  1.8891  6.1841 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   1.759e+01  2.636e-01  66.723  < 2e-16 ***
## X.d2.bspl4.1 -1.294e+04  3.849e+03  -3.361 0.001040 ** 
## X.d2.bspl4.2  9.261e+03  2.901e+03   3.192 0.001801 ** 
## X.d2.bspl4.3 -1.215e+03  1.426e+03  -0.852 0.395973    
## X.d2.bspl4.4  9.804e+02  1.092e+03   0.897 0.371275    
## X.d2.bspl4.5 -1.599e+03  1.126e+03  -1.420 0.158232    
## X.d2.bspl4.6  6.896e+03  1.802e+03   3.826 0.000207 ***
## X.d2.bspl4.7 -7.985e+03  1.438e+03  -5.554 1.69e-07 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.994 on 121 degrees of freedom
## Multiple R-squared:  0.9518, Adjusted R-squared:  0.9491 
## F-statistic: 341.6 on 7 and 121 DF,  p-value: < 2.2e-16
## 
## -Names of possible atypical curves: No atypical curves 
## -Names of possible influence curves: 140
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-27-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(res.basis0}\SpecialCharTok{$}\NormalTok{beta.est)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "done"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(res.basis1}\SpecialCharTok{$}\NormalTok{beta.est)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "done"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(res.basis2}\SpecialCharTok{$}\NormalTok{beta.est)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-27-2} \end{center}

\begin{verbatim}
## [1] "done"
\end{verbatim}

The choice of the appropiate basis (and the number of basis elements) becomes now in a crucial step:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.basis.cv }\OtherTok{=} \FunctionTok{fregre.basis}\NormalTok{(X, y)}
\FunctionTok{summary}\NormalTok{(res.basis.cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  *** Summary Functional Data Regression with representation in Basis *** 
## 
## Call:
## fregre.basis(fdataobj = X, y = y)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.3657  -2.1421  -0.1094   2.1998   6.5177 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   17.5884     0.2757  63.790   <2e-16 ***
## X.bspl4.1   -200.0776    93.7095  -2.135   0.0348 *  
## X.bspl4.2    242.9587   112.0151   2.169   0.0321 *  
## X.bspl4.3   -133.6675    72.6212  -1.841   0.0682 .  
## X.bspl4.4     23.7801    33.6901   0.706   0.4817    
## X.bspl4.5     14.2783    19.4473   0.734   0.4643    
## X.bspl4.6    -23.9989    16.8771  -1.422   0.1577    
## X.bspl4.7     46.5994    27.9568   1.667   0.0982 .  
## X.bspl4.8   -104.1480    65.9443  -1.579   0.1169    
## X.bspl4.9    154.0717   108.4606   1.421   0.1581    
## X.bspl4.10  -123.4221    94.1397  -1.311   0.1924    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.132 on 118 degrees of freedom
## Multiple R-squared:  0.9486, Adjusted R-squared:  0.9443 
## F-statistic: 217.8 on 10 and 118 DF,  p-value: < 2.2e-16
## 
## -Names of possible atypical curves: 43 
## -Names of possible influence curves: 86 140 43 6
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-28-1} \end{center}

\begin{itemize}
\tightlist
\item
  Functional Principal Components (FPC).\citep{Cardot_1999}, \texttt{fregre.pc()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OtherTok{\textless{}{-}}\NormalTok{X}
\NormalTok{basis.pc0 }\OtherTok{=} \FunctionTok{create.pc.basis}\NormalTok{(X,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\NormalTok{res.pc1 }\OtherTok{=} \FunctionTok{fregre.pc}\NormalTok{(X, y, }\AttributeTok{basis.x =}\NormalTok{ basis.pc)}
\FunctionTok{summary}\NormalTok{(res.pc1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  *** Summary Functional Data Regression with Principal Components ***
## 
## Call:
## fregre.pc(fdataobj = X, y = y, basis.x = basis.pc)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -24.471  -4.389   0.886   5.328  14.585 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  17.58837    0.72555  24.241   <2e-16 ***
## PC1           0.99689    0.09838  10.133   <2e-16 ***
## PC2          -1.50819    1.18685  -1.271    0.206    
## PC3         -21.84347    1.90208 -11.484   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 8.241 on 125 degrees of freedom
## Multiple R-squared:  0.6231, Adjusted R-squared:  0.614 
## F-statistic: 68.88 on 3 and 125 DF,  p-value: < 2.2e-16
## 
## 
## -With 3 Principal Components is  explained  99.47 %
##  of the variability of explicative variables. 
## 
## -Variability for each  principal components -PC- (%):
##   PC1   PC2   PC3 
## 98.54  0.66  0.26 
## -Names of possible atypical curves: No atypical curves 
## -Names of possible influence curves: 18 185 99 140 44
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-29-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.pc2 }\OtherTok{=} \FunctionTok{fregre.pc.cv}\NormalTok{(X, y)}
\FunctionTok{summary}\NormalTok{(res.pc2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            Length Class     Mode   
## fregre.pc  19     fregre.fd list   
## pc.opt      5     -none-    numeric
## lambda.opt  1     -none-    numeric
## PC.order    8     -none-    numeric
## MSC.order   8     -none-    numeric
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(res.pc1}\SpecialCharTok{$}\NormalTok{beta.est)}
\FunctionTok{plot}\NormalTok{(res.pc2[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{beta.est)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-29-2} \end{center}

\hypertarget{flm-with-functional-and-non-functional-covariates}{%
\section{FLM with functional and non functional covariates}\label{flm-with-functional-and-non-functional-covariates}}

\[E(y)=\alpha+\mathbf{Z}\beta+\sum_{q=1}^Q \left\langle \mathcal{X}^{q}(t),\beta_{q}(t)\right\rangle
\]

where \(\left\{\mathcal{X}_q(t)\right\}_{q=1}^Q\) are function covariates and \(\mathbf{Z}=\left\{{Z_j}\right\}_{j=1}^J\) the non--functional covariates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataf }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(tecator[[}\StringTok{"y"}\NormalTok{]][ind,]) }\CommentTok{\# Fat, Protein, Water}
\NormalTok{basis.pc2 }\OtherTok{=} \FunctionTok{create.pc.basis}\NormalTok{(X.d2,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)}
\NormalTok{basis.x }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\AttributeTok{X =}\NormalTok{ basis.pc0, }\AttributeTok{X.d2 =}\NormalTok{basis.pc2)}
\NormalTok{f }\OtherTok{=}\NormalTok{ Fat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X}\SpecialCharTok{+}\NormalTok{X.d2}
\NormalTok{ldata }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\AttributeTok{df =}\NormalTok{ dataf, }\AttributeTok{X=}\NormalTok{X,}\AttributeTok{X.d2=}\NormalTok{X.d2)}
\NormalTok{res.lm1 }\OtherTok{=} \FunctionTok{fregre.lm}\NormalTok{(f, ldata, }\AttributeTok{basis.x =}\NormalTok{ basis.x)}
\NormalTok{f }\OtherTok{=}\NormalTok{ Fat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Water}\SpecialCharTok{+}\NormalTok{X.d2}
\NormalTok{res.lm2 }\OtherTok{=} \FunctionTok{fregre.lm}\NormalTok{(f, ldata, }\AttributeTok{basis.x =}\NormalTok{ basis.x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = pf, data = XX, x = TRUE)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.9417 -1.6197 -0.2995  1.5864  9.4955 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    17.5884     0.2659  66.142  < 2e-16 ***
## X.PC1           0.1127     0.1023   1.102  0.27283    
## X.PC2           7.1807     3.2173   2.232  0.02746 *  
## X.PC3         -19.8307     6.9644  -2.847  0.00518 ** 
## X.d2.PC1     3066.6836   563.3407   5.444 2.78e-07 ***
## X.d2.PC2     5507.7858  2668.4922   2.064  0.04115 *  
## X.d2.PC3     1879.3468  1017.4538   1.847  0.06717 .  
## X.d2.PC4    -2644.9925  3131.4915  -0.845  0.39998    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.02 on 121 degrees of freedom
## Multiple R-squared:  0.951,  Adjusted R-squared:  0.9482 
## F-statistic: 335.4 on 7 and 121 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-31-1} \end{center}

\hypertarget{predict-method-for-functional-regression-model}{%
\subsection{Predict method for functional regression model}\label{predict-method-for-functional-regression-model}}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-32-1} \end{center}

\hypertarget{other-procedures}{%
\section{Other procedures}\label{other-procedures}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Other procedures
\end{enumerate}

\begin{itemize}
\item
  Partial Least Squares (FPLS). \texttt{fregre.pls()}, \citep[\citet{pedra2005}]{kramer}
\item
  Penalized versions and parameter selection: \texttt{fregre.pc.cv}, \texttt{fregre.basis.cv}, \texttt{fregre.np.cv} \citep{Febrero2012}
\item
  F-test for the FLM with scalar response: \texttt{flm.Ftest}, \texttt{F-test} \citep{garcia2014goodness}
\item
  Goodness-of-fit test for the FLM with scalar response: \texttt{flm.test} \citep{garcia2014goodness}
\item
  Measures of influence in FLM with scalar response: \texttt{influence.fdata},\citep{Febrero2010}
\item
  Beta parameter estimation by wild or smoothed bootstrap procedure: \texttt{fregre.bootstrap}
\item
  FLM with a functional response: \texttt{fregre.basis.fr} \citep{chiou2004functional}
\end{itemize}

\hypertarget{non-linear-model-fv2006}{%
\section{\texorpdfstring{Non Linear Model \citep{FV2006}}{Non Linear Model {[}@FV2006{]}}}\label{non-linear-model-fv2006}}

Supose \((\mathcal{X},Y)\) are a pair of r.v. with \(y\in \mathbb{R}\) where \(\mathbb{E}\) is a semi-metric space. To predict the resonse \(Y\) with \(\mathcal{X}\), the estimation is:

\[m(\mathcal{X})=\mathbb{E}(Y|X=\mathcal{X})\], where the NW estimator is given by:

\[\hat{m}(\mathcal{X})=\frac{\sum_{i=1}^n Y_i{ K(d(\mathcal{X},X_i)/h)}}{\sum_{i=1}^n  {K(d(\mathcal{X},X_i)/h)}}\]

where K is an asymmetric kernel function and h is the bandwidth parameter.

\hypertarget{semi-linear-model-aneiros2005}{%
\section{\texorpdfstring{Semi Linear Model \citep{Aneiros2005}}{Semi Linear Model {[}@Aneiros2005{]}}}\label{semi-linear-model-aneiros2005}}

Let \((\mathcal{X},\mathbf{Z},y)\) with \(y\in \mathbb{R}\) (response), \(\mathcal{X}\in \mathbb{E}\) (functional) and \(\mathbf{Z} \in \mathbb{R}^p\) (MV covariates).

\[y = Z + m(X) + \varepsilon\]

Arguments for fregre.np() and fregre.plm() function

\begin{itemize}
\tightlist
\item
  --Ker--: type of asymmetric kernel function, by default asymmetric normal kernel (cosine, epanechnicov, quadratic,\ldots.).
\item
  --metric--: type of metric or semimetric.
  --type.S--: type of smoothing matrix \(\mathbf{S}\): \texttt{S.NW}, \texttt{S.LLR}, \texttt{S.KNN}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tecator}\OtherTok{\textless{}{-}}\FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{y,}\StringTok{"absorp.fdata"}\OtherTok{=}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{absorp.fdata)}
\NormalTok{X}\OtherTok{=}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{absorp.fdata}
\NormalTok{y}\OtherTok{\textless{}{-}}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{Fat}

\NormalTok{np}\OtherTok{\textless{}{-}}\FunctionTok{fregre.np}\NormalTok{(X, y, }\AttributeTok{metric =}\NormalTok{ semimetric.deriv, }\AttributeTok{nderiv =} \DecValTok{1}\NormalTok{,}\AttributeTok{type.S =}\NormalTok{ S.KNN)}
\end{Highlighting}
\end{Shaded}

Again, it has also implemented the function \texttt{fregre.np.cv} to estimate the smoothing parameter \(h\) by the validation criteria.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np}\OtherTok{\textless{}{-}}\FunctionTok{fregre.np}\NormalTok{(X, y, }\AttributeTok{metric =}\NormalTok{ semimetric.deriv, }\AttributeTok{nderiv =} \DecValTok{1}\NormalTok{,}\AttributeTok{type.S =}\NormalTok{ S.KNN)}
\NormalTok{np.cv}\OtherTok{\textless{}{-}}\FunctionTok{fregre.np.cv}\NormalTok{(X, y, }\AttributeTok{metric =}\NormalTok{ semimetric.deriv, }\AttributeTok{nderiv =} \DecValTok{1}\NormalTok{,}\AttributeTok{type.S =}\NormalTok{ S.KNN,}\AttributeTok{h=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{:}\DecValTok{9}\NormalTok{))}
\FunctionTok{c}\NormalTok{(np}\SpecialCharTok{$}\NormalTok{h.opt,np.cv}\SpecialCharTok{$}\NormalTok{h.opt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{c}\NormalTok{(np}\SpecialCharTok{$}\NormalTok{r2,np.cv}\SpecialCharTok{$}\NormalTok{r2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8687145 0.9438220
\end{verbatim}

\hypertarget{generalized-linear-models}{%
\section{Generalized Linear Models}\label{generalized-linear-models}}

One natural extension of LM model is the generalized functional linear regression model (GFLM) which allows various types of the response. In the GLM framework it is generally assumed that \(y_i|X_i\) can be chosen within the set of distributions belonging to the exponential family \citep{muller2005generalized}.

In Generalized Functional Linear Model (FGLM), The scalar response \(y\)(belonging to a Exponential Family PDF) is estimated by functional \(\left\{\mathcal{X}_q(t)\right\}_{q=1}^Q\) and also non--functional \(\mathbf{Z}=\left\{{Z_j}\right\}_{j=1}^J\) covariates by:

\[E(y)=g^{-1}\left(\alpha+\mathbf{Z}\beta+\sum_{q=1}^Q \left\langle \mathcal{X}^{q}(t),\beta_{q}(t)\right\rangle\right) 
\]
where \(g()\) is the inverse link function.

\begin{quote}
Example of logistic regression
\end{quote}

In logistic regression, the probability, \(\pi_i\) , of the occurrence of an event, \(Y_i = 1\), rather than the event \(Y_i = 0\), conditional on a vector of covariates \(\mathcal{X}_i(t)\) is expressed as:

\[ p_i = \mathbb{P}[Y = 1|{X_i(t): t \in T }]=\frac{+exp\left\{\alpha+\int_{T}X_{i}(t)\beta(t)dt \right\}}{1+exp\left\{\alpha+\int_{T}X_{i}(t)\beta(t)dt \right\}}\ , i= 1,\ldots,n\]

with \(\epsilon\) are the independent errors with zero mean.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(tecator)}
\FunctionTok{names}\NormalTok{(tecator)[}\DecValTok{2}\NormalTok{]}\OtherTok{\textless{}{-}}\StringTok{"df"}
\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{fat15}\OtherTok{\textless{}{-}}\FunctionTok{ifelse}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{Fat}\SpecialCharTok{\textless{}}\DecValTok{15}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{absorp.d2}\OtherTok{=}\FunctionTok{fdata.deriv}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{absorp.fdata,}\AttributeTok{nderiv=}\DecValTok{2}\NormalTok{)}
\NormalTok{res.glm}\OtherTok{\textless{}{-}}\FunctionTok{fregre.glm}\NormalTok{(fat15 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ absorp.d2,}\AttributeTok{data=}\NormalTok{tecator,}\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{())}
\CommentTok{\#summary(a)}
\NormalTok{yfit}\OtherTok{\textless{}{-}}\FunctionTok{ifelse}\NormalTok{(res.glm}\SpecialCharTok{$}\NormalTok{fitted.values}\SpecialCharTok{\textless{}}\NormalTok{.}\DecValTok{5}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\FunctionTok{table}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{fat15,yfit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    yfit
##       0   1
##   0 109   3
##   1   2 101
\end{verbatim}

\hypertarget{generalized-functional-additive-model}{%
\section{Generalized Functional Additive Model}\label{generalized-functional-additive-model}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generalized Functional Spectral Additive Linear Model (FGSAM), \citep{muller2012functional}
\end{enumerate}

\[E(y)=g^{-1}\left(\alpha+\sum_{j=1}^J f_{j}\left(\mathbf{Z}^{j}\right)+\sum_{q=1}^Q s_q\left(\mathcal{X}_{i}^{q}(t)\right)\right)\]

where \({f}(\cdot),{s}(\cdot)\) are the smoothed functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.gsam}\OtherTok{\textless{}{-}}\FunctionTok{fregre.gsam}\NormalTok{(fat15}\SpecialCharTok{\textasciitilde{}} \FunctionTok{s}\NormalTok{(absorp.d2),}\AttributeTok{data=}\NormalTok{tecator,}\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{())}
\NormalTok{yfit}\OtherTok{\textless{}{-}}\FunctionTok{ifelse}\NormalTok{(res.gsam}\SpecialCharTok{$}\NormalTok{fitted}\SpecialCharTok{\textless{}}\NormalTok{.}\DecValTok{5}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\FunctionTok{table}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{fat15,yfit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    yfit
##       0   1
##   0 112   0
##   1   0 103
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Generalized Functional Kernel Additive Linear Model (FGKAM), \citep{Febrero2013}
\end{enumerate}

\[E(y)=g^{-1}\left(\alpha+\sum_{q=1}^Q\mathcal{K}\left(\mathcal{X}^{q}_i(t)\right)\right)\]
where \(\mathcal{K}(\cdot)\) is the kernel estimator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# tecator2\textless{}{-}tecator[{-}1]}
\CommentTok{\# tecator$df$fat15 \textless{}{-} as.factor(tecator$df$fat15)}
\CommentTok{\# res.gkam\textless{}{-}fregre.gkam(fat15 \textasciitilde{} absorp.d2,data=tecator2, family=binomial(),}
\CommentTok{\#             control = list(maxit = 1))}
\CommentTok{\# res.gkam}
\CommentTok{\# yfit\textless{}{-}ifelse(res.gkam$fitted.values\textless{}.5,0,1)}
\CommentTok{\# table(tecator$df$fat15,yfit)}
\end{Highlighting}
\end{Shaded}

\hypertarget{functional-gls-model}{%
\section{Functional GLS model}\label{functional-gls-model}}

See \citet{Oviedo2018} for more details about the below algorithm:

A. Jointly estimation (nlme package): Minimize for \((\beta,\theta)\) the GLS criteria, i.e,

\[\Psi(\beta,\theta)=\left(y-\left\langle  X,\beta \right\rangle\right)\Sigma(\theta)^{-1}\left(y-\left\langle  X,\beta \right\rangle\right)\]

B. Iterative Estimation: In multivariate case, \citet{zivot2006modeling} show that estimation of \(\beta\) by \(\hat{\beta}_{ML}\) is equivalent to the iterative estimation of \(\hat{\beta}\) recomputed at each iteration by the update estimator of \(\Sigma\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Begin with a preliminary estimation of \(\hat{\theta}=\theta_0\). Compute \(\hat{W}=\Sigma(\theta_0)^{-1}\).
\item
  Estimate \({b}_\Sigma={(Z^\prime\hat{W}Z)^{-1}Z^\prime\hat{W}}y\)
\item
  Based on \(\hat{e}=({y-{Z}{b}_\Sigma})\), update \(\hat{\theta}=\rho({\hat{e}})\) where \(\rho\) depends on the dependence structure chosen.
\item
  Repeat steps 2 and 3 until convergence.
\end{enumerate}

The generalized correlated cross-validation (GCCV) criterion is an extension to GCV within the context of correlated errors, \citet{carmack2012generalised}. It is defined as follows:

\[GCCV(K_x,K_\beta,\mathbf{b},\phi)=\frac{\sum_{i=1}^n \left(y_{i}-\hat{y}_{i,\mathbf{b}}\right)^2}{
    \left({1-\frac{{tr}(\mathbf{G})}{n}}\right)^2} \]

where \({G}=2{H}\Sigma(\phi)-{H}\Sigma(\phi)H^\prime\) takes into account the effect of the dependence, the trace of \({G}\) is an estimation of the degrees of freedom consumed by the model and \({H}\) is the hat matrix.

The important advantage of this criterion is that it is rather easy to compute because it avoids the need to compute the inverse of the matrix \(\Sigma\). Even so, the complexity of the GLS criterion depends on the structure of \(\Sigma\) and it could sometimes be hard either to minimize or computationally expensive.

\hypertarget{dependent-data-example}{%
\subsection{Dependent data example,}\label{dependent-data-example}}

We use the \texttt{fregre.gls()} function that has the same arguments as the \texttt{fregre.lm()} function and: \textbf{correlation} argument, same functionality as in \texttt{gls()} and \textbf{criteria} argument, it require \texttt{GCCV.S()} function to calculate the GCCV score proposed by \citet{carmack2012generalised}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(tecator)}
\FunctionTok{ts.plot}\NormalTok{(tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Fat"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-38-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Fat"}\NormalTok{,}\AttributeTok{drop=}\NormalTok{F],tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Water"}\NormalTok{,}\AttributeTok{drop=}\NormalTok{F])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          Water
## Fat -0.9881002
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Fat"}\NormalTok{,}\AttributeTok{drop=}\NormalTok{F],tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Protein"}\NormalTok{,}\AttributeTok{drop=}\NormalTok{F])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Protein
## Fat -0.8608965
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dcor.xy}\NormalTok{(tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Fat"}\NormalTok{,}\AttributeTok{drop=}\NormalTok{F],tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Water"}\NormalTok{,}\AttributeTok{drop=}\NormalTok{F])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  dcor t-test of independence
## 
## data:  D1 and D2
## T = 571.71, df = 22789, p-value < 2.2e-16
## sample estimates:
## Bias corrected dcor 
##           0.9668619
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dcor.xy}\NormalTok{(tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Fat"}\NormalTok{,}\AttributeTok{drop=}\NormalTok{F],tecator[[}\StringTok{"y"}\NormalTok{]][,}\StringTok{"Protein"}\NormalTok{,}\AttributeTok{drop=}\NormalTok{F])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  dcor t-test of independence
## 
## data:  D1 and D2
## T = 155.43, df = 22789, p-value < 2.2e-16
## sample estimates:
## Bias corrected dcor 
##           0.7173448
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x.d2}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(tecator[[}\StringTok{"absorp.fdata"}\NormalTok{]],}\AttributeTok{nderiv=}\DecValTok{2}\NormalTok{)}
\NormalTok{ldata}\OtherTok{=}\FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\NormalTok{tecator[[}\StringTok{"y"}\NormalTok{]],}\StringTok{"x.d2"}\OtherTok{=}\NormalTok{x.d2)}

\NormalTok{res.gls}\OtherTok{=}\FunctionTok{fregre.gls}\NormalTok{(Fat}\SpecialCharTok{\textasciitilde{}}\NormalTok{x.d2, }\AttributeTok{data=}\NormalTok{ldata, }\AttributeTok{correlation=}\FunctionTok{corAR1}\NormalTok{())}
\FunctionTok{coef}\NormalTok{(res.gls[[}\StringTok{"modelStruct"}\NormalTok{]],F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## corStruct.Phi 
##     0.4942661
\end{verbatim}

The previous model is restricted to a structure determined by \texttt{gls()} function of \textbf{nlme} The function \texttt{fregre.igls()} is presented as an alternative because it allows any type of dependence structures designed by the user.

The code bellow shows a simple use of iterative scheme (iGLS). In particular, we use a iGLS-AR(\(p=1\)) scheme for error estimation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.igls}\OtherTok{=}\FunctionTok{fregre.igls}\NormalTok{(Fat}\SpecialCharTok{\textasciitilde{}}\NormalTok{x.d2, }\AttributeTok{data=}\NormalTok{ldata, }\AttributeTok{correlation=}\FunctionTok{list}\NormalTok{(}\StringTok{"cor.ARMA"}\OtherTok{=}\FunctionTok{list}\NormalTok{()),}\AttributeTok{control=}\FunctionTok{list}\NormalTok{(}\StringTok{"p"}\OtherTok{=}\DecValTok{1}\NormalTok{))}
\FunctionTok{coef}\NormalTok{(res.igls[[}\StringTok{"corStruct"}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      ar1 
## 0.488854
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.igls}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## list("fregre.basis")
## 
## Coefficients:
##  (Intercept)  x.d2.bspl4.1  x.d2.bspl4.2  x.d2.bspl4.3  x.d2.bspl4.4  
##        18.12       -608.20       6203.09      -8252.76       6271.43  
## x.d2.bspl4.5  
##     -7156.85
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.igls}\SpecialCharTok{$}\NormalTok{corStruct}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $ar
## 
## Call:
## arima(x = x, order = c(p, d, q), include.mean = FALSE, transform.pars = TRUE)
## 
## Coefficients:
##          ar1
##       0.4889
## s.e.  0.0600
## 
## sigma^2 estimated as 8.076:  log likelihood = -529.76,  aic = 1063.53
\end{verbatim}

Both examples estimate an AR(1) with \(\phi=0.49\). Thus, the estimation and the prediction made with these models will be more accurate than the classical functional models in which it is assumed that the errors are independent.

\hypertarget{functional-response-model}{%
\subsection{Functional Response Model}\label{functional-response-model}}

Reference papers: \citet{faraway1997regression}, \citet{ferraty2012regression}

R expample of function \texttt{fregre.basis.fr()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(aemet)}
\NormalTok{log10precfdata}\OtherTok{\textless{}{-}}\NormalTok{aemet}\SpecialCharTok{$}\NormalTok{logprec; tempfdata}\OtherTok{\textless{}{-}}\NormalTok{aemet}\SpecialCharTok{$}\NormalTok{temp}
\NormalTok{res2}\OtherTok{\textless{}{-}}\FunctionTok{fregre.basis.fr}\NormalTok{(tempfdata,log10precfdata)}
\NormalTok{i}\OtherTok{\textless{}{-}}\DecValTok{1}

\FunctionTok{plot}\NormalTok{(log10precfdata[i],}\AttributeTok{lty=}\DecValTok{1}\NormalTok{,}\AttributeTok{main=}\FunctionTok{paste0}\NormalTok{(}\StringTok{"Weather station, "}\NormalTok{,i))}
\FunctionTok{lines}\NormalTok{(res2}\SpecialCharTok{$}\NormalTok{fitted.values[i],}\AttributeTok{lty=}\DecValTok{2}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{col=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-40-1} \end{center}

\hypertarget{other-models}{%
\subsection{Other Models:}\label{other-models}}

\begin{itemize}
\tightlist
\item
  Functional Quantile Regession Model, see \citet{kato2012estimation}, \citet{cardot2005quantile}.
\item
  Functional Single Index Model, see \citet{ferraty2011estimation}.
\item
  Functional Projection Pursuit Regression Model, see \citet{ferraty2013functional}.
\item
  Functional Machine Learning methods (SVM, RPART, NNET, random Forest)
\end{itemize}

\hypertarget{references-1}{%
\section{References}\label{references-1}}

\hypertarget{functional-supervised-classification}{%
\chapter{Functional Supervised Classification}\label{functional-supervised-classification}}

This section describes the usage of functional classification using fda.usc package in R.

Let a sample \((\mathcal{X},Y)\in E \times \mathbb{G}={1,\cdots,G}\).

Aim: How predict the class \(g\) of Y (categorical variable) given a functional variable \(\mathcal{X}\)

Bayes rule: Estimate the posterior probability of belonging to each group:

\[p_g(X)=\mathbb{P}(Y=g | \mathcal{X}=\chi)=\mathbb{E}(1_{Y=g} |\mathcal{X}=\chi)\]

The predicted class is given by the Bayes rule,

\[\hat{Y}=\arg \max_{g\in \mathbb{G}} \hat{p}_g(\chi)\]

The package allows the estimation of the groups in a training set of functional data by

\begin{itemize}
\tightlist
\item
  Logistic Classifier (linear model): \texttt{classif.glm}
\item
  Logistic Classifier (additive model): \texttt{classif.gsam} and \texttt{classif.gkam}
\item
  k-Nearest Neighbor Classifier: \texttt{classif.knn}
\item
  Kernel Classifier: \texttt{classif.kernel}
\item
  Distance Classifier: `classif.distv
\item
  Maximum Depth Classifier: \texttt{classif.depth}
\item
  DD Clasisifier: \texttt{classif.DD}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fda.usc)}
\FunctionTok{data}\NormalTok{(tecator)}
\NormalTok{x}\OtherTok{=}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{absorp.fdata}
\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{$}\NormalTok{Fat}\OtherTok{\textless{}{-}}\FunctionTok{ifelse}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{$}\NormalTok{Fat}\SpecialCharTok{\textgreater{}}\DecValTok{20}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)}

\NormalTok{x.d1}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(x)}
\NormalTok{dataf}\OtherTok{=}\FunctionTok{as.data.frame}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y)}
\NormalTok{ldata}\OtherTok{=}\FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\NormalTok{dataf,}\StringTok{"x"}\OtherTok{=}\NormalTok{x,}\StringTok{"x.d1"}\OtherTok{=}\NormalTok{x.d1)}
\NormalTok{ycat}\OtherTok{\textless{}{-}}\NormalTok{ldata}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{Fat}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression-model-glm-classif.glm}{%
\section{\texorpdfstring{Logistic Regression Model (GLM): \texttt{classif.glm}}{Logistic Regression Model (GLM): classif.glm}}\label{logistic-regression-model-glm-classif.glm}}

As a particular case of Generalized Linear Models, the logistic regression model models the posterior probability given \(\mathbf{d}\) as

\[  p(Y=i|\mathcal{X}(t))=\log \left(\frac{p(Y=i|\mathcal{X}(t))}{1-p(Y=i|\mathcal{X}(t))}\right)=\alpha_i+ \left\langle \mathcal{X}_i(t),\beta_{i}(t)\right\rangle\]

where the curve \(\mathcal{X}(t)\) is assigned to class \(i\) if \(p(i|\mathcal{X})>p(j|\mathcal{X}), j=1,\cdots, g, j\ne i\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.bin}\OtherTok{=}\FunctionTok{fregre.glm}\NormalTok{(Fat}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,ldata,}\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{())}
\NormalTok{res.gsam}\OtherTok{\textless{}{-}}\FunctionTok{classif.glm}\NormalTok{(Fat}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data=}\NormalTok{ldata)}
\FunctionTok{summary}\NormalTok{(res.gsam)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      - SUMMARY - 
## 
## -Probability of correct classification by group (prob.classification):
##         0         1 
## 0.9782609 0.9610390 
## 
## -Confusion matrix between the theoretical groups (by rows)
##   and estimated groups (by column) 
##    
##       0   1
##   0 135   3
##   1   3  74
## 
## -Probability of correct classification:  0.9721
\end{verbatim}

\hypertarget{generalized-additive-models-gam-classif.gsam-and-classif.gkam}{%
\section{\texorpdfstring{Generalized Additive Models (GAM): \texttt{classif.gsam} and \texttt{classif.gkam}}{Generalized Additive Models (GAM): classif.gsam and classif.gkam}}\label{generalized-additive-models-gam-classif.gsam-and-classif.gkam}}

Generalized Additive Models (see \citet{wood2004}) relax the linearity assumption in GLMs, allowing the use of a sum of general smooth functions \(f_j\) for the posterior probability; i.e.,

\[  p(Y=i|\mathcal{X}(t))=\log \left(\frac{p(Y=i|\mathcal{X}(t))}{1-p(Y=i|\mathcal{X}(t))}\right)=\alpha_i+  f_i\left(\mathcal{X}_{i}(t)\right)\]

where the functions \(f_{i}\) may belong to a known parametric family (polynomials, for instance) or they may even be functions to be estimated non-parametrically.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.gsam}\OtherTok{\textless{}{-}}\FunctionTok{classif.gsam}\NormalTok{(Fat}\SpecialCharTok{\textasciitilde{}}\FunctionTok{s}\NormalTok{(x),}\AttributeTok{data=}\NormalTok{ldata)}
\FunctionTok{summary}\NormalTok{(res.gsam)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      - SUMMARY - 
## 
## -Probability of correct classification by group (prob.classification):
##         0         1 
## 0.9855072 0.9610390 
## 
## -Confusion matrix between the theoretical groups (by rows)
##   and estimated groups (by column) 
##    
##       0   1
##   0 136   2
##   1   3  74
## 
## -Probability of correct classification:  0.9767
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#res.gkam\textless{}{-}classif.gkam(Fat\textasciitilde{}x,data=ldata)}
\end{Highlighting}
\end{Shaded}

\hypertarget{nonparametric-classification-methods-classif.knn-and-classif.np-ferraty2003}{%
\section{\texorpdfstring{Nonparametric classification methods: \texttt{classif.knn} and \texttt{classif.np} \citep{Ferraty2003}}{Nonparametric classification methods: classif.knn and classif.np {[}@Ferraty2003{]}}}\label{nonparametric-classification-methods-classif.knn-and-classif.np-ferraty2003}}

These methods are based on non-parametric estimates of the densities of the groups. The most simple (and classical) one is \(k\)--nearest neighbour (\(k\)NN) in which, given \(k \in \mathbb{N}\), the point \(\mathbf{d}\) is assigned to the class containing a majority of the \(k\) nearest data points in the training sample.

Another possibility is to estimate \(p(Y=g|\mathcal{X})\) through the Nadaraya--Watson estimator:

\[p(Y=g|\mathcal{X})=\frac{\sum_{n=1}^N \mathbf{1}_{G_n=g}\  K\left(m(\mathcal{X},\mathcal{X}_i(t))/h\right)}{\sum_{n=1}^N K\left(m(\mathcal{X}_i(t))/h\right)},\]

where \(N\) is the size of the training sample, \(G_n\) is the class of \(i\)-th curve in the training sample, \(K\) is a kernel and \(m(\cdot,\cdot )\) is a measure of closeness between two curves (a suitable distancewhich is re-scaled by the bandwidth parameter \(h\)) .

\begin{verbatim}
## y
##         0         1 
## 0.8550725 0.7142857
\end{verbatim}

A \(k\)NN method could be considered an NP method using the uniform kernel and a locally selected bandwidth.

\begin{verbatim}
## y
##         0         1 
## 0.7826087 0.7012987
\end{verbatim}

\hypertarget{maximum-depth-classif.depth-li2012}{%
\section{\texorpdfstring{Maximum depth: \texttt{classif.depth} \citep{Li2012}}{Maximum depth: classif.depth {[}@Li2012{]}}}\label{maximum-depth-classif.depth-li2012}}

The most basic rule is to assign a new observation \(x_0\) to the group that provides the highest depth to that observation (Maximum depth (MD)). The maximum depth classifier was the first attempt to use data depths instead of multivariate raw data to construct a classification rule.

\textbf{Perform the following example with hidden code}

Given a sample depth measure and a new observation \(x_0\) (use the \texttt{xx.d1} curves):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Evaluate the depth of \(x_0\) in both sub-samples defined by \texttt{ycat} variable (only the first 10 values are printed)
\end{enumerate}

\begin{verbatim}
## Depth g1 0.4811594 0.164058 0.5717391 0.5602899 0.4068116 0.127971 0.1237681 0.7730435 0.3373913 0.2971014
\end{verbatim}

\begin{verbatim}
## Depth g2 0.4811594 0.164058 0.5717391 0.5602899 0.4068116 0.127971 0.1237681 0.7730435 0.3373913 0.2971014
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Assign \(x_0\) according to the data set where it is more deeply placed.
\end{enumerate}

\begin{verbatim}
## group.est: 0 1 0 0 1 1 1 0 1 1
\end{verbatim}

\begin{verbatim}
## ycat     : 1 1 0 0 1 1 1 0 0 0
\end{verbatim}

The function \texttt{classif.depth} performs previous tasks:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.depth}\OtherTok{\textless{}{-}}\FunctionTok{classif.depth}\NormalTok{(ycat,x.d1,}\AttributeTok{depth=}\StringTok{"FM"}\NormalTok{)}
\FunctionTok{data.frame}\NormalTok{(res.depth}\SpecialCharTok{$}\NormalTok{dep,group.est,res.depth}\SpecialCharTok{$}\NormalTok{dep}\SpecialCharTok{{-}}\FunctionTok{cbind}\NormalTok{(d1,d2))[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           X1        X2 group.est d1 d2
## 1  0.4811594 0.4051948         0  0  0
## 2  0.1640580 0.4766234         1  0  0
## 3  0.5717391 0.2659740         0  0  0
## 4  0.5602899 0.2522078         0  0  0
## 5  0.4068116 0.5083117         1  0  0
## 6  0.1279710 0.3948052         1  0  0
## 7  0.1237681 0.3254545         1  0  0
## 8  0.7730435 0.3135065         0  0  0
## 9  0.3373913 0.4568831         1  0  0
## 10 0.2971014 0.4238961         1  0  0
\end{verbatim}

\hypertarget{the-ddgclassifier-classif.dd}{%
\section{\texorpdfstring{The DD\(^G\)--classifier \texttt{classif.DD}}{The DD\^{}G--classifier classif.DD}}\label{the-ddgclassifier-classif.dd}}

Suppose that we have implementations of a process in the product space \(\mathcal{X}=\mathcal{X}_1\times\cdots\times\mathcal{X}_p\) (multivariate (functional) data) where we have \(g\) groups (classes or distributions) to be separated using data depths, \citep{cuesta2017}. The DD\(^G\)--classifier begins by selecting a depth \(D\) and computing the following map (for \(p=1\)):

\[
\mathcal{ X} \rightarrow  \mathbb{R}^g
\\
x  \rightarrow \mathbb{d}=({D}_1(x),\cdots,{D}_g(x)).
\]

We can now apply any available classification procedure that works in a \(g\)--dimensional space to separate the \(g\) groups.

where \(D_0k(x)\) is the depth of x with respect to the group \(k = 1,\cdots,g\).
So, the DDG -Classifier compresses the information of \({y_i,x_i}\) into a real space of dimension \((g + 1)\) with the form \(\left\{y_i,D_1(x_i),\cdots,D_g(x_i)\right\}\).

Classification techniques in \(\mathbb{R}^g\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear Discriminant Analysis (LDA)
\item
  Quadratic Discriminant Analysis (QDA)
\item
  Generalized Linear Models (GLM)
\item
  Generalized Additive Models (GAM)
\item
  k-Nearest Neighbors (kNN)
\item
  Kernel Classification Method (NP)
\item
  Classification Trees (Tree)
\item
  ANN, SVMs, \ldots{}
\end{enumerate}

The aim of the DD-classifier (\citep{Li2012}) is to extend the Maximum depth classifier using a polynomial up to degree k passing through the origin as classification rule.

The DD--classifier has resolved several serious limitations of the maximum depth classifier .

Properties of the DDG -classifier:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A lot of classification methods available (All in the multivariate framework)
\item
  Using classical classification methods in the DD-plot can provide useful insights about what's going on (which depths are influential or probabilities of belonging to a certain group).
\item
  Possible reduction in the dimension of the classification problem, specially interesting in the Functional Framework (or in High Dimensional problems).
\item
  No matters how complex is the space to be analyzed, only matters that a depth function can be defined (for example, multivariate functional data MFD: \(\mathcal{X}=\mathcal{X}_1\times\cdots\times\mathcal{X}_p\).
\end{enumerate}

\begin{quote}
Example DD with 2 groups
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.DD}\OtherTok{\textless{}{-}}\FunctionTok{classif.DD}\NormalTok{(ycat,x.d1,}\AttributeTok{classif=}\StringTok{"gam"}\NormalTok{,}\AttributeTok{depth=}\StringTok{"mode"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-50-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.depth}\SpecialCharTok{$}\NormalTok{prob.classification}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## group
##         0         1 
## 0.8260870 0.9350649
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res.DD}\SpecialCharTok{$}\NormalTok{prob.classification}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## group
##         0         1 
## 0.9782609 0.9610390
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#res.DD}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Example dDD with G groups
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#ycat\textless{}{-}cut(ldata$df$Fat,3,labels=1:3) }
\CommentTok{\# DD{-}classif for functional data: G levels }
\FunctionTok{data}\NormalTok{(phoneme)}
\NormalTok{mlearn}\OtherTok{\textless{}{-}}\NormalTok{phoneme[[}\StringTok{"learn"}\NormalTok{]]}
\NormalTok{mlearn2}\OtherTok{\textless{}{-}}\NormalTok{phoneme[[}\StringTok{"test"}\NormalTok{]]}
\NormalTok{glearn}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(phoneme[[}\StringTok{"classlearn"}\NormalTok{]])}\SpecialCharTok{{-}}\DecValTok{1}
\NormalTok{out20}\OtherTok{=}\FunctionTok{classif.DD}\NormalTok{(glearn,mlearn,}\AttributeTok{depth=}\StringTok{"mode"}\NormalTok{,}\AttributeTok{classif=}\StringTok{"glm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-51-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out21}\OtherTok{=}\FunctionTok{classif.DD}\NormalTok{(glearn,}\FunctionTok{list}\NormalTok{(mlearn,mlearn2),}\AttributeTok{depth=}\StringTok{"modep"}\NormalTok{,}\AttributeTok{classif=}\StringTok{"glm"}\NormalTok{,}\AttributeTok{control=}\FunctionTok{list}\NormalTok{(}\AttributeTok{draw=}\NormalTok{F))}
\NormalTok{out20 }\CommentTok{\# univariate functional data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -Call:
## classif.DD(group = glearn, fdataobj = mlearn, depth = "mode",     classif = "glm")
## 
## -Probability of correct classification:  0.928
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out21 }\CommentTok{\# multivariate functional data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -Call:
## classif.DD(group = glearn, fdataobj = list(mlearn, mlearn2),     depth = "modep", classif = "glm", control = list(draw = F))
## 
## -Probability of correct classification:  0.952
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#summary.classif(out21)}
\end{Highlighting}
\end{Shaded}

\hypertarget{classifiers-adapted-from-multivariate-framework}{%
\section{Classifiers adapted from Multivariate Framework}\label{classifiers-adapted-from-multivariate-framework}}

The idea is to recycle all the procedures known in the Multivariate Frameworkconverting an object of infinite dimension into a finite dimension.
When to apply:
+ The basis is enough for accounting all information.
+ Binary/multiclass problems depends on multivariate classifier method.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(phoneme)}
\NormalTok{ldata}\OtherTok{=}\FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{glearn=}\NormalTok{phoneme}\SpecialCharTok{$}\NormalTok{classlearn),}\StringTok{"x"}\OtherTok{=}\NormalTok{phoneme}\SpecialCharTok{$}\NormalTok{learn)}
\CommentTok{\# require e1071 package}
\NormalTok{res.svm}\OtherTok{=}\FunctionTok{classif.svm}\NormalTok{(glearn}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data=}\NormalTok{ldata)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "fdata2model"
## [1] "sale fdata2model"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# require nnet package}
\NormalTok{res.nnet}\OtherTok{=}\FunctionTok{classif.nnet}\NormalTok{(glearn}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data=}\NormalTok{ldata,}\AttributeTok{trace=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "fdata2model"
## [1] "sale fdata2model"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# require rpart package}
\NormalTok{res.rpart}\OtherTok{=}\FunctionTok{classif.rpart}\NormalTok{(glearn}\SpecialCharTok{\textasciitilde{}}\NormalTok{x,}\AttributeTok{data=}\NormalTok{ldata)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "fdata2model"
## [1] "sale fdata2model"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(res.svm}\SpecialCharTok{$}\NormalTok{prob.classification),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.904
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(res.nnet}\SpecialCharTok{$}\NormalTok{prob.classification),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.892
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(res.rpart}\SpecialCharTok{$}\NormalTok{prob.classification),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.896
\end{verbatim}

Add utilities in the classification functions: for example, majority voting scheme (by default ONE vs REST). R example (work in progress)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ii}\OtherTok{\textless{}{-}}  \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}\DecValTok{51}\SpecialCharTok{:}\DecValTok{60}\NormalTok{,}\DecValTok{101}\SpecialCharTok{:}\DecValTok{110}\NormalTok{,}\DecValTok{151}\SpecialCharTok{:}\DecValTok{160}\NormalTok{,}\DecValTok{201}\SpecialCharTok{:}\DecValTok{250}\NormalTok{)}
\NormalTok{mlearn}\OtherTok{\textless{}{-}}\NormalTok{phoneme[[}\StringTok{"learn"}\NormalTok{]][ii];glearn}\OtherTok{\textless{}{-}}\NormalTok{phoneme[[}\StringTok{"classlearn"}\NormalTok{]][ii]}
\NormalTok{mtest}\OtherTok{\textless{}{-}}\NormalTok{phoneme[[}\StringTok{"test"}\NormalTok{]];gtest}\OtherTok{\textless{}{-}}\NormalTok{phoneme[[}\StringTok{"classtest"}\NormalTok{]]}
\NormalTok{dataf}\OtherTok{\textless{}{-}}\FunctionTok{data.frame}\NormalTok{(glearn);ldata}\OtherTok{=}\FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\NormalTok{dataf,}\StringTok{"x"}\OtherTok{=}\NormalTok{mlearn);newdat}\OtherTok{\textless{}{-}}\FunctionTok{list}\NormalTok{(}\StringTok{"x"}\OtherTok{=}\NormalTok{mtest)}
\NormalTok{a1}\OtherTok{\textless{}{-}}\FunctionTok{classif.glm}\NormalTok{(glearn}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{data =}\NormalTok{ ldata)}
\NormalTok{a2}\OtherTok{\textless{}{-}}\FunctionTok{classif.glm}\NormalTok{(glearn}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{data =}\NormalTok{ ldata,}\AttributeTok{type=}\StringTok{"majority"}\NormalTok{)}
\NormalTok{a3}\OtherTok{\textless{}{-}}\FunctionTok{classif.glm}\NormalTok{(glearn}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{data =}\NormalTok{ ldata,}\AttributeTok{type=}\StringTok{"majority"}\NormalTok{,}\AttributeTok{weights=}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{4}\NormalTok{,}\AttributeTok{len=}\DecValTok{40}\NormalTok{),}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{50}\NormalTok{)))}
\CommentTok{\# mean(predict(a1,newdat)==gtest);mean(predict(a2,newdat)==gtest);mean(predict(a3,newdat)==gtest)}
\end{Highlighting}
\end{Shaded}

\hypertarget{k-means-clustering-for-functional-data}{%
\section{K-Means Clustering for functional data}\label{k-means-clustering-for-functional-data}}

Perform k-means clustering on functional data \citep{hartigan1979algorithm}.
The method searches the locations around which are grouped data (for a predetermined number of groups).
+ If ncl=NULL, randomizes the initial centers, ncl=2 using kmeans.center.ini function.
+ If ncl is an integer, indicating the number of groups to classify,
are selected ncl initial centers using kmeans.center.ini function.
+ If ncl is a vector of integers, indicating the position of the initial centers with length(ncl) equal to number of groups.
+ If ncl is a fdata class objecct, ncl are the initial centers curves with nrow(ncl) number of groups.

The fucntion return a list with:
+ cluster: Indexes of groups assigned.
+ centers: Curves centers.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(phoneme)}
\NormalTok{mlearn}\OtherTok{\textless{}{-}}\NormalTok{phoneme}\SpecialCharTok{$}\NormalTok{learn[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{150}\NormalTok{,]}
\NormalTok{ylearn}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(phoneme}\SpecialCharTok{$}\NormalTok{classlearn[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{150}\NormalTok{])}
\CommentTok{\# Unsupervised classification}
\NormalTok{kmeans.assig.groups }\OtherTok{\textless{}{-}}\NormalTok{ fda.usc}\SpecialCharTok{:::}\NormalTok{kmeans.assig.groups }
\NormalTok{kmeans.center.ini }\OtherTok{\textless{}{-}}\NormalTok{ fda.usc}\SpecialCharTok{:::}\NormalTok{kmeans.center.ini}
\NormalTok{kmeans.centers.update}\OtherTok{\textless{}{-}}\NormalTok{fda.usc}\SpecialCharTok{:::}\NormalTok{kmeans.centers.update}
\NormalTok{out.fd1}\OtherTok{=}\NormalTok{fda.usc}\SpecialCharTok{:::}\FunctionTok{kmeans.fd}\NormalTok{(mlearn,}\AttributeTok{ncl=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{51}\NormalTok{,}\DecValTok{101}\NormalTok{),}\AttributeTok{draw=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-54-1} \end{center}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-54-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(out.fd1}\SpecialCharTok{$}\NormalTok{cluster,ylearn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    ylearn
##      1  2  3
##   1 50 10  3
##   2  0 40  1
##   3  0  0 46
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Time consuming}
\CommentTok{\# out.fd2=kmeans.fd(mlearn,ncl=3,draw=FALSE,par.ini=list(method="exact"))}

\CommentTok{\# Different Depth function}
\CommentTok{\# ind=c(17,77,126)}
\CommentTok{\# out.fd3=kmeans.fd(mlearn,ncl=mlearn[ind,],draw=FALSE,}
\CommentTok{\# dfunc=func.trim.FM,par.dfunc=list(trim=0.1))}
\end{Highlighting}
\end{Shaded}

\hypertarget{functional-anova}{%
\section{Functional ANOVA}\label{functional-anova}}

\hypertarget{oneway-anova-model-for-functional-data-cuevas2004}{%
\subsection{\texorpdfstring{One--way anova model for functional data, \citet{Cuevas2004}}{One--way anova model for functional data, @Cuevas2004}}\label{oneway-anova-model-for-functional-data-cuevas2004}}

One--way anova model for k independent samples of functional data. The function contrasts the null hypothesis of equality of mean functions of functional data based on the an asymptotic version of the anova F--test. The function returns the p--value of test using one--way anova model over nboot runs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(MCO)}
\NormalTok{grupo}\OtherTok{\textless{}{-}}\NormalTok{MCO}\SpecialCharTok{$}\NormalTok{classintact}
\NormalTok{datos}\OtherTok{\textless{}{-}}\NormalTok{MCO}\SpecialCharTok{$}\NormalTok{intact}
\NormalTok{res}\OtherTok{=}\FunctionTok{fanova.onefactor}\NormalTok{(datos,grupo,}\AttributeTok{nboot=}\DecValTok{50}\NormalTok{,}\AttributeTok{plot=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-55-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res}\SpecialCharTok{$}\NormalTok{pvalue}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\hypertarget{functional-anova-with-random-project-cuesta-albertos2010}{%
\subsection{\texorpdfstring{Functional ANOVA with Random Project, \citet{Cuesta-Albertos2010}}{Functional ANOVA with Random Project, @Cuesta-Albertos2010}}\label{functional-anova-with-random-project-cuesta-albertos2010}}

The procedure is based on the analysis of randomly chosen one-dimensional projections. The function tests ANOVA models for functional data with continuous covariates and perform special contrasts for the factors in the formula.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(phoneme)}
\FunctionTok{names}\NormalTok{(phoneme)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "learn"      "test"       "classlearn" "classtest"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# A MV matrix obtained from functional data}
\NormalTok{data}\OtherTok{=}\FunctionTok{as.data.frame}\NormalTok{(phoneme}\SpecialCharTok{$}\NormalTok{learn}\SpecialCharTok{$}\NormalTok{data[,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{150}\NormalTok{,}\DecValTok{10}\NormalTok{)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{])]) }
\NormalTok{group}\OtherTok{=}\NormalTok{phoneme}\SpecialCharTok{$}\NormalTok{classlearn}
\NormalTok{n}\OtherTok{=}\FunctionTok{nrow}\NormalTok{(data)}
\NormalTok{group.rand}\OtherTok{=}\FunctionTok{as.factor}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\AttributeTok{len=}\NormalTok{n),n))}
\NormalTok{RP}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{30}\NormalTok{)}
\CommentTok{\#ex 1: real factor and random factor}
\NormalTok{m03}\OtherTok{=}\FunctionTok{data.frame}\NormalTok{(group,group.rand)}
\NormalTok{resul1}\OtherTok{=}\FunctionTok{fanova.RPm}\NormalTok{(phoneme}\SpecialCharTok{$}\NormalTok{learn,}\SpecialCharTok{\textasciitilde{}}\NormalTok{group}\SpecialCharTok{+}\NormalTok{group.rand,m03,}\AttributeTok{RP=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{30}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(resul1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      - SUMMARY fanova.RPm - 
## 
##  p-value for Bonferroni method 
##      group group.rand
## RP5      0    0.31629
## RP30     0    1.00000
## 
##   p-value for False Discovery Rate method 
##      group group.rand
## RP5      0    0.26453
## RP30     0    0.37664
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#ex 2: real factor with special contrast}
\NormalTok{m0}\OtherTok{=}\FunctionTok{data.frame}\NormalTok{(group)}
\NormalTok{cr5}\OtherTok{=}\FunctionTok{contr.sum}\NormalTok{(}\DecValTok{5}\NormalTok{)   }\CommentTok{\#each level vs last level}
\NormalTok{resul03c1}\OtherTok{=}\FunctionTok{fanova.RPm}\NormalTok{(data,}\SpecialCharTok{\textasciitilde{}}\NormalTok{group,m0,}\AttributeTok{contrast=}\FunctionTok{list}\NormalTok{(}\AttributeTok{group=}\NormalTok{cr5))}
\FunctionTok{summary}\NormalTok{(resul03c1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      - SUMMARY fanova.RPm - 
## 
##  p-value for Bonferroni method 
##      group C1.group C2.group C3.group C4.group
## RP16     0        0        0        0        0
## 
##   p-value for False Discovery Rate method 
##      group C1.group C2.group C3.group C4.group
## RP16     0        0        0        0        0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#ex 3: random factor with special contrast. Same projs as ex 2.}
\NormalTok{m0}\OtherTok{=}\FunctionTok{data.frame}\NormalTok{(group.rand)}
\NormalTok{zz}\OtherTok{=}\NormalTok{resul03c1}\SpecialCharTok{$}\NormalTok{proj}
\NormalTok{cr3}\OtherTok{=}\FunctionTok{contr.sum}\NormalTok{(}\DecValTok{3}\NormalTok{)   }\CommentTok{\#each level vs last level}
\NormalTok{resul03c1}\OtherTok{=}\FunctionTok{fanova.RPm}\NormalTok{(data,}\SpecialCharTok{\textasciitilde{}}\NormalTok{group.rand,m0,}\AttributeTok{contrast=}\FunctionTok{list}\NormalTok{(}\AttributeTok{group.rand=}\NormalTok{cr3),}\AttributeTok{zproj=}\NormalTok{zz)}
\FunctionTok{summary}\NormalTok{(resul03c1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      - SUMMARY fanova.RPm - 
## 
##  p-value for Bonferroni method 
##      group.rand C1.group.rand C2.group.rand
## RP16          1       0.90142       0.65644
## 
##   p-value for False Discovery Rate method 
##      group.rand C1.group.rand C2.group.rand
## RP16    0.74012       0.56936       0.51361
\end{verbatim}

\newcommand{\al}{\mbox{$\alpha$}}
\newcommand{\be}{\mbox{$\beta$}}
\newcommand{\ep}{\mbox{$\epsilon$}}
\newcommand{\gam}{\mbox{$\gamma$}}
\newcommand{\sig}{\mbox{$\sigma$}}

\newcommand{\calA}{\mbox{${\cal A}$}}
\newcommand{\calB}{\mbox{${\cal B}$}}
\newcommand{\calC}{\mbox{${\cal C}$}}

\newcommand{\Nat}{\mbox{$\mathbb{N}$}}
\newcommand{\Rea}{\mbox{$\mathbb{R}$}}
\newcommand{\Prob}{\mbox{$\mathbf{P}$}}
\newcommand{\ProbQ}{\mbox{$\mathbf{Q}$}}

\newcommand{\nin}{\mbox{$n \in \mathbb{N}$}}
\newcommand{\suc}{\mbox{$\{X_{n}\}$}}
\newcommand{\sucP}{\mbox{$\mathbb{P}_{n}\}$}

\newcommand{\conv}{\rightarrow}
\newcommand{\convn}{\rightarrow_{n\rightarrow \infty}}
\newcommand{\convp}{\rightarrow_{\mbox{c.p.}}}
\newcommand{\convs}{\rightarrow_{\mbox{a.s.}}}
\newcommand{\convw}{\rightarrow_al D}{\rightarrow}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\PR}{\mathbb{P}}

\newcommand{\I}{\mathbb{I}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lc}{\left[}
\newcommand{\rc}{\right]}
\newcommand{\lb}{\left\{}
\newcommand{\rb}{\right\}}
\newcommand{\lf}{\left.}
\newcommand{\ri}{\right.}
\newcommand{\id}{\stackrel{d}{=}}
\newcommand{\prob}[1]{\PR\lp#1\rp}
\newcommand{\esp}[2]{\mathbf{E}_#1\lc#2\rc}
\newcommand{\espe}[1]{\mathbf{E}\lc#1\rc}
\providecommand{\abs}[1]{\left|#1\right|}

\newcommand{\sign}{\textrm{sign}}
\newcommand{\corr}[2]{\textrm{Corr}\lp#1,#2\rp}
\newcommand{\cov}[2]{\textrm{Cov}\lp#1,#2\rp}
\newcommand{\Cov}[1]{\textrm{Cov}\lp#1\rp}
\newcommand{\var}[1]{\textrm{Var}\lp#1\rp}

\newcommand{\E}[1]{\mathbf{E}\lc #1\rc}
\newcommand{\V}[1]{\mathbb{V}\mathrm{ar}\lc #1\rc}
\newcommand{\Es}[2]{\mathbf{E}_{#2}\lc #1\rc}
\newcommand{\Vs}[2]{\mathbb{V}\mathrm{ar}_{#2}\lc #1\rc}
\newcommand{\mse}[1]{\mathrm{MSE}\lrc{#1}}
\newcommand{\mise}[1]{\mathrm{MISE}\lrc{#1}}
\newcommand{\amise}[1]{\mathrm{AMISE}\lrc{#1}}

\newcommand{\norm}[1]{\left|\left| #1\right|\right|}
\newcommand{\tr}[1]{\text{tr}\left[#1\right]}
\newcommand{\inprod}[2]{\langle#1,#2\rangle}

\newcommand{\vlinel}[1]{\multicolumn{1}{|c}{#1}}
\newcommand{\vliner}[1]{\multicolumn{1}{c|}{#1}}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\ind}[1]{\mathbbm{1}_{\lrb{#1}}}
\newcommand{\DC}{\mathcal{R}}

\hypertarget{variable-selection}{%
\chapter{Variable Selection}\label{variable-selection}}

\hypertarget{functiondal-regression-with-points-of-impact}{%
\section{Functiondal regression with points of impact}\label{functiondal-regression-with-points-of-impact}}

\hypertarget{state-of-art}{%
\subsection{State of Art}\label{state-of-art}}

\begin{itemize}
\item
  Nonparametric variable selection approach (NOVAS). NOVAS is quite expensive from a computational perspective, \citet{ferraty2010most}.
\item
  A wavelet-based weighted LASSO functional linear (FWLASSO).
  FWLASSO requires transforming the original variables and assuming a linear model, \citet{Zhao2015}.
\item
  \citet{berrendero2016variable} use the Maxima-hunting proposal to choose the most relevant design points in functional classification setting.
\end{itemize}

\hypertarget{local-maxima-distance-correlation-approach-lmdc-ordonez2018}{%
\subsection{\texorpdfstring{Local maxima distance correlation approach (LMDC), \citep{Ordonez2018}}{Local maxima distance correlation approach (LMDC), {[}@Ordonez2018{]}}}\label{local-maxima-distance-correlation-approach-lmdc-ordonez2018}}

\begin{itemize}
\item
  In this work we study the utility of distance correlation \citet{Szekely2007} as an intrinsic method for variable selection.
\item
  Neither projection nor transformation of the variables is needed. Moreover, it is unnecessary to assume an a priori regression model.
\item
  LMDC approach consists in calculating the local maxima of the distance correlation along the curve.
\end{itemize}

\hypertarget{lmdc-algorithm-lmdc.select-function}{%
\subsection{\texorpdfstring{LMDC Algorithm: \texttt{LMDC.select()} function}{LMDC Algorithm: LMDC.select() function}}\label{lmdc-algorithm-lmdc.select-function}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Calculate de distance correlation (DC) \(R(t) = \left \lbrace R(X(t_j),Y) \right\rbrace {_{j=1}^N}\), from the data \(\left \lbrace X_i (t_j),Y_i \right\rbrace _{i=1}^n\).
\item
  Calculate the LM of the \(\hat{\mathcal{R}} (t)\).
  Only the significant local maxima for a default level of significance are selected.
  Denoting the arguments values (argvals) of the local maxima a \(\tilde t_1,\tilde t_2,\ldots,\tilde t_{\tilde{N}}\) (\(\tilde{N}<N\)), we ordered them from highest to lowest values of DC, that is \(\hat{\mathcal{R}}(\tilde t_1) \geq \hat{ \mathcal{R}}(\tilde t_2) >\ldots \geq \hat {\mathcal{R}}(\tilde t_{\tilde{N}})\)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fda.usc)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(tecator)}
\NormalTok{X.d2}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(tecator[[}\StringTok{"absorp.fdata"}\NormalTok{]],}
\AttributeTok{nderiv =} \DecValTok{2}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(X.d2[[}\StringTok{"data"}\NormalTok{]])}\OtherTok{\textless{}{-}}\FunctionTok{paste0}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\FunctionTok{round}\NormalTok{(X.d2[[}\StringTok{"argvals"}\NormalTok{]]))}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\StringTok{"y"}\OtherTok{=}\NormalTok{tecator[[}\StringTok{"y"}\NormalTok{]][[}\StringTok{"Fat"}\NormalTok{]],X.d2[[}\StringTok{"data"}\NormalTok{]] )}
\NormalTok{tol}\OtherTok{\textless{}{-}}\NormalTok{.}\DecValTok{2}
\NormalTok{dc.raw }\OtherTok{\textless{}{-}} \FunctionTok{LMDC.select}\NormalTok{(}\StringTok{"y"}\NormalTok{,}\AttributeTok{data =}\NormalTok{ dat, }\AttributeTok{tol =}\NormalTok{ tol,}\AttributeTok{pvalue =} \FloatTok{0.05}\NormalTok{,}
\AttributeTok{plot=}\NormalTok{F)}
\CommentTok{\# Preselected impact points }
\NormalTok{covar}\OtherTok{\textless{}{-}}\FunctionTok{names}\NormalTok{(dat)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{][dc.raw[[}\StringTok{"maxLocal"}\NormalTok{]]]}
\NormalTok{covar}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "X933"  "X1046" "X907"  "X886"  "X896"  "X1010" "X1020" "X1030" "X945" 
## [10] "X876"  "X915"  "X862"  "X993"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(covar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13
\end{verbatim}

\hypertarget{lmdc-algorithm-lmdc.regre-function}{%
\subsection{\texorpdfstring{LMDC Algorithm: \texttt{LMDC.regre()} function}{LMDC Algorithm: LMDC.regre() function}}\label{lmdc-algorithm-lmdc.regre-function}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  (Optionally) Check if the relationship between the reponse and the predictor variables is linear: \(H_0:\,Y=\big<X,\beta\big>+\epsilon\), versus a general alternative using a test of linearity proposed in \citet{garcia2014goodness}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ftest}\OtherTok{\textless{}{-}}\FunctionTok{flm.test}\NormalTok{(dat[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], dat[,}\StringTok{"y"}\NormalTok{],}
\AttributeTok{verbose=}\NormalTok{F,}\AttributeTok{plot.it=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "PLS1"
## [1] "PLS2"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ftest}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  PCvM test for the functional linear model using optimal PLS basis
##  representation
## 
## data:  Y=<X,b>+e
## PCvM statistic = 216.51, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ftest[[}\StringTok{"p.value"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Fit a regression model to the response of interest \(Y\) using the vector of covariates \(X(\tilde{t})=\{X(\tilde t_1), \ldots, X(\tilde{t}_{\tilde{N}})\}\). A linear model will be used if the null hypothesis is not rejected and a nonparametric (e.g.~generalized additive model) model otherwise.
\item
  (Optionally) Once the type model has been selected, we propose to
  Apply a forward stepwise regression method to determine the significant covariates, taking advantage of the fact that the local maxima have been ordered. This means we start with a model with the first covariate (the one with the highest value of distance correlation), and the rest of the ordered covariates are added to the model in turn. This substantially reduces the computing time.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (ftest}\SpecialCharTok{$}\NormalTok{p.value }\SpecialCharTok{\textgreater{}} \FloatTok{0.05}\NormalTok{) \{ }\CommentTok{\# Linear relationship, step{-}wise lm is recommended}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{LMDC.regre}\NormalTok{(}\AttributeTok{y =} \StringTok{"y"}\NormalTok{, }\AttributeTok{covar =}\NormalTok{ covar, }\AttributeTok{data =}\NormalTok{ dat, }\AttributeTok{pvalue=}\NormalTok{.}\DecValTok{05}\NormalTok{, }\AttributeTok{method =}\StringTok{"lm"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}\CommentTok{\# Non{-}Linear relationship, step{-}wise gam is recommended}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{LMDC.regre}\NormalTok{(}\AttributeTok{y =} \StringTok{"y"}\NormalTok{, }\AttributeTok{covar =}\NormalTok{ covar, }\AttributeTok{data =}\NormalTok{ dat,}\AttributeTok{pvalue=}\NormalTok{.}\DecValTok{05}\NormalTok{, }\AttributeTok{method =}\StringTok{"gam"}\NormalTok{)\}  }
\NormalTok{out}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $model
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## y ~ s(X933, k = 4) + s(X1046, k = 4) + s(X907, k = 4) + s(X886, 
##     k = 4) + s(X896, k = 4) + s(X1010, k = 4) + s(X1020, k = 4) + 
##     s(X1030, k = 4) + s(X945, k = 4) + s(X876, k = 4) + s(X915, 
##     k = 4) + s(X993, k = 4)
## 
## Estimated degrees of freedom:
## 3.00 2.55 2.83 2.00 1.00 1.00 2.94 
## 2.92 2.81 2.57 1.00 2.85  total = 28.48 
## 
## GCV score: 0.4403176     
## 
## $xvar
##  [1] "X933"  "X1046" "X907"  "X886"  "X896"  "X1010" "X1020" "X1030" "X945" 
## [10] "X876"  "X915"  "X993" 
## 
## $pred
## NULL
## 
## $edf
## [1] 28.48142
## 
## $nvar
## [1] 12
\end{verbatim}

Differences in mean square prediction error between linear (usign \texttt{lm} model) and non-linear (usign \texttt{gam} model) model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{LMDC.regre}\NormalTok{(}\AttributeTok{y =} \StringTok{"y"}\NormalTok{, }\AttributeTok{covar =}\NormalTok{ covar, }\AttributeTok{data =}\NormalTok{ dat[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{165}\NormalTok{,],}\AttributeTok{newdata=}\NormalTok{dat[}\DecValTok{166}\SpecialCharTok{:}\DecValTok{215}\NormalTok{,], }\AttributeTok{pvalue=}\NormalTok{.}\DecValTok{05}\NormalTok{, }\AttributeTok{method =}\StringTok{"lm"}\NormalTok{)}
\FunctionTok{mean}\NormalTok{((out}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{{-}}\NormalTok{dat}\SpecialCharTok{$}\NormalTok{y[}\DecValTok{166}\SpecialCharTok{:}\DecValTok{215}\NormalTok{])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11.75474
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{LMDC.regre}\NormalTok{(}\AttributeTok{y =} \StringTok{"y"}\NormalTok{, }\AttributeTok{covar =}\NormalTok{ covar, }\AttributeTok{data =}\NormalTok{ dat[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{165}\NormalTok{,],}\AttributeTok{newdata=}\NormalTok{dat[}\DecValTok{166}\SpecialCharTok{:}\DecValTok{215}\NormalTok{,], }\AttributeTok{pvalue=}\NormalTok{.}\DecValTok{05}\NormalTok{, }\AttributeTok{method =}\StringTok{"gam"}\NormalTok{)}
\FunctionTok{mean}\NormalTok{((out}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{{-}}\NormalTok{dat}\SpecialCharTok{$}\NormalTok{y[}\DecValTok{166}\SpecialCharTok{:}\DecValTok{215}\NormalTok{])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.148573
\end{verbatim}

\begin{quote}
Binary classification example (Impact point selection, model estimation and prediction)
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(tecator)}
\NormalTok{X.d2}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(tecator[[}\StringTok{"absorp.fdata"}\NormalTok{]],}
\AttributeTok{nderiv =} \DecValTok{2}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(X.d2[[}\StringTok{"data"}\NormalTok{]])}\OtherTok{\textless{}{-}}\FunctionTok{paste0}\NormalTok{(}\StringTok{"X"}\NormalTok{,}\FunctionTok{round}\NormalTok{(X.d2[[}\StringTok{"argvals"}\NormalTok{]]))}
\NormalTok{y2groups }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(tecator[[}\StringTok{"y"}\NormalTok{]][[}\StringTok{"Fat"}\NormalTok{]]}\SpecialCharTok{\textless{}}\DecValTok{12}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\StringTok{"y2groups"}\OtherTok{=}\NormalTok{y2groups,X.d2[[}\StringTok{"data"}\NormalTok{]] )}
\NormalTok{tol}\OtherTok{\textless{}{-}}\NormalTok{.}\DecValTok{1}
\NormalTok{dc.raw }\OtherTok{\textless{}{-}} \FunctionTok{LMDC.select}\NormalTok{(}\StringTok{"y2groups"}\NormalTok{,}\AttributeTok{data =}\NormalTok{ dat, }\AttributeTok{tol =}\NormalTok{ tol,}\AttributeTok{pvalue =} \FloatTok{0.05}\NormalTok{,}
\AttributeTok{plot=}\NormalTok{F)}
\CommentTok{\# Preselected impact points }
\NormalTok{covar}\OtherTok{\textless{}{-}}\FunctionTok{names}\NormalTok{(dat)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{][dc.raw[[}\StringTok{"maxLocal"}\NormalTok{]]]}
\NormalTok{covar}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "X945"  "X905"  "X933"  "X886"  "X1020" "X876"  "X1030" "X896"  "X1046"
## [10] "X862"  "X1010" "X915"  "X965"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(covar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# GLM model (using binomial family), other multivariate model can be used}
\NormalTok{ind }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{129}
\NormalTok{ldata}\OtherTok{\textless{}{-}}\FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\NormalTok{dat[ind,])}
\NormalTok{form.glm}\OtherTok{\textless{}{-}}\FunctionTok{formula}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"y2groups\textasciitilde{}"}\NormalTok{,}\FunctionTok{paste0}\NormalTok{(covar,}\AttributeTok{collapse=}\StringTok{"+"}\NormalTok{)))}
\NormalTok{out.glm  }\OtherTok{\textless{}{-}} \FunctionTok{classif.glm}\NormalTok{(form.glm, }\AttributeTok{data =}\NormalTok{ ldata)}
\FunctionTok{summary}\NormalTok{(out.glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      - SUMMARY - 
## 
## -Probability of correct classification by group (prob.classification):
## 0 1 
## 1 1 
## 
## -Confusion matrix between the theoretical groups (by rows)
##   and estimated groups (by column) 
##    
##      0  1
##   0 58  0
##   1  0 71
## 
## -Probability of correct classification:  1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prediction}
\NormalTok{newldata}\OtherTok{\textless{}{-}}\FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\NormalTok{dat[}\SpecialCharTok{{-}}\NormalTok{ind,])}
\NormalTok{pred.glm}\OtherTok{\textless{}{-}}\FunctionTok{predict}\NormalTok{(out.glm,newldata)}

\CommentTok{\# Confusion matrix}
\FunctionTok{table}\NormalTok{(newldata}\SpecialCharTok{$}\NormalTok{df}\SpecialCharTok{$}\NormalTok{y2groups,pred.glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    pred.glm
##      0  1
##   0 40  0
##   1  3 43
\end{verbatim}

\hypertarget{variable-selection-in-functional-regression}{%
\section{Variable selection in functional regression}\label{variable-selection-in-functional-regression}}

\citet{Febrero-Bande2019} consider the problem of variable selection in regression models in the case of functional variables that may be mixed with other type of variables (scalar, multivariate, directional, etc.).

Our proposal begins with a simple null model and sequentially selects a new variable to be incorporated into the model based on the use of distance correlation proposed by \citep{Szekely2007}. For the sake of simplicity, this paper only uses additive models.

\[
Y_i=\alpha+\sum_{j=1}^Jf_j({X_i^{(j)}})+\varepsilon_i,\quad i=1,\ldots,N
\]

The proposed algorithm may assess the type of contribution (linear, non linear, \ldots) of each variable. The algorithm has shown quite promising results when applied to simulations and real data sets.

\hypertarget{state-of-art-1}{%
\subsection{State of Art}\label{state-of-art-1}}

\begin{itemize}
\item
  Stepwise regression, \citet{Akaike1973}. The main idea is to use some diagnostic tools, directly derived from the linear model, to evaluate the contribution of a new covariate and decide whether it should be included in the model. The final subset is usually constructed using: the forward and/or the backward selection.
\item
  Feature Selection using LASSO. The work by Tibshirani, 1996 proposing the LASSO estimator includes a \(l_1\)-type constraint for the coefficient vector \(\beta\). Several examples following the same line but using penalties or constraints such as: LARS (\citet{efron2004least}) and COSSO (\citet{lin2006component}). Each methods is based on a specific model, all the covariates must be included in the model at the same time and for functional data problems, the previous steps that commonly include variable standardization.
\item
  \citet{berrendero2016variable} use the Minimum Redundance Maximum Relevance (mRMR) procedure to choose the most relevant design points in functional classification setting.
\item
  A pure feature selection methods where the covariate is selected without a model. This is the approach employed in minimum Redundancy Maximum Relevance (mRMR), (\citet{peng2005feature}) where a new candidate covariate must have a great relevancy with the response while maintaining a lower redundancy with the covariates already selected in the model. he main advantage of this approach is that it is an incremental rule but the measures for redundancy and relevancy must be chosen in function of the regression model applied to ensure good predictive results in the final model. \citet{berrendero2018} used the Reproducing Kernel Hilbert Space (RKHS) for variable selection in FLM.
\item
  Boosting, see \citet{Ferraty2009} in a functional data context. Boosting selects at each step the best covariate/model with respect to the unexplained part of the response. The final prediction is constructed as a combination of the different steps.
\item
  Partial distance correlation (PDC): used in \citet{Yenigun2015} for VS in multivariate linear models, a definition of PDC among \(X\) and \(Y\) given \(Z\) was introduced based on computing the distance correlation among the residuals of two models: \(Y\) respect to \(Z\) and \(X\) respect to \(Z\). PDC is constructed under linear relationship assumptions among variables. Its implementation only uses the distance matrices among elements of \(X\), \(Y\) and \(Z\) (variables should have a similar scale).
\end{itemize}

Specifically, \(Z\) (the variables already in the model) could be a mix of functional, scalar or multivariate variables where an appropriate distance using all of them must be hard to compute. Even restricting ourselves to the scalar case, those variables should have a similar scale.

\hypertarget{algorithm}{%
\subsection{Algorithm}\label{algorithm}}

All the previous solutions are not completely satisfactory in a functional data framework, specially when the number of possible covariates can be arbitrarily large. We are interested in an automatic regression procedure capable of dealing with a large number of covariates of different nature, possibly very closely related to one another.

The key of the whole procedure is the extensive use of the DC that presents two important advantages: the choice of the variate is made without considering a model and it is possible to compute this quantity for variates of different nature as it is only computed from distances. The distance correlation (DC) is computed among the residuals of the current model with each candidate. Taking into account that the residuals have the same nature as the response variable, the DC can always be computed at each step.

Our proposal is presented is a very general way, we have restricted ourselves to additive models that offer a balanced compromise between predictive ability and simplicity. The obtained results are quite promising in scenarios where no competitors are available because no other procedure can deal with variates of different nature in a homogeneous way.

The procedure was applied to a real problem related with the Iberian Energy Market (Price and Demand) where the number of possible covariates is really big. The algorithm was able to find synthetic regression models offering interesting insights about the relationship among the response and the covariates. The final selected models mix functional, scalar and categorical information.

Our algorithm can be formalized as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Let \(Y\) the response and \(S=\{X^1,\ldots,X^p\}\) the set of all possible predictors.
\item
  Set \(\hat{Y}=\bar{Y}\), and let \(M^{(0)}=\emptyset\) the initial set of the variates included in the model. Set \(i=0\).
\item
  Compute the residuals of the current model: \(\hat{\varepsilon}=Y-\hat{Y}\).
\item
  Choose \(X^j\in S\) such that: 1) \(\mathcal{R}\{\hat{\varepsilon},X^j\}\ge \mathcal{R}\{\hat{\varepsilon},X^k\}, \forall k\ne j\in S\) and 2) the null hypothesis for the test of independence among \(\left\{X^j\right\}\) and \(\hat{\varepsilon}\) is rejected. IF NOT, END.
\item
  Update the sets \(M\) and \(S\): \(M^{(i+1)}=M^{(i)}\cup\{X^j\}\), and \(S=S\backslash\{X^j\}\).
\item
  Compute the new model for \(Y\) using \(M^{(i+1)}\) choosing the best contribution of the new covariate. Typically, there will be a catalog of all possible ways of constructing correct models with the variates in \(M^{(i+1)}\) fixing the contributions of the variates in \(M^{(i)}\) and adding the new one.
\item
  Analyze the contribution of \(X^j\) in the new model respect to the current:
\end{enumerate}

\begin{itemize}
\item
  IF this contribution is not relevant (typically comparing with the current model)
  THEN \(M^{(i+1)}=M^{(i+1)}\backslash\{X^j\}\) and the current model remains unalterable
\item
  ELSE the new model becomes the current model and provides new predictions (\(\hat{Y}\)). Along the paper we have employed an additive model: \(\hat{Y}=\bar{Y}+\sum_{m\in M}\hat{f}_m\left(X^{(m)}\right)\) where at each step \(\hat{f}_m\) could be linear or nonlinear.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  Update the number of iterations: \(i=i+1\) and go to 3
\item
  END.
\end{enumerate}

The current model is the final model with the variates included in \(M^{(i)}\).
\(S\) is either the empty set or contains those variables that accept the null hypothesis of the test of independence respect to the residuals of the current model.

\hypertarget{r-example}{%
\subsection{R example}\label{r-example}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(tecator)}
\NormalTok{y}\OtherTok{=}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{$}\NormalTok{Fat}

\CommentTok{\# Potential functional covariates}
\NormalTok{x}\OtherTok{=}\NormalTok{tecator}\SpecialCharTok{$}\NormalTok{absorp.fdata}
\NormalTok{x1}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(x)}
\NormalTok{x2}\OtherTok{\textless{}{-}}\FunctionTok{fdata.deriv}\NormalTok{(x,}\AttributeTok{nderiv=}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Potential factor covariates}
\NormalTok{xcat0}\OtherTok{\textless{}{-}}\FunctionTok{cut}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(y)),}\DecValTok{4}\NormalTok{)}
\NormalTok{xcat1}\OtherTok{\textless{}{-}}\FunctionTok{cut}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{$}\NormalTok{Protein,}\DecValTok{4}\NormalTok{)}
\NormalTok{xcat2}\OtherTok{\textless{}{-}}\FunctionTok{cut}\NormalTok{(tecator}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{$}\NormalTok{Water,}\DecValTok{4}\NormalTok{)}
\NormalTok{ind }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{129}

\CommentTok{\# 3 functionals (x,x1,x2), 3 factors (xcat0, xcat1, xcat2)}
\CommentTok{\# and 100 potential scalars covariates (impact poitns of x1) }
\NormalTok{dat    }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\StringTok{"Fat"}\OtherTok{=}\NormalTok{y, x1}\SpecialCharTok{$}\NormalTok{data, xcat1, xcat2)}
\NormalTok{ldat }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\NormalTok{dat[ind,],}\StringTok{"x"}\OtherTok{=}\NormalTok{x[ind,],}\StringTok{"x1"}\OtherTok{=}\NormalTok{x1[ind,],}\StringTok{"x2"}\OtherTok{=}\NormalTok{x2[ind,])}

\CommentTok{\# Time consuming}
\NormalTok{res.gam1}\OtherTok{\textless{}{-}}\FunctionTok{fregre.gsam.vs}\NormalTok{(}\AttributeTok{data=}\NormalTok{ldat,}\AttributeTok{y=}\StringTok{"Fat"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(res.gam1}\SpecialCharTok{$}\NormalTok{model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Fat             X97                x2.PC1              x2.PC2          
##  Min.   : 0.90   Min.   :-0.015836   Min.   :-0.003367   Min.   :-3.082e-03  
##  1st Qu.: 7.70   1st Qu.:-0.010165   1st Qu.:-0.002367   1st Qu.:-7.165e-04  
##  Median :14.60   Median :-0.009397   Median :-0.000950   Median :-2.801e-05  
##  Mean   :18.24   Mean   :-0.009414   Mean   : 0.000000   Mean   : 0.000e+00  
##  3rd Qu.:27.80   3rd Qu.:-0.008574   3rd Qu.: 0.001717   3rd Qu.: 6.665e-04  
##  Max.   :49.10   Max.   :-0.006377   Max.   : 0.009573   Max.   : 6.246e-03  
##      x2.PC3               x2.PC4          
##  Min.   :-0.0032986   Min.   :-1.041e-03  
##  1st Qu.:-0.0001912   1st Qu.:-1.193e-04  
##  Median : 0.0001008   Median : 2.363e-05  
##  Mean   : 0.0000000   Mean   : 0.000e+00  
##  3rd Qu.: 0.0002728   3rd Qu.: 1.270e-04  
##  Max.   : 0.0034901   Max.   : 1.131e-03
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prediction like fregre.gsam() }
\NormalTok{newldat }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{"df"}\OtherTok{=}\NormalTok{dat[}\SpecialCharTok{{-}}\NormalTok{ind,],}\StringTok{"x"}\OtherTok{=}\NormalTok{x[}\SpecialCharTok{{-}}\NormalTok{ind,],}\StringTok{"x1"}\OtherTok{=}\NormalTok{x1[}\SpecialCharTok{{-}}\NormalTok{ind,],}\StringTok{"x2"}\OtherTok{=}\NormalTok{x2[}\SpecialCharTok{{-}}\NormalTok{ind,])}
\NormalTok{pred.gam1}\OtherTok{\textless{}{-}}\FunctionTok{predict}\NormalTok{(res.gam1,newldat)}
\FunctionTok{plot}\NormalTok{(dat[}\SpecialCharTok{{-}}\NormalTok{ind,}\StringTok{"Fat"}\NormalTok{],pred.gam1)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{bookdown_intro_files/figure-latex/unnamed-chunk-64-1} \end{center}

\hypertarget{references-2}{%
\chapter*{References}\label{references-2}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{documentation}{%
\chapter*{Documentation}\label{documentation}}
\addcontentsline{toc}{chapter}{Documentation}

\begin{quote}
RPubs documents
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  fda.usc vignette: \href{http://rpubs.com/moviedo/fda_usc_introduction}{Installation and Descriptive Statistics}
\item
  fda.usc vignette:\href{http://rpubs.com/moviedo/fda_usc_regression}{Functional Regression}
\item
  fda.usc vignette: \href{http://rpubs.com/moviedo/fda_usc_classification}{Functional Classification and ANOVA}
\item
  fda.usc vignette: \href{http://rpubs.com/moviedo/fda_usc_VS}{Variable Selection in Functional Regression (and Classification)}
\item
  fda.usc reference card \href{http://rpubs.com/moviedo/fda_usc_rcard}{Rcard}
\end{enumerate}

\begin{quote}
Links:
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \href{http://cran.r-project.org/web/packages/fda.usc}{fda.usc R package}
\item
  \href{http://www.jstatsoft.org/article/view/v051i04}{JSS paper}
\item
  \href{https://cran.r-project.org/web/packages/fda.usc/fda.usc.pdf}{fda.usc R Manual}
\item
  \href{http://eio.usc.es/pub/moviedo}{Manuel Oviedo's website}
\end{enumerate}

\protect\hyperlink{top}{Back to Top}

\hypertarget{funding-and-financial-support}{%
\chapter{Funding and Financial Support:}\label{funding-and-financial-support}}

This work has been supported by Project MTM2016-76969-P from Ministerio de Economa y Competitividad - Agencia Estatal de Investigacion and European Regional Development Fund (ERDF).

  \bibliography{packages.bib,bibliografia.bib}

\end{document}
