[["index.html", "Functional Data Analysis using fda.usc and fda.clust packages Introduction CRAN Task View Functional Data Analysis in R Installation Quick Start", " Functional Data Analysis using fda.usc and fda.clust packages Manuel Oviedo de la Fuente (Universidade of A Coruña, CITIC, MODES RG) and Manuel Febrero-Bande (Universidade de Santaigo de Compostela, MODESTYA RG) December, 2024 Introduction This vignette describes the usage of fda.usc in R. fda.usc package carries out exploratory and descriptive analysis of functional data exploring its most important features such as: Functional Data Representation Functional Regression Functional Supervised Classification Functional Clustering Functional ANOVA Feature and Variable Selection The colaborator of fda.usc is Manuel Febrero-Bande, the contributors are Pedro Galeano (UC3M), Alicia Nieto (UNICAN) and Eduardo Garcia-Portugues (UC3M). The coauthor of fda.clust is Prof. Manuel Febrero-Bande, the contributors is **Rikelvi Gonzalez (former master’s student, UDC)*. CRAN Task View R task view devoted to FDA: https://cran.r-project.org/web/views/FunctionalData.html is included in the set of the 7 recommended core packages. Version: 2.2.0 Depends: R (≥ 3.5.0), fda, splines, MASS, mgcv, knitr Imports: methods, grDevices, graphics, utils, stats, nlme, doParallel, parallel, iterators, foreach, kSamples Suggests: rmarkdown+ Reverse imports: biosensors.usc, etree, FADPclust, FiSh, funcharts, FuncNN, goffda, GPFDA, logitFD, longsurr, mlmts, puls, robflreg, slasso, SpatFD Reverse suggests: GET, mlr, photobiologyInOut Versions (by year): 5 (2012), 6 (2013), 2 (2014), 1 (2015), 2 (2016), 1 (2018), 2(2019), 1(2020), 1(2022), 1(2024) fda.usc package download by month: Functional Data Analysis in R fda It is a basic reference to work in R with functional data, see (Ramsay and Silverman 2005a), http://ego.psych.mcgill.ca/misc/fda/ (Frédéric Ferraty and Vieu 2006) processed FD from a nonparametric point of view (normed or semi–normed functional spaces). These authors are part of the French group STAPH maintaining the page http://www.lsp.ups-tlse.fr/staph/ Core packages: FDboost: Boosting Functional Regression Models fds: Functional Data Sets ftsa: Functional Time Series Analysis fdasrvf: Elastic Functional Data Analysis refund: Regression with Functional Data fdapace: Functional Data Analysis and Empirical Dynamics 4.Interactive tools: + StatFda: exploratory analysis and functional regression models,http://www.statfda.com forand refund.shiny packagefor interactive plotting. refund.shiny: Interactive Plotting for Functional Data Analyses tidyfun: makes data wrangling and exploratory analysis of functional data easier, https://fabian-s.github.io/tidyfun Other packages: GPFDA: Use Functional regression as the mean structure and Gaussian Process as the covariance structure. MFHD: Multivariate Functional Halfspace Depth rainbow: Bagplots, Boxplots and Rainbow Plots for Functional Data NITPicker: Finds the Best Subset of Points to Sample fdatest: Interval Testing Procedure for Functional Data fdakma: Functional Data Analysis: K-Mean Alignment fdaMixed: Functional data analysis in a mixed model framework geofd: Spatial Prediction for Function Value Data goffda: Goodness-of-Fit Tests for Functional Data mlr: Machine Learning in R Information to be updated Installation Like many other R packages, the simplest way to obtain fda.usc is to install it directly from CRAN. Type the following command in R console: install.packages(&quot;fda.usc&quot;, repos = &quot;http://cran.us.r-project.org&quot;) Users may change the repos options depending on their locations and preferences. Other options such as the directories where to install the packages can be altered in the command. For more details, see help(install.packages). Here the R package has been downloaded and installed to the default directories. Alternatively, users can download the package source at http://cran.r-project.org/web/packages/fda.usc/index.html and type Unix commands to install it to the desired location. Reference manual Quick Start The purpose of this section is to give users a general sense of the package, including the components, what they do and some basic usage. We will briefly go over the main functions, see the basic operations and have a look at the outputs. Users may have a better idea after this section what functions are available, which one to choose, or at least where to seek help. More details are given in later sections. First, we load the fda.usc package: library(fda.usc) References ———. 2006. Nonparametric Functional Data Analysis. Springer Series in Statistics. New York: Springer-Verlag. Ramsay, J. O., and B. W. Silverman. 2005a. Functional Data Analysis. Springer. "],["definition.html", "Chapter 1 Functional Data: Definition, Representation and Manipulation 1.1 Some definitions of Functional Data 1.2 In fda.usc: ``The data are curves’’ 1.3 Resume by smoothing 1.4 Correlation Distances (G. J. Székely, Rizzo, and Bakirov 2007) References", " Chapter 1 Functional Data: Definition, Representation and Manipulation 1.1 Some definitions of Functional Data Functional data analysis is a branch of statistics that analyzes data providing information about curves, surfaces or anything else varying over a continuum. The continuum is often time, but may also be spatial location, wavelength, probability, etc. Functional data analysis is a branch of statistics concerned with analysing data in the form of functions. Definition 1: A random variable \\(\\mathcal{X}\\) is called a functional variable if it takes values in a functional space \\(\\mathcal{E}\\) –complete normed (or seminormed) space–, (Frédéric Ferraty and Vieu 2006) Definition 2: A functional dataset \\(\\{\\mathcal{X}_1,\\ldots,\\mathcal{X}_n\\}\\) is the observation of n functional variables \\(\\mathcal{X}_1,\\ldots,\\mathcal{X}_n\\) identically distributed as \\(\\mathcal{X}\\), (Frédéric Ferraty and Vieu 2006). A research stream focuses on the computational treatment that is used to working with functional data as an extension of multivariate data. Thus, the following definition of functional data proposed by (Muller et al. 2008) is also common in practice. Definition 3. Functional data is multivariate data with an ordering on the dimensions, so that \\(a=t_1&lt;t_2,\\ldots,t_{m-1}&lt; b=t_m\\). 1.2 In fda.usc: ``The data are curves’’ \\(X_i(t)\\) represents the mean temperature (averaged over 1980-2009 years) at the ith weather station in Spain, and at time time \\(t\\) during the year. Temperature curves (right) located in Spanish airports (left), But what code has been used to obtain the graph? data(aemet) par(mfrow=c(1,2)) CanaryIslands &lt;- ifelse(aemet$df$latitude &lt; 31, 2, 4) plot(aemet$df[,c(&quot;longitude&quot;,&quot;latitude&quot;)], col = CanaryIslands, lwd=2) plot(temp, col = CanaryIslands, lwd=2) 1.2.1 Definition of –fdata– class in R Definition of fdata class object: An object called fdata as a list of the following components: data: typically a matrix of (n x m) dimension which contains a set of n curves discretized in m points or argvals. argvals: locations of the discretization points, by default: \\({{t}_1=1,\\ldots,{t}_m=m}\\}\\). rangeval: rangeval of discretization points. names: (optional) list with three components: main, an overall title, xlab, a title for the x axis and ylab, a title for the y axis. lapply(aemet,class) names(aemet$df) lapply(aemet$temp,class) temp &lt;- aemet$temp names(temp) dim(temp) length(argvals(temp)) rangeval(temp) temp$names 1.2.2 Some utilities of fda.usc package Basic operations for fdata class objects: Group Math: abs, sqrt, floor, ceiling,semimetric.basis(),trunc,round,signif,exp,log,cos,sin,tan`. Other operations: [], is.fdata(), c(), dim(), ncol(), nrow(). is.fdata(aemet$temp) is.fdata(aemet$df) dim(aemet$df) dim(aemet$temp) Convert the class The class fdata only uses the evaluations at the discretization points. The fdata2fd() converts fdata object to fd object (using the basis representation). Inversely, the fdata() converts object of class: fd, fds, fts, sfts, vector, matrix, data.frame to an object of class fdata. temp.fd=fdata2fd(temp,type.basis=&quot;fourier&quot;,nbasis=15) temp.fdata=fdata(temp.fd) #back to fdata class(temp.fd) ## [1] &quot;fd&quot; class(temp.fdata) ## [1] &quot;fdata&quot; split.fdata() and unlist(): A wrapper for the split and unlist function for fdata object # Canary Islands in red vs Iberian Peninsula in blue l1 &lt;- fda.usc:::split.fdata(temp,CanaryIslands) par(mfrow=c(1,2)) plot(l1[[1]],col=2,ylim=c(0,30)) plot(l1[[2]],col=4,ylim=c(0,30)) dim(l1[[1]]);dim(l1[[2]]) ## [1] 9 365 ## [1] 64 365 Group Ops: + -, *, /, ^, %%, %/%, &amp;, |, !, ==, !=, &lt;, &lt;=, &gt;=, &gt;. l1[[1]]==l1[[2]] ## [1] FALSE order.fdata() A wrapper for the order function. The funcional data is ordered w.r.t the sample order of the values of vector. temp2 &lt;- order.fdata(order(CanaryIslands),temp) Generate random process of fdata class. rproc2fdata() function generates Functional data from: Ornstein Uhlenbeck process, Brownian process, Gaussian process or Exponential variogram process. par(mfrow=c(1,2)) lent &lt;- 30 tt &lt;- seq(0,1,len=lent) xgen1 &lt;- rproc2fdata(200,t=tt,sigma=&quot;OU&quot;,par.list=list(&quot;scale&quot;=1)) plot(xgen1) mu &lt;- fdata(sin(2*pi*tt),tt) xgen2 &lt;- rproc2fdata(200,mu=mu,sigma=&quot;OU&quot;,par.list=list(&quot;scale&quot;=1)) plot(xgen2) 1.2.3 Definition of –ldata– class in R ldata is a list with two type of objects: df is a data frame with the multivariate data with n rows. ... objects of class fdata with n rows. ldat &lt;- ldata(df=aemet$df,temp=aemet$temp, logprec=aemet$logprec ) plot(ldat) 1.3 Resume by smoothing If we supposed that our functional data \\(Y(t)\\) is observed through the model \\(Y(t_i)=X(t_i)+\\varepsilon(t_i)\\) where the residuals \\(\\varepsilon(t)\\) are independent with \\(X(t)\\). We can to get back the original signal \\(X(t)\\) using a linear smoother: \\[\\hat{X}(t_i)=\\sum_{i=1}^{n} s_{i}(t_j)Y(t_i) \\Rightarrow \\mathbf{\\hat{X}}=\\mathbf{S}\\mathbf{Y} \\] where \\(s_{i}(t_j)\\) is the weight that the point \\(t_j\\) gives to the point \\(t_i\\). We use two methods to estimate the smoothing matrix \\(S\\). Finite representation in a fixed basis (Ramsay and Silverman 2005a): Let \\(X(t)\\in \\mathcal{L}_2\\), \\[ X(t)= \\sum_{k\\in\\mathbb{N}}{c_k\\phi_k(t)}\\approx\\sum_{k=1}^K c_k\\phi_k(t)=c^{\\top}\\Phi\\] The smoothing matrix is given by: \\(\\mathbf{S}=\\Phi(\\Phi^{\\top}W\\Phi+\\lambda R)^{-1}\\Phi^{\\top}W\\) where \\(\\lambda\\) is the penalty parameter. Type of basis: Fourier: Design to represent periodic functions. Orthonormal BSplines: Set of polynomials (of order m) defined in subintervals constructed in such a way that in the border of the subintervals the polynomials coincide (up to \\(m - 2\\) derivative). Banded Wavelets: Powerful especially when the grid is a power of 2. Orthonormal Polynomials: Adjust a polynomial to the whole curve. Example: raw (left) and smoothed temperature curves with 111 (center) and 11 (right) elements of a B-spline base: bsp11 &lt;- create.bspline.basis(temp$rangeval,nbasis=11) bsp111 &lt;- create.bspline.basis(temp$rangeval,nbasis=111) S.bsp11 &lt;- S.basis(temp$argvals, bsp11) S.bsp111 &lt;- S.basis(temp$argvals, bsp111) temp.bsp11 &lt;- temp.bsp111 &lt;- temp temp.bsp11$data &lt;- temp$data%*%S.bsp11 temp.bsp111$data &lt;- temp$data%*%S.bsp111 par(mfrow=c(1,3)) plot(temp) plot(temp.bsp111, main=&quot;111 Bspline basis elements&quot;) plot(temp.bsp11, main=&quot;11 Bspline basis elements&quot;) # Another way out1 &lt;- optim.basis(temp, type.CV=CV.S) names(out1) ## [1] &quot;gcv&quot; &quot;numbasis&quot; &quot;lambda&quot; &quot;fdataobj&quot; &quot;fdata.est&quot; ## [6] &quot;gcv.opt&quot; &quot;numbasis.opt&quot; &quot;lambda.opt&quot; &quot;S.opt&quot; &quot;base.opt&quot; out1$numbasis.opt ## [1] 76 Kernel Smoothing (Frédéric Ferraty and Vieu 2006) The problem is to estimate the smoothing parameter or bandwidth \\(\\nu=h\\) that better represents the functional data using kernel smoothing. Now,the nonparametric smoothing of functional data is given by the smoothing matrix \\(S\\): \\[s_{ij}=\\frac{1}{h}K\\left(\\frac{t_i-t_j}{h}\\right)\\] \\[S(h)=(s_j(t_i))=\\frac{K\\left(\\frac{t_i-t_j}{h}\\right)}{\\sum_{k=1}^{\\top}K\\left(\\frac{t_k-t_j}{h}\\right)}\\] where \\(h\\) is the bandwidth and \\(K()\\) the Kernel function. Different types of kernels \\(K()\\) are defined in the package, see Kernel function. Example: three examples of non-parametric smoothing with different values of the bandwidth parameter (\\(h = 1\\), left; \\(h = 10\\), center and \\(h\\) selected by the GCV criteria, right). S &lt;- S.NW(temp$argvals,h=1) temp &lt;- aemet$temp temp.hat &lt;- temp temp.hat$data &lt;- temp$data%*%S args(optim.np) ## function (fdataobj, h = NULL, W = NULL, Ker = Ker.norm, type.CV = GCV.S, ## type.S = S.NW, par.CV = list(trim = 0, draw = FALSE), par.S = list(), ## correl = TRUE, verbose = FALSE, ...) ## NULL # Another way temp.est &lt;- optim.np(temp,h=1)$fdata.est par(mfrow=c(1,3)) plot(temp.est,main=&quot;Kernel smooth - h=1&quot;) temp.est == temp.hat ## [1] FALSE plot(temp.hat, main=&quot;Kernel smooth - h=10&quot;) plot(optim.np(temp)$fdata.est, main=&quot;Kernel smooth- h-GCV&quot;) Finite representation in a Data-driven basis (Cardot, Ferraty, and Sarda 1999) Functional Principal Components (FPC) Functional Principal Components Analysis (FPCA) and the Functional PLS (FPLS) allow display the functional in a few components. The functional data can be rewritten as a decomposition in an orthonormal PC basis: maximizing the variance of \\(X(t)\\): \\[ \\hat{X}_{i}(t)=\\sum_{i=1}^{K}f_{ik}\\xi_{k}(t) \\] where \\(f_{i,k}\\) is the score of the principal component PC, \\(\\xi_k\\). Example: Functional principal component analysis (FPCA) y &lt;- rowSums(aemet$logprec$data) par(mfrow=c(1,3)) plot(aemet$temp) plot((pc &lt;- create.pc.basis(aemet$temp,l=1:2))[[&quot;fdata.est&quot;]], main=&quot;Temperature, 2 FPC&quot;) pc2 &lt;- fdata2pc(aemet$temp) plot(pc2$rotation, main=&quot;2 FPC basis&quot;, xlab=&quot;day&quot;) summary(pc2) ## [1] TRUE ## ## - SUMMARY: fdata2pc object - ## ## -With 2 components are explained 98.78 % ## of the variability of explicative variables. ## ## -Variability for each component (%): ## [1] 85.57 13.21 Functional Partial Least Squares (FPLS) (Krämer and Sugiyama 2011) The PLS components seek to maximize the covariance of \\(X(t)\\) and \\(y\\). args(create.pls.basis) args(fdata2pls) An integrated version of these functions is coming. Example: Exploratory analysis for spectrometric curves Tecator dataset: 215 spectrometric curves of meat samples also with Fat, Water and Protein contents obtained by analytic procedures. Tecator Goal: Explain the fat content through spectrometric curves. data(tecator) names(tecator) ## [1] &quot;absorp.fdata&quot; &quot;y&quot; names(tecator$y) ## [1] &quot;Fat&quot; &quot;Water&quot; &quot;Protein&quot; As shown in tecator Figure, the plot of \\(X(t)\\) against \\(t\\) is not necessarily the most informative and other semi-metric can allow to extract much information from functional variables. The information about fat seems to be contained in the shape of the curves, so, the semimetric of derivatives could be preferred to \\(\\mathcal{L}_2\\). It is difficult to find the best plot given a particular functional dataset because the shape of graphics depends strongly on the proximity measure. x &lt;- tecator$absorp par(mfrow=c(1,2)) boxplot(tecator$y$Fat) ycat &lt;- ifelse(tecator$y$Fat &lt; 20,2,4) plot(x, col=ycat) 1.3.1 Derivatives To compute derivatives there are several options (see fdata.deriv function): Raw Diferentiation: Only interesting when the number of discretization points are dense. Basis representation: Dierentiate a finite representation in a basis. Spline Interpolation: Perform a spline interpolation of given data points and use this interpolation for computing derivatives. Nonparametric Estimation: Use a Local Polynomial Estimator of the functional data. Example: ### Computing distances, norms and inner products Utilities for computing distances, norm and inner products are included in the package. Below are described those but of course, many others can be built with the only restriction that the first two arguments correspond to class –fdata–. In addition, the procedures of the package fda.usc that contains the argument –metric– allow the use of metric or semi-metric functions as shown in the following sections. Distance between functional elements, metric.lp: \\(d(X(t),Y(t))=\\left\\|X(t)-Y(t)\\right\\|\\) with a norm \\(\\left\\| \\cdot \\right\\|\\) Norm norm.fdata: \\(\\left\\|X(t)\\right\\|=\\sqrt{\\left\\langle X(t),X(t)\\right\\rangle}\\) Inner product inprod.fdata: \\(\\left\\langle x,y \\right\\rangle=(1/4)(\\left\\|x+y\\right\\|^2-\\left\\|x-y\\right\\|^2)\\). fda.usc package collects several metric and semi-metric functions which allow to extract as much information possible from the functional variable. Option 1: If the data is in a Hilbert space, represent your data in a basis, and compute all the things accordingly. From a practical point of way, the representation of a functional datum must be approximated by a finite number of terms. semimetric.basis() Option 2: Approximates all quantities using numerical approximations (Valid for Hilbert and non-Hilbert spaces). This usually involves numerical approximations of integrals, derivatives and so on. A collection of semi-metrics proposed by (Frédéric Ferraty and Vieu 2006) are also included in the package. semimetric.pca(), based on the Principal Components. semimetric.mplsr(), based on the Partial Least Squares. semimetric.deriv(), based on B-spline representation. semimetric.hshift(), measure the horizontal shift effect. semimetric.fourier(), based on ther Fourier representation Example of computing norms Consider \\(X(t) = t^2; t\\in[0, 1]\\). In this case \\(||X_1|| =\\sqrt{1/5}=0.4472136\\) and \\(||X_2|| =1/3\\) t = seq(0, 1, len = 101) x2 = t^2 x2 = fdata(x2, t) (c(norm.fdata(x2), norm.fdata(x2, lp = 1))) # L2, L1 norm ## [1] 0.4472509 0.3333500 f1 = fdata(rep(1, length(t)), t) f2 = fdata(sin(2 * pi * t), t) * sqrt(2) f3 = fdata(cos(2 * pi * t), t) * sqrt(2) f4 = fdata(sin(4 * pi * t), t) * sqrt(2) f5 = fdata(cos(4 * pi * t), t) * sqrt(2) fou5 = c(f1, f2, f3, f4, f5) # Fourier basis is Orthonormal round(inprod.fdata(fou5), 5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 Example of computing distances set.seed(1:4) p &lt;- 1001 r &lt;- rnorm(p,sd=.1) x &lt;- seq(0,2*pi,length=p) fx &lt;- fdata((sin(x)+r)/sqrt(pi),x) fx0 &lt;- fdata(r,x) plot(c(fx,fx0)) round(metric.lp(fx,fx0)[1,1],3) ## [1] 1.003 round(semimetric.basis(fx,fx0,nbasis1=5,nbasis2=5)[1,1],3) ## [1] 0.993 round(semimetric.basis(fx,fx0,nbasis1=11,nbasis2=111)[1,1],3) ## [1] 0.999 round(semimetric.fourier(fx,fx0,nbasis=5)[1,1],3) ## [1] 0.997 round(semimetric.fourier(fx,fx0,nbasis=11)[1,1],3) ## [1] 0.999 round(semimetric.deriv(fx,fx0,nderiv=0,nknot=5)[1,1],3) ## [1] 0.993 round(semimetric.deriv(fx,fx0,nderiv=0,nknot=11)[1,1],3) ## [1] 0.998 integrate(function(x){(sin(x)/sqrt(pi))^2},0,2*pi) ## 1 with absolute error &lt; 7.3e-10 Example of semi-metric as classification rule data(tecator) set.seed(4:1) #ind &lt;- sample(215,30) #xx &lt;- tecator$absorp[ind] yy &lt;- ifelse(tecator$y$Fat&lt;17,0,1) ind &lt;- c(sample(which(yy==0),10),sample(which(yy==1),10)) xx &lt;- fdata.deriv(tecator$absorp[ind]) yy &lt;- yy[ind] par(mfrow=c(1,1)) d1 &lt;- as.dist(metric.lp(xx),T,T) c1 &lt;- hclust(d1) c1$labels=yy plot(c1,main=&quot;Raw data -- metric.lp&quot;,xlab=&quot;Class of eah leaf&quot;) Try with the derivative of the curves: ## ## yy 1 2 ## 0 10 0 ## 1 8 2 ## ## yy 1 2 ## 0 10 0 ## 1 6 4 Now the fhclust() function from the fda.clust package allows the user to do this easily. The call would be fhclust(xx) See help for more details. Extension: Multivariate Functional Data To combine the information of multiple components, a \\(p\\)-dimensional metric can be used. A common choice is the Euclidean metric, defined as: \\[ m\\left(\\left(x_0^1, \\ldots, x_0^p\\right), \\left(x_i^1, \\ldots, x_i^p\\right)\\right) := \\sqrt{m_1\\left(x_0^1, x_i^1\\right)^2 + \\cdots + m_p\\left(x_0^p, x_i^p\\right)^2} \\] where \\(m_{i}\\) represents the metric corresponding to the \\(i\\)-th component of the product space. For implementation details, see the fda.usc:::metric.ldata function. It is essential to ensure that the metrics for the individual components have similar scales to prevent one component from disproportionately influencing the overall distance. # Checking the range of two components to assess scale differences range(d1) ## [1] 0.005531143 0.091797359 range(d2) ## [1] 0.0003143458 0.0105466510 Now the mfhclust() function from the fda.clust package allows the user to do this easily. The call would be: ## ## yy 1 2 ## 0 10 0 ## 1 8 2 ## ## yy 1 2 ## 0 10 0 ## 1 6 4 ## ## yy 1 2 ## 0 10 0 ## 1 5 5 And an example with real data for the weather in Spain: data(aemet, package = &quot;fda.usc&quot;) datos &lt;- mfdata(&quot;temp&quot;=aemet$temp,&quot;logprec&quot;=aemet$logprec) # dd = metric.mfdata(datos) # Perform hierarchical clustering using the default method (ward.D2) result &lt;- mfhclust(datos, method = &quot;ward.D2&quot;) # Plot the dendrogram plot(result, main = &quot;Dendrogram of Multivariate Functional Data (Ward&#39;s Method)&quot;) # Cut the dendrogram into clusters groups &lt;- cutree(result, k = 3) colors &lt;- c(rgb(1, 0, 0, alpha = 0.25), rgb(0, 1, 0, alpha = 0.25), rgb(0, 0, 1, alpha = 0.25)) gcolors &lt;- colors[groups] par(mfrow=c(1,3)) plot(aemet$temp,col=gcolors ) plot(aemet$logprec,col=gcolors ) plot(aemet$df[7:8],col=gcolors ,asp=T) See Example of multivariate FDA usingh Spanish Weather to view a use case. NOTE: Dependent data in tecator dataset? # acf(tecator$y$Fat) ar(tecator$y$Fat) ## ## Call: ## ar(x = tecator$y$Fat) ## ## Coefficients: ## 1 2 ## 0.6552 0.1161 ## ## Order selected 2 sigma^2 estimated as 72.82 # plot(tecator$y,col=1:3) par(mfrow=c(1,1)) # ts.plot(tecator$y$Fat) barplot(t(tecator$y)) 1.4 Correlation Distances (G. J. Székely, Rizzo, and Bakirov 2007) The correlation Distances characterizes independence between vectors of arbitrary finite dimensions. Recently, in (Gábor J. Székely and Rizzo 2013), a bias-corrected version is considered and a test of independence developed. data(tecator) xx &lt;- tecator$absorp xx.d2 &lt;- fdata.deriv(xx,nderiv=2) dcor.xy(xx,xx.d2) ## ## dcor t-test of independence ## ## data: D1 and D2 ## T = 35.933, df = 22789, p-value &lt; 2.2e-16 ## sample estimates: ## Bias corrected dcor ## 0.2315581 d1 &lt;- metric.lp(xx) d2 &lt;- metric.lp(xx.d2) bcdcor.dist(d1,d2) ## [1] 0.2315581 Fat &lt;- tecator$y[,&quot;Fat&quot;,drop=FALSE] d3 &lt;- metric.dist(Fat) bcdcor.dist(d1,d3) ## [1] 0.1963254 bcdcor.dist(d2,d3) ## [1] 0.9147076 # Functional / Factor Fat.cut &lt;- as.matrix(cut(Fat[,1],labels=1:4,breaks=4)) dcor.xy(Fat.cut,xx.d2) ## ## dcor t-test of independence ## ## data: D1 and D2 ## T = 252.57, df = 22789, p-value &lt; 2.2e-16 ## sample estimates: ## Bias corrected dcor ## 0.8583661 1.4.1 Depth for functional data Depth (in univariate or multivariate context) is a statistical tool that provides a center-outward ordering of data points. This ordering can be employed to define location measures (and by oposition outliers). Also, some measures can be defined as maximizers of a particular depth. Mahalanobis Depth (Mean) mdepth.MhD Halfspace Depth, also known as Tukey Depth (Median) mdepth.HS Convex Hull Peeling Depth (Mode) Oja Depth Simplicial Depth mdepth.SD Likelihood Depth (Mode) mdepth.LD y12 &lt;- tecator$y[,1:2] dep &lt;- mdepth.LD(x=y12,scale=T)$dep plot(y12,col=grey(1-dep)) points(y12[which.max(dep),],col=4,lwd=3) Many of the preceding depth functions defined in a multivariate contex cannot be applied to functional data. In any case, given a depth for functional data. The deepest point is a location measure and based on the depth function can have a dierent interpretation: Mean, Median, Mode … Computing the depths of the whole sample and ordering them (in decreasing order) gives a rank from the most central point (\\(x_{[1]}\\)) to the most outlying one (\\(x_{[n]}\\)). So, those points with larger rank are the candidates to be outliers w.r.t. the data cloud (in the sense of the depth function). Summarizing the \\((1-\\alpha)\\) deepest points could lead also to a robust location measure. Fraiman-Muniz Depth (Fraiman and Muniz 2001) depth.FM Modal Depth (Cuevas, Febrero, and Fraiman 2007) depth.mode Random Projection Depth (Cuevas, Febrero, and Fraiman 2007) depth.RP Random Tukey Depth (J. Cuesta-Albertos and Nieto-Reyes 2008) depth.RT Band Depth (López-Pintado and Romo 2009) depth.MBD (private function) x &lt;- tecator$absorp depth.mode(x,draw=TRUE) 1.4.2 Depth (and distances) for multivariate functional data (J. A. Cuesta-Albertos, Febrero-Bande, and Oviedo de la Fuente 2017) Modify the procedure to incorporate the extended information. Fraiman–Muniz: Compute a multivariate depth marginally. depth.FMp Modal depth: Use a new distance between data (for derivatives, for example, the Sobolev metric). depth.FMp Random Projection: Consider a multivariate depth to be applied to the dierent projections (also the random projection method could be applied twice). depth.RPp x &lt;- tecator$absorp x.d1 &lt;- fdata.deriv(x) depth.modep(list(x=x,x.d1=x.d1),draw=T) Example poblenou dataset Hourly levels of nitrogen oxides in Poblenou (Barcelona). This dataset has 127 daily records (2005/01/06-2005/06/26). Objective: Explain the diferences in NOx levels as a function of day. data(poblenou) dayw = ifelse(poblenou$df$day.week == 7 | poblenou$df$day.festive ==1, 3, ifelse(poblenou$df$day.week == 6, 2, 1)) plot(poblenou$nox,col=dayw) The \\(\\mathcal{L}_2\\) space (distance is area between curves) seems appropriate although other possibilities could be take into account (\\(\\mathcal{L}_1\\), for example). fmd = depth.FM(poblenou$nox,draw=T) md = depth.mode(poblenou$nox) rpd = depth.RP(poblenou$nox, nproj = 50) rtd = depth.RT(poblenou$nox) print(cur &lt;- c(fmd$lmed, md$lmed, rpd$lmed, rtd$lmed)) ## 2005-05-06 2005-05-06 2005-05-06 2005-03-04 ## 63 63 63 10 plot(poblenou$nox,col=&quot;grey&quot;) lines(poblenou$nox[cur], lwd = 2, lty = 1:4, col = 1:4) legend(&quot;topleft&quot;, c(&quot;FMD&quot;, &quot;MD&quot;, &quot;RPD&quot;, &quot;RTD&quot;), lwd = 2, lty = 1:4,col = 1:4) 1.4.3 Outliers detection There is no general accepted definition of outliers in Functional data so, we define outlier as a datum generated from a dierent process than the rest of the sample with the following characteristics Its number in the sample is unknown but probably low. An outlier will have low depth and it will be an outlier in the sense of the depth used. md1 = depth.mode(lab &lt;- poblenou$nox[dayw == 1]) #Labour days md2 = depth.mode(sat &lt;- poblenou$nox[dayw == 2]) #Saturdays md3 = depth.mode(sun &lt;- poblenou$nox[dayw == 3]) #Sundays/Festive rbind(poblenou$df[dayw == 1, ][which.min(md1$dep), ], poblenou$df[dayw == 2, ][which.min(md2$dep), ], poblenou$df[dayw == 3, ][which.min(md3$dep),]) ## date day.week day.festive ## 22 2005-03-18 5 0 ## 57 2005-04-30 6 0 ## 58 2005-05-01 7 0 plot(poblenou$nox,col=&quot;grey&quot;) lines(poblenou$nox[c(22,57,58),], lwd = 2, lty = 2:4, col = 2:4) # Method for detecting outliers, see Febrero-Bande, 2008 #out1 &lt;- outliers.depth.trim(poblenou$nox[dayw == 1],nb=100)$outliers Two procedures for detecting outliers are implemented in the package. (Febrero-Bande, Galeano, and González-Manteiga 2008) Detecting outliers based on trimming: outliers.depth.trim() Detecting outliers based on weighting: outliers.depth.pond() # Method for detecting outliers, see Febrero-Bande, 2008 #out1 &lt;- outliers.depth.trim(poblenou$nox[dayw == 1],nb=100)$outliers #out2 &lt;- outliers.depth.pond(poblenou$nox[dayw == 1],nb=100)$outliers References References Cardot, Hervé, Frédéric Ferraty, and Pascal Sarda. 1999. “Functional Linear Model.” Statist. Probab. Lett. 45 (1): 11–22. Cuesta-Albertos, Juan A, Manuel Febrero-Bande, and Manuel Oviedo de la Fuente. 2017. Test 26 (1): 119–42. Cuesta-Albertos, Juan, and Alicia Nieto-Reyes. 2008. “The Random Tukey Depth.” Computational Statistics and Data Analysis 52 (11): 4979–88. Cuevas, Antonio, Manuel Febrero, and Ricardo Fraiman. 2007. “Robust Estimation and Classification for Functional Data via Projection-Based Depth Notions.” Comput. Statist. 22 (3): 481–96. http://link.springer.com/article/10.1007/s00180-007-0053-0. Febrero-Bande, Manuel, Pedro Galeano, and Wenceslao González-Manteiga. 2008. “Outlier Detection in Functional Data by Depth Measures, with Application to Identify Abnormal \\({\\rm NO}_x\\) Levels.” Environmetrics 19 (4): 331–45. ———. 2006. Nonparametric Functional Data Analysis. Springer Series in Statistics. New York: Springer-Verlag. Fraiman, Ricardo, and Graciela Muniz. 2001. “Trimmed Means for Functional Data.” Test 10 (2): 419–40. Krämer, Nicole, and Masashi Sugiyama. 2011. “The Degrees of Freedom of Partial Least Squares Regression.” Journal of the American Statistical Association 106 (494): 697–705. López-Pintado, Satan, and Juan Romo. 2009. “On the Concept of Depth for Functional Data.” Journal of the American Statistical Association 104: 718–34. Muller, Hans-Georg et al. 2008. “Functional Modeling of Longitudinal Data.” In Longitudinal Data Analysis, 237–66. Chapman; Hall/CRC. Ramsay, J. O., and B. W. Silverman. 2005a. Functional Data Analysis. Springer. Székely, G. J., M. L. Rizzo, and N. K. Bakirov. 2007. “Measuring and Testing Dependence by Correlation of Distances.” The Annals of Statistics 35 (6): 2769–94. http://projecteuclid.org/euclid.aos/1201012979. Székely, Gábor J, and Maria L Rizzo. 2013. “The Distance Correlation \\(t\\)-Test of Independence in High Dimension.” J. Multivariate Anal. 117: 193–213. "],["regression.html", "Chapter 2 Functional Regression Model 2.1 Functional linear model (FLR) with basis representation 2.2 FLM with functional and non functional covariates 2.3 Other procedures 2.4 Non Linear Model (Frédéric Ferraty and Vieu 2006) 2.5 Semi Linear Model (Aneiros-Pérez and Vieu 2006) 2.6 Generalized Linear Models (Müller and Stadtmüller 2005) 2.7 Generalized Functional Additive Model 2.8 Functional GLS model 2.9 Functional Response Model 2.10 Other Models: References", " Chapter 2 Functional Regression Model Regression models are those techniques for modeling and analyzing the relationship between a dependent variable and one or more independent variables. When one of the variables have a functional nature, we have functional regression models. This section is devoted to all the functional regression models where the response variable is scalar and at least, there is one functional covariate. For illustration, we will use the Tecator dataset to predict the fat contents from The explanatory variables to introduce in the models are:p The curves of absorbance \\(X(t)\\) as functional data or one of its two first derivatives (\\(X.d1,X.d2\\)) and/or Water content as real variable. library(fda.usc.devel) data(tecator) absorp&lt;-tecator$absorp ind&lt;-sample(215,129) #ind = 1:129 tt = absorp[[&quot;argvals&quot;]] y = tecator[[&quot;y&quot;]]$Fat[ind] X = absorp[ind, ] X.d1 = fdata.deriv(X, nbasis = 19, nderiv = 1) X.d2 = fdata.deriv(X, nbasis = 19, nderiv = 2) par(mfrow=c(2,2)) plot(X) plot(X.d1) plot(X.d2) boxplot(y) In the following sections, regression methods implemented –fda.usc– pacakge in the package are presented one by one and illustrated with examples for estimating the Fat content of the Tecator dataset. 2.1 Functional linear model (FLR) with basis representation Supose that \\(\\mathcal{X} \\in \\mathcal{L}_{2}(T)\\) and \\(y \\in \\mathbb{R}\\). Assume also that \\(\\mathbb{E}[\\mathcal{X}(t)]=0, \\forall t \\in [0,T]\\) and \\(\\mathbb{E}[y]=0\\). The FLM states that \\[y= \\left\\langle \\mathcal{X},\\beta \\right\\rangle +\\varepsilon=\\int_{T}X(t)\\beta(t)dt+\\varepsilon\\] where \\(\\beta \\in \\mathcal{L}_{2}(T)\\) and \\(\\varepsilon\\) is the errror term. One way of estimating \\(\\beta\\), it is representing the parametmer (and \\(\\mathcal{X}\\)) in a \\(\\mathcal{L}_2\\)-basis in the following way: \\[\\beta(t)=\\sum_k \\beta_k \\theta_k(t), \\mathbf{X}(t)=\\sum_k c_i \\psi_k(t)\\] fregre.basis() fucntion uses fixed basis: B–spline, Fourier, etc. Ramsay and Silverman (2005b), Cardot, Ferraty, and Sarda (1999)) The next code illustrates how to estimate the fat contents using a sample of absorbances curves. rangett &lt;- X$rangeval basis1 = create.bspline.basis(rangeval = rangett, nbasis = 17) basis2 = create.bspline.basis(rangeval = rangett, nbasis = 7) res.basis0 = fregre.basis(X, y, basis.x = basis1, basis.b = basis2) res.basis1 = fregre.basis(X.d1, y, basis.x = basis1, basis.b = basis2) res.basis2 = fregre.basis(X.d2, y, basis.x = basis1, basis.b = basis2) res.basis0$r2;res.basis1$r2;res.basis2$r2 ## [1] 0.9385496 ## [1] 0.9360606 ## [1] 0.9518397 summary(res.basis2) ## *** Summary Functional Data Regression with representation in Basis *** ## ## Call: ## fregre.basis(fdataobj = X.d2, y = y, basis.x = basis1, basis.b = basis2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.9498 -1.5962 -0.2428 1.8891 6.1841 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.759e+01 2.636e-01 66.723 &lt; 2e-16 *** ## Spectrometriccurves.bspl4.1 -1.294e+04 3.849e+03 -3.361 0.001040 ** ## Spectrometriccurves.bspl4.2 9.261e+03 2.901e+03 3.192 0.001801 ** ## Spectrometriccurves.bspl4.3 -1.215e+03 1.426e+03 -0.852 0.395973 ## Spectrometriccurves.bspl4.4 9.804e+02 1.092e+03 0.897 0.371275 ## Spectrometriccurves.bspl4.5 -1.599e+03 1.126e+03 -1.420 0.158232 ## Spectrometriccurves.bspl4.6 6.896e+03 1.802e+03 3.826 0.000207 *** ## Spectrometriccurves.bspl4.7 -7.985e+03 1.438e+03 -5.554 1.69e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.994 on 121 degrees of freedom ## Multiple R-squared: 0.9518, Adjusted R-squared: 0.9491 ## F-statistic: 341.6 on 7 and 121 DF, p-value: &lt; 2.2e-16 ## ## -Names of possible atypical curves: No atypical curves ## -Names of possible influence curves: 140 par(mfrow=c(1,3)) plot(res.basis0$beta.est) ## [1] &quot;done&quot; plot(res.basis1$beta.est) ## [1] &quot;done&quot; plot(res.basis2$beta.est) ## [1] &quot;done&quot; The choice of the appropiate basis (and the number of basis elements) becomes now in a crucial step: res.basis.cv = fregre.basis(X, y) summary(res.basis.cv) ## *** Summary Functional Data Regression with representation in Basis *** ## ## Call: ## fregre.basis(fdataobj = X, y = y) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.3657 -2.1421 -0.1094 2.1998 6.5177 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.5884 0.2757 63.790 &lt;2e-16 *** ## Spectrometriccurves.bspl4.1 -200.0776 93.7095 -2.135 0.0348 * ## Spectrometriccurves.bspl4.2 242.9587 112.0151 2.169 0.0321 * ## Spectrometriccurves.bspl4.3 -133.6675 72.6212 -1.841 0.0682 . ## Spectrometriccurves.bspl4.4 23.7801 33.6901 0.706 0.4817 ## Spectrometriccurves.bspl4.5 14.2783 19.4473 0.734 0.4643 ## Spectrometriccurves.bspl4.6 -23.9989 16.8771 -1.422 0.1577 ## Spectrometriccurves.bspl4.7 46.5994 27.9568 1.667 0.0982 . ## Spectrometriccurves.bspl4.8 -104.1480 65.9443 -1.579 0.1169 ## Spectrometriccurves.bspl4.9 154.0717 108.4606 1.421 0.1581 ## Spectrometriccurves.bspl4.10 -123.4221 94.1397 -1.311 0.1924 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.132 on 118 degrees of freedom ## Multiple R-squared: 0.9486, Adjusted R-squared: 0.9443 ## F-statistic: 217.8 on 10 and 118 DF, p-value: &lt; 2.2e-16 ## ## -Names of possible atypical curves: 43 ## -Names of possible influence curves: 86 89 140 43 6 Functional Principal Components (FPC).(Cardot, Ferraty, and Sarda 1999), fregre.pc() x&lt;-X basis.pc0 = create.pc.basis(X,1:3) res.pc1 = fregre.pc(X, y, basis.x = basis.pc) summary(res.pc1) ## *** Summary Functional Data Regression with Principal Components *** ## ## Call: ## fregre.pc(fdataobj = X, y = y, basis.x = basis.pc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.471 -4.389 0.886 5.328 14.585 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.58837 0.72555 24.241 &lt;2e-16 *** ## PC1 0.99689 0.09838 10.133 &lt;2e-16 *** ## PC2 -1.50819 1.18685 -1.271 0.206 ## PC3 -21.84347 1.90208 -11.484 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.241 on 125 degrees of freedom ## Multiple R-squared: 0.6231, Adjusted R-squared: 0.614 ## F-statistic: 68.88 on 3 and 125 DF, p-value: &lt; 2.2e-16 ## ## ## -With 3 Principal Components is explained 99.47 % ## of the variability of explicative variables. ## ## -Variability for each principal components -PC- (%): ## PC1 PC2 PC3 ## 98.54 0.66 0.26 ## -Names of possible atypical curves: No atypical curves ## -Names of possible influence curves: 18 185 99 140 44 res.pc2 = fregre.pc.cv(X, y) summary(res.pc2) ## Length Class Mode ## fregre.pc 19 fregre.fd list ## pc.opt 5 -none- numeric ## lambda.opt 1 -none- numeric ## PC.order 8 -none- numeric ## MSC.order 8 -none- numeric par(mfrow=c(1,2)) plot(res.pc1$beta.est) plot(res.pc2[[1]]$beta.est) 2.2 FLM with functional and non functional covariates \\[E(y)=\\alpha+\\mathbf{Z}\\beta+\\sum_{q=1}^Q \\left\\langle \\mathcal{X}^{q}(t),\\beta_{q}(t)\\right\\rangle \\] where \\(\\left\\{\\mathcal{X}_q(t)\\right\\}_{q=1}^Q\\) are function covariates and \\(\\mathbf{Z}=\\left\\{{Z_j}\\right\\}_{j=1}^J\\) the non–functional covariates. dataf = as.data.frame(tecator[[&quot;y&quot;]][ind,]) # Fat, Protein, Water basis.pc2 = create.pc.basis(X.d2,1:4) basis.x = list(X = basis.pc0, X.d2 =basis.pc2) f = Fat ~ X+X.d2 ldata = list(df = dataf, X=X,X.d2=X.d2) res.lm1 = fregre.lm(f, ldata, basis.x = basis.x) f = Fat ~ Water+X.d2 res.lm2 = fregre.lm(f, ldata, basis.x = basis.x) ## ## Call: ## lm(formula = pf, data = XX, x = TRUE) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.9417 -1.6197 -0.2995 1.5864 9.4955 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.5884 0.2659 66.142 &lt; 2e-16 *** ## X.PC1 0.1127 0.1023 1.102 0.27283 ## X.PC2 7.1807 3.2173 2.232 0.02746 * ## X.PC3 -19.8307 6.9644 -2.847 0.00518 ** ## X.d2.PC1 3066.6836 563.3407 5.444 2.78e-07 *** ## X.d2.PC2 5507.7858 2668.4922 2.064 0.04115 * ## X.d2.PC3 1879.3468 1017.4538 1.847 0.06717 . ## X.d2.PC4 -2644.9925 3131.4915 -0.845 0.39998 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.02 on 121 degrees of freedom ## Multiple R-squared: 0.951, Adjusted R-squared: 0.9482 ## F-statistic: 335.4 on 7 and 121 DF, p-value: &lt; 2.2e-16 2.2.1 Predict method for functional regression model 2.3 Other procedures Other procedures Partial Least Squares (FPLS). fregre.pls(), Preda and Saporta (2005) Penalized versions and parameter selection: fregre.pc.cv, fregre.basis.cv, fregre.np.cv (Febrero-Bande and Oviedo de la Fuente 2012) F-test for the FLM with scalar response: flm.Ftest, F-test (Garcı́a-Portugués, González-Manteiga, and Febrero-Bande 2014) Goodness-of-fit test for the FLM with scalar response: flm.test (Garcı́a-Portugués, González-Manteiga, and Febrero-Bande 2014) Measures of influence in FLM with scalar response: influence.fdata,(Febrero-Bande, Galeano, and González-Manteiga 2010) Beta parameter estimation by wild or smoothed bootstrap procedure: fregre.bootstrap FLM with a functional response: fregre.basis.fr (Chiou et al. 2004) 2.4 Non Linear Model (Frédéric Ferraty and Vieu 2006) Supose \\((\\mathcal{X},Y)\\) are a pair of r.v. with \\(y\\in \\mathbb{R}\\) where \\(\\mathbb{E}\\) is a semi-metric space. To predict the resonse \\(Y\\) with \\(\\mathcal{X}\\), the estimation is: \\[m(\\mathcal{X})=\\mathbb{E}(Y|X=\\mathcal{X})\\], where the NW estimator is given by: \\[\\hat{m}(\\mathcal{X})=\\frac{\\sum_{i=1}^n Y_i{ K(d(\\mathcal{X},X_i)/h)}}{\\sum_{i=1}^n {K(d(\\mathcal{X},X_i)/h)}}\\] where K is an asymmetric kernel function and h is the bandwidth parameter. 2.5 Semi Linear Model (Aneiros-Pérez and Vieu 2006) Let \\((\\mathcal{X},\\mathbf{Z},y)\\) with \\(y\\in \\mathbb{R}\\) (response), \\(\\mathcal{X}\\in \\mathbb{E}\\) (functional) and \\(\\mathbf{Z} \\in \\mathbb{R}^p\\) (MV covariates). \\[y = Z + m(X) + \\varepsilon\\] Arguments for fregre.np() and fregre.plm() function –Ker–: type of asymmetric kernel function, by default asymmetric normal kernel (cosine, epanechnicov, quadratic,….). –metric–: type of metric or semimetric. –type.S–: type of smoothing matrix \\(\\mathbf{S}\\): S.NW, S.LLR, S.KNN. tecator&lt;-list(&quot;df&quot;=tecator$y,&quot;absorp.fdata&quot;=tecator$absorp.fdata) X=tecator$absorp.fdata y&lt;-tecator$df$Fat np&lt;-fregre.np(X, y, metric = semimetric.deriv, nderiv = 1,type.S = S.KNN) Again, it has also implemented the function fregre.np.cv to estimate the smoothing parameter \\(h\\) by the validation criteria. np&lt;-fregre.np(X, y, metric = semimetric.deriv, nderiv = 1,type.S = S.KNN) np.cv&lt;-fregre.np.cv(X, y, metric = semimetric.deriv, nderiv = 1,type.S = S.KNN,h=c(3:9)) c(np$h.opt,np.cv$h.opt) ## [1] 12 3 c(np$r2,np.cv$r2) ## [1] 0.7388655 0.9438220 2.6 Generalized Linear Models (Müller and Stadtmüller 2005) One natural extension of LM model is the generalized functional linear regression model (GFLM) which allows various types of the response. In the GLM framework it is generally assumed that \\(y_i|X_i\\) can be chosen within the set of distributions belonging to the exponential family. In Generalized Functional Linear Model (FGLM), The scalar response \\(y\\)(belonging to a Exponential Family PDF) is estimated by functional \\(\\left\\{\\mathcal{X}_q(t)\\right\\}_{q=1}^Q\\) and also non–functional \\(\\mathbf{Z}=\\left\\{{Z_j}\\right\\}_{j=1}^J\\) covariates by: \\[E(y)=g^{-1}\\left(\\alpha+\\mathbf{Z}\\beta+\\sum_{q=1}^Q \\left\\langle \\mathcal{X}^{q}(t),\\beta_{q}(t)\\right\\rangle\\right) \\] where \\(g()\\) is the inverse link function. Example of logistic regression In logistic regression, the probability, \\(\\pi_i\\) , of the occurrence of an event, \\(Y_i = 1\\), rather than the event \\(Y_i = 0\\), conditional on a vector of covariates \\(\\mathcal{X}_i(t)\\) is expressed as: \\[ p_i = \\mathbb{P}[Y = 1|{X_i(t): t \\in T }]=\\frac{+exp\\left\\{\\alpha+\\int_{T}X_{i}(t)\\beta(t)dt \\right\\}}{1+exp\\left\\{\\alpha+\\int_{T}X_{i}(t)\\beta(t)dt \\right\\}}\\ , i= 1,\\ldots,n\\] with \\(\\epsilon\\) are the independent errors with zero mean. data(tecator) names(tecator)[2]&lt;-&quot;df&quot; tecator$df$fat15&lt;-ifelse(tecator$df$Fat&lt;15,0,1) tecator$absorp.d2=fdata.deriv(tecator$absorp.fdata,nderiv=2) res.glm&lt;-fregre.glm(fat15 ~ absorp.d2,data=tecator,family=binomial()) #summary(a) yfit&lt;-ifelse(res.glm$fitted.values&lt;.5,0,1) table(tecator$df$fat15,yfit) ## yfit ## 0 1 ## 0 111 1 ## 1 1 102 2.7 Generalized Functional Additive Model Generalized Functional Spectral Additive Linear Model (FGSAM), (Müller and Yao 2012) \\[E(y)=g^{-1}\\left(\\alpha+\\sum_{j=1}^J f_{j}\\left(\\mathbf{Z}^{j}\\right)+\\sum_{q=1}^Q s_q\\left(\\mathcal{X}_{i}^{q}(t)\\right)\\right)\\] where \\({f}(\\cdot),{s}(\\cdot)\\) are the smoothed functions. res.gsam&lt;-fregre.gsam(fat15~ s(absorp.d2),data=tecator,family=binomial()) yfit&lt;-ifelse(res.gsam$fitted&lt;.5,0,1) table(tecator$df$fat15,yfit) ## yfit ## 0 1 ## 0 112 0 ## 1 0 103 Generalized Functional Kernel Additive Linear Model (FGKAM), (Febrero-Bande and González-Manteiga 2013) \\[E(y)=g^{-1}\\left(\\alpha+\\sum_{q=1}^Q\\mathcal{K}\\left(\\mathcal{X}^{q}_i(t)\\right)\\right)\\] where \\(\\mathcal{K}(\\cdot)\\) is the kernel estimator. # tecator2&lt;-tecator[-1] # tecator$df$fat15 &lt;- as.factor(tecator$df$fat15) # res.gkam&lt;-fregre.gkam(fat15 ~ absorp.d2,data=tecator2, family=binomial(), # control = list(maxit = 1)) # res.gkam # yfit&lt;-ifelse(res.gkam$fitted.values&lt;.5,0,1) # table(tecator$df$fat15,yfit) 2.8 Functional GLS model See Oviedo de la Fuente et al. (2018) for more details about the below algorithm: A. Jointly estimation (nlme package): Minimize for \\((\\beta,\\theta)\\) the GLS criteria, i.e, \\[\\Psi(\\beta,\\theta)=\\left(y-\\left\\langle X,\\beta \\right\\rangle\\right)\\Sigma(\\theta)^{-1}\\left(y-\\left\\langle X,\\beta \\right\\rangle\\right)\\] B. Iterative Estimation: In multivariate case, Zivot and Wang (2007) show that estimation of \\(\\beta\\) by \\(\\hat{\\beta}_{ML}\\) is equivalent to the iterative estimation of \\(\\hat{\\beta}\\) recomputed at each iteration by the update estimator of \\(\\Sigma\\). Begin with a preliminary estimation of \\(\\hat{\\theta}=\\theta_0\\). Compute \\(\\hat{W}=\\Sigma(\\theta_0)^{-1}\\). Estimate \\({b}_\\Sigma={(Z^\\prime\\hat{W}Z)^{-1}Z^\\prime\\hat{W}}y\\) Based on \\(\\hat{e}=({y-{Z}{b}_\\Sigma})\\), update \\(\\hat{\\theta}=\\rho({\\hat{e}})\\) where \\(\\rho\\) depends on the dependence structure chosen. Repeat steps 2 and 3 until convergence. The generalized correlated cross-validation (GCCV) criterion is an extension to GCV within the context of correlated errors, Carmack, Spence, and Schucany (2012). It is defined as follows: \\[GCCV(K_x,K_\\beta,\\mathbf{b},\\phi)=\\frac{\\sum_{i=1}^n \\left(y_{i}-\\hat{y}_{i,\\mathbf{b}}\\right)^2}{ \\left({1-\\frac{{tr}(\\mathbf{G})}{n}}\\right)^2} \\] where \\({G}=2{H}\\Sigma(\\phi)-{H}\\Sigma(\\phi)H^\\prime\\) takes into account the effect of the dependence, the trace of \\({G}\\) is an estimation of the degrees of freedom consumed by the model and \\({H}\\) is the hat matrix. The important advantage of this criterion is that it is rather easy to compute because it avoids the need to compute the inverse of the matrix \\(\\Sigma\\). Even so, the complexity of the GLS criterion depends on the structure of \\(\\Sigma\\) and it could sometimes be hard either to minimize or computationally expensive. 2.8.1 Dependent data example, We use the fregre.gls() function that has the same arguments as the fregre.lm() function and: correlation argument, same functionality as in gls() and criteria argument, it require GCCV.S() function to calculate the GCCV score proposed by Carmack, Spence, and Schucany (2012). data(tecator) ts.plot(tecator[[&quot;y&quot;]][,&quot;Fat&quot;]) cor(tecator[[&quot;y&quot;]][,&quot;Fat&quot;,drop=F],tecator[[&quot;y&quot;]][,&quot;Water&quot;,drop=F]) ## Water ## Fat -0.9881002 cor(tecator[[&quot;y&quot;]][,&quot;Fat&quot;,drop=F],tecator[[&quot;y&quot;]][,&quot;Protein&quot;,drop=F]) ## Protein ## Fat -0.8608965 dcor.xy(tecator[[&quot;y&quot;]][,&quot;Fat&quot;,drop=F],tecator[[&quot;y&quot;]][,&quot;Water&quot;,drop=F]) ## ## dcor t-test of independence ## ## data: D1 and D2 ## T = 571.71, df = 22789, p-value &lt; 2.2e-16 ## sample estimates: ## Bias corrected dcor ## 0.9668619 dcor.xy(tecator[[&quot;y&quot;]][,&quot;Fat&quot;,drop=F],tecator[[&quot;y&quot;]][,&quot;Protein&quot;,drop=F]) ## ## dcor t-test of independence ## ## data: D1 and D2 ## T = 155.43, df = 22789, p-value &lt; 2.2e-16 ## sample estimates: ## Bias corrected dcor ## 0.7173448 x.d2&lt;-fdata.deriv(tecator[[&quot;absorp.fdata&quot;]],nderiv=2) ldata=list(&quot;df&quot;=tecator[[&quot;y&quot;]],&quot;x.d2&quot;=x.d2) res.gls=fregre.gls(Fat~x.d2, data=ldata, correlation=corAR1()) coef(res.gls[[&quot;modelStruct&quot;]],F) ## corStruct.Phi ## 0.4942661 The previous model is restricted to a structure determined by gls() function of nlme The function fregre.igls() is presented as an alternative because it allows any type of dependence structures designed by the user. The code bellow shows a simple use of iterative scheme (iGLS). In particular, we use a iGLS-AR(\\(p=1\\)) scheme for error estimation. res.igls=fregre.igls(Fat~x.d2, data=ldata, correlation=list(&quot;cor.ARMA&quot;=list()),control=list(&quot;p&quot;=1)) coef(res.igls[[&quot;corStruct&quot;]][[1]]) ## ar1 ## 0.488854 res.igls ## ## Call: ## list(&quot;fregre.basis&quot;) ## ## Coefficients: ## (Intercept) x.d2.bspl4.1 x.d2.bspl4.2 x.d2.bspl4.3 x.d2.bspl4.4 ## 18.12 -608.20 6203.09 -8252.76 6271.43 ## x.d2.bspl4.5 ## -7156.85 res.igls$corStruct ## $ar ## ## Call: ## arima(x = x, order = c(p, d, q), include.mean = FALSE, transform.pars = TRUE) ## ## Coefficients: ## ar1 ## 0.4889 ## s.e. 0.0600 ## ## sigma^2 estimated as 8.076: log likelihood = -529.76, aic = 1063.53 Both examples estimate an AR(1) with \\(\\phi=0.49\\). Thus, the estimation and the prediction made with these models will be more accurate than the classical functional models in which it is assumed that the errors are independent. 2.9 Functional Response Model Reference papers: Faraway (1997), Frédéric Ferraty, Van Keilegom, and Vieu (2012) R expample of function fregre.basis.fr() data(aemet) log10precfdata&lt;-aemet$logprec; tempfdata&lt;-aemet$temp res2&lt;-fregre.basis.fr(tempfdata,log10precfdata) i&lt;-1 plot(log10precfdata[i],lty=1,main=paste0(&quot;Weather station, &quot;,i)) lines(res2$fitted.values[i],lty=2,lwd=2,col=4) 2.10 Other Models: Functional Quantile Regession Model, see Kato et al. (2012), Cardot, Crambes, and Sarda (2005). Functional Single Index Model, see Frédéric Ferraty, Park, and Vieu (2011). Functional Projection Pursuit Regression Model, see Frédéric Ferraty et al. (2013). Functional Machine Learning methods (SVM, RPART, NNET, random Forest) Among others. References References Aneiros-Pérez, Germán, and Philippe Vieu. 2006. “Semi-Functional Partial Linear Regression.” Statist. Probab. Lett. 76 (11): 1102–10. Cardot, Hervé, Christophe Crambes, and Pascal Sarda. 2005. “Quantile Regression When the Covariates Are Functions.” Nonparametric Statistics 17 (7): 841–56. Cardot, Hervé, Frédéric Ferraty, and Pascal Sarda. 1999. “Functional Linear Model.” Statist. Probab. Lett. 45 (1): 11–22. Carmack, Patrick S, Jeffrey S Spence, and William R Schucany. 2012. “Generalised Correlated Cross-Validation.” Journal of Nonparametric Statistics 24 (2): 269–82. Chiou, Jeng-Min, Hans-Georg Muller, Jane-Ling Wang, et al. 2004. “Functional Response Models.” Statistica Sinica 14 (3): 675–94. Faraway, Julian J. 1997. “Regression Analysis for a Functional Response.” Technometrics 39 (3): 254–61. ———. 2010. “Measures of Influence for the Functional Linear Model with Scalar Response.” J. Multivariate Anal. 101 (2): 327–39. Febrero-Bande, Manuel, and Wenceslao González-Manteiga. 2013. “Generalized Additive Models for Functional Data.” Test 22 (2): 278–92. http://dx.doi.org/10.1007/s11749-012-0308-0. Febrero-Bande, Manuel, and M Oviedo de la Fuente. 2012. “Statistical Computing in Functional Data Analysis: The R Package fda.usc.” J. Statist. Software 51 (4): 1–28. Ferraty, Frédéric, Aldo Goia, Ernesto Salinelli, and Philippe Vieu. 2013. “Functional Projection Pursuit Regression.” Test 22 (2): 293–320. Ferraty, Frédéric, Juhyun Park, and Philippe Vieu. 2011. “Estimation of a Functional Single Index Model.” In Recent Advances in Functional Data Analysis and Related Topics, 111–16. Springer. Ferraty, Frédéric, Ingrid Van Keilegom, and Philippe Vieu. 2012. “Regression When Both Response and Predictor Are Functions.” Journal of Multivariate Analysis 109: 10–28. ———. 2006. Nonparametric Functional Data Analysis. Springer Series in Statistics. New York: Springer-Verlag. Garcı́a-Portugués, Eduardo, Wenceslao González-Manteiga, and Manuel Febrero-Bande. 2014. “A Goodness-of-Fit Test for the Functional Linear Model with Scalar Response.” Journal of Computational and Graphical Statistics 23 (3): 761–78. Kato, Kengo et al. 2012. “Estimation in Functional Linear Quantile Regression.” The Annals of Statistics 40 (6): 3108–36. Müller, Hans-Georg, and Ulrich Stadtmüller. 2005. “Generalized Functional Linear Models.” Annals of Statistics, 774–805. Müller, Hans-Georg, and Fang Yao. 2012. “Functional Additive Models.” Journal of the American Statistical Association. Oviedo de la Fuente, Manuel, Manuel Febrero-Bande, Marı́a Pilar Muñoz, and Àngela Domı́nguez. 2018. “Predicting Seasonal Influenza Transmission Using Functional Regression Models with Temporal Dependence.” PloS One 13 (4): e0194250. Preda, C., and G. Saporta. 2005. “PLS Regression on a Stochastic Process.” Comput. Statist. Data Anal. 48 (1): 149–58. ———. 2005b. Functional Data Analysis. Second. Springer Series in Statistics. New York: Springer-Verlag. Zivot, Eric, and Jiahui Wang. 2007. Modeling Financial Time Series with s-Plus. Vol. 191. Springer Science &amp; Business Media. "],["functional-supervised-classification.html", "Chapter 3 Functional Supervised Classification 3.1 Logistic Regression Model (GLM): classif.glm 3.2 Generalized Additive Models (GAM): classif.gsam and classif.gkam 3.3 Nonparametric classification methods: classif.knn and classif.np (Frédéric Ferraty and Vieu 2003) 3.4 Maximum depth: classif.depth (Li, Cuesta-Albertos, and Liu 2012) 3.5 The DD\\(^G\\)–classifier classif.DD (J. A. Cuesta-Albertos, Febrero-Bande, and Oviedo de la Fuente 2017) 3.6 Classifiers adapted from Multivariate Framework References", " Chapter 3 Functional Supervised Classification This section describes the usage of functional classification using fda.usc package in R. Let a sample \\((\\mathcal{X},Y)\\in E \\times \\mathbb{G}={1,\\cdots,G}\\). Aim: How predict the class \\(g\\) of Y (categorical variable) given a functional variable \\(\\mathcal{X}\\) Bayes rule: Estimate the posterior probability of belonging to each group: \\[p_g(X)=\\mathbb{P}(Y=g | \\mathcal{X}=\\chi)=\\mathbb{E}(1_{Y=g} |\\mathcal{X}=\\chi)\\] The predicted class is given by the Bayes rule, \\[\\hat{Y}=\\arg \\max_{g\\in \\mathbb{G}} \\hat{p}_g(\\chi)\\] The package allows the estimation of the groups in a training set of functional data by Logistic Classifier (linear model): classif.glm Logistic Classifier (additive model): classif.gsam and classif.gkam k-Nearest Neighbor Classifier: classif.knn Kernel Classifier: classif.kernel Distance Classifier: `classif.distv Maximum Depth Classifier: classif.depth DD Clasisifier: classif.DD library(fda.usc) data(tecator) x=tecator$absorp.fdata tecator$y$Fat&lt;-ifelse(tecator$y$Fat&gt;20,1,0) x.d1&lt;-fdata.deriv(x) dataf=as.data.frame(tecator$y) ldat = ldata(&quot;df&quot;=dataf,&quot;x&quot;=x,&quot;x.d1&quot;=x.d1) ycat&lt;-ldat$df$Fat 3.1 Logistic Regression Model (GLM): classif.glm As a particular case of Generalized Linear Models, the logistic regression model models the posterior probability given \\(\\mathbf{d}\\) as \\[ p(Y=i|\\mathcal{X}(t))=\\log \\left(\\frac{p(Y=i|\\mathcal{X}(t))}{1-p(Y=i|\\mathcal{X}(t))}\\right)=\\alpha_i+ \\left\\langle \\mathcal{X}_i(t),\\beta_{i}(t)\\right\\rangle\\] where the curve \\(\\mathcal{X}(t)\\) is assigned to class \\(i\\) if \\(p(i|\\mathcal{X})&gt;p(j|\\mathcal{X}), j=1,\\cdots, g, j\\ne i\\). res.bin=fregre.glm(Fat~x,ldat,family=binomial()) res.glm&lt;-fda.usc:::classif.glm(Fat~x,data=ldat) summary(res.glm) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## 0 1 ## 1 1 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 0 1 ## 0 138 0 ## 1 0 77 ## ## -Probability of correct classification: 1 3.2 Generalized Additive Models (GAM): classif.gsam and classif.gkam Generalized Additive Models (see Wood (2004)) relax the linearity assumption in GLMs, allowing the use of a sum of general smooth functions \\(f_j\\) for the posterior probability; i.e., \\[ p(Y=i|\\mathcal{X}(t))=\\log \\left(\\frac{p(Y=i|\\mathcal{X}(t))}{1-p(Y=i|\\mathcal{X}(t))}\\right)=\\alpha_i+ f_i\\left(\\mathcal{X}_{i}(t)\\right)\\] where the functions \\(f_{i}\\) may belong to a known parametric family (polynomials, for instance) or they may even be functions to be estimated non-parametrically. res.gsam&lt;-classif.gsam(Fat~s(x),data=ldat) summary(res.gsam) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## 0 1 ## 1 1 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 0 1 ## 0 138 0 ## 1 0 77 ## ## -Probability of correct classification: 1 #res.gkam&lt;-classif.gkam(Fat~x,data=ldata) 3.3 Nonparametric classification methods: classif.knn and classif.np (Frédéric Ferraty and Vieu 2003) These methods are based on non-parametric estimates of the densities of the groups. The most simple (and classical) one is \\(k\\)–nearest neighbour (\\(k\\)NN) in which, given \\(k \\in \\mathbb{N}\\), the point \\(\\mathbf{d}\\) is assigned to the class containing a majority of the \\(k\\) nearest data points in the training sample. Another possibility is to estimate \\(p(Y=g|\\mathcal{X})\\) through the Nadaraya–Watson estimator: \\[p(Y=g|\\mathcal{X})=\\frac{\\sum_{n=1}^N \\mathbf{1}_{G_n=g}\\ K\\left(m(\\mathcal{X},\\mathcal{X}_i(t))/h\\right)}{\\sum_{n=1}^N K\\left(m(\\mathcal{X}_i(t))/h\\right)},\\] where \\(N\\) is the size of the training sample, \\(G_n\\) is the class of \\(i\\)-th curve in the training sample, \\(K\\) is a kernel and \\(m(\\cdot,\\cdot )\\) is a measure of closeness between two curves (a suitable distancewhich is re-scaled by the bandwidth parameter \\(h\\)) . ## y ## 0 1 ## 0.8550725 0.7142857 A \\(k\\)NN method could be considered an NP method using the uniform kernel and a locally selected bandwidth. ## y ## 0 1 ## 0.7826087 0.7012987 3.4 Maximum depth: classif.depth (Li, Cuesta-Albertos, and Liu 2012) The most basic rule is to assign a new observation \\(x_0\\) to the group that provides the highest depth to that observation (Maximum depth (MD)). The maximum depth classifier was the first attempt to use data depths instead of multivariate raw data to construct a classification rule. Perform the following example with hidden code Given a sample depth measure and a new observation \\(x_0\\) (use the xx.d1 curves): Evaluate the depth of \\(x_0\\) in both sub-samples defined by ycat variable (only the first 10 values are printed) ## Depth g1 0.4811594 0.164058 0.5717391 0.5602899 0.4068116 0.127971 0.1237681 0.7730435 0.3373913 0.2971014 ## Depth g2 0.4811594 0.164058 0.5717391 0.5602899 0.4068116 0.127971 0.1237681 0.7730435 0.3373913 0.2971014 Assign \\(x_0\\) according to the data set where it is more deeply placed. ## group.est: 0 1 0 0 1 1 1 0 1 1 ## ycat : 1 1 0 0 1 1 1 0 0 0 The function classif.depth performs previous tasks: res.depth&lt;-classif.depth(ycat,x.d1,depth=&quot;FM&quot;) data.frame(res.depth$dep,group.est,res.depth$dep-cbind(d1,d2))[1:10,] ## X1 X2 group.est d1 d2 ## 1 0.4811594 0.4051948 0 0 0 ## 2 0.1640580 0.4766234 1 0 0 ## 3 0.5717391 0.2659740 0 0 0 ## 4 0.5602899 0.2522078 0 0 0 ## 5 0.4068116 0.5083117 1 0 0 ## 6 0.1279710 0.3948052 1 0 0 ## 7 0.1237681 0.3254545 1 0 0 ## 8 0.7730435 0.3135065 0 0 0 ## 9 0.3373913 0.4568831 1 0 0 ## 10 0.2971014 0.4238961 1 0 0 3.5 The DD\\(^G\\)–classifier classif.DD (J. A. Cuesta-Albertos, Febrero-Bande, and Oviedo de la Fuente 2017) Suppose that we have implementations of a process in the product space \\(\\mathcal{X}=\\mathcal{X}_1\\times\\cdots\\times\\mathcal{X}_p\\) (multivariate (functional) data) where we have \\(g\\) groups (classes or distributions) to be separated using data depths. The DD\\(^G\\)–classifier begins by selecting a depth \\(D\\) and computing the following map (for \\(p=1\\)): \\[ \\mathcal{ X} \\rightarrow \\mathbb{R}^g \\\\ x \\rightarrow \\mathbb{d}=({D}_1(x),\\cdots,{D}_g(x)). \\] We can now apply any available classification procedure that works in a \\(g\\)–dimensional space to separate the \\(g\\) groups. where \\(D_0k(x)\\) is the depth of x with respect to the group \\(k = 1,\\cdots,g\\). So, the DDG -Classifier compresses the information of \\({y_i,x_i}\\) into a real space of dimension \\((g + 1)\\) with the form \\(\\left\\{y_i,D_1(x_i),\\cdots,D_g(x_i)\\right\\}\\). Classification techniques in \\(\\mathbb{R}^g\\): Linear Discriminant Analysis (LDA) Quadratic Discriminant Analysis (QDA) Generalized Linear Models (GLM) Generalized Additive Models (GAM) k-Nearest Neighbors (kNN) Kernel Classification Method (NP) Classification Trees (Tree) ANN, SVMs, … The aim of the DD-classifier ((Li, Cuesta-Albertos, and Liu 2012)) is to extend the Maximum depth classifier using a polynomial up to degree k passing through the origin as classification rule. The DD–classifier has resolved several serious limitations of the maximum depth classifier . Properties of the DDG -classifier: A lot of classification methods available (All in the multivariate framework) Using classical classification methods in the DD-plot can provide useful insights about what’s going on (which depths are influential or probabilities of belonging to a certain group). Possible reduction in the dimension of the classification problem, specially interesting in the Functional Framework (or in High Dimensional problems). No matters how complex is the space to be analyzed, only matters that a depth function can be defined (for example, multivariate functional data MFD: \\(\\mathcal{X}=\\mathcal{X}_1\\times\\cdots\\times\\mathcal{X}_p\\). Example DD with 2 groups res.DD&lt;-classif.DD(ycat,x.d1,classif=&quot;gam&quot;,depth=&quot;mode&quot;) res.depth$prob.classification ## group ## 0 1 ## 0.8260870 0.9350649 res.DD$prob.classification ## group ## 0 1 ## 0.9782609 0.9610390 #res.DD Example dDD with G groups #ycat&lt;-cut(ldata$df$Fat,3,labels=1:3) # DD-classif for functional data: G levels data(phoneme) mlearn&lt;-phoneme[[&quot;learn&quot;]] mlearn2&lt;-phoneme[[&quot;test&quot;]] glearn&lt;-as.numeric(phoneme[[&quot;classlearn&quot;]])-1 out20=classif.DD(glearn,mlearn,depth=&quot;mode&quot;,classif=&quot;glm&quot;) out21=classif.DD(glearn,list(mlearn,mlearn2),depth=&quot;modep&quot;,classif=&quot;glm&quot;,control=list(draw=F)) out20 # univariate functional data ## ## -Call: ## classif.DD(group = glearn, fdataobj = mlearn, depth = &quot;mode&quot;, classif = &quot;glm&quot;) ## ## -Probability of correct classification: 0.928 out21 # multivariate functional data ## ## -Call: ## classif.DD(group = glearn, fdataobj = list(mlearn, mlearn2), depth = &quot;modep&quot;, classif = &quot;glm&quot;, control = list(draw = F)) ## ## -Probability of correct classification: 0.952 #summary.classif(out21) 3.6 Classifiers adapted from Multivariate Framework The idea is to recycle all the procedures known in the Multivariate Frameworkconverting an object of infinite dimension into a finite dimension. When to apply: + The basis is enough for accounting all information. + Binary/multiclass problems depends on multivariate classifier method. data(phoneme) ldat=ldata(&quot;df&quot;=data.frame(glearn=phoneme$classlearn),&quot;x&quot;=phoneme$learn) # require e1071 package res.svm=classif.svm(glearn~x,data=ldat) # require nnet package res.nnet=classif.nnet(glearn~x,data=ldat,trace=FALSE) # require rpart package res.rpart=classif.rpart(glearn~x,data=ldat) round(mean(res.svm$prob.classification),3) ## [1] 0.904 round(mean(res.nnet$prob.classification),3) ## [1] 0.892 round(mean(res.rpart$prob.classification),3) ## [1] 0.896 Add utilities in the classification functions: for example, majority voting scheme (by default ONE vs REST). R example (work in progress) ii&lt;- c(1:10,51:60,101:110,151:160,201:250) mlearn&lt;-phoneme[[&quot;learn&quot;]][ii];glearn&lt;-phoneme[[&quot;classlearn&quot;]][ii] mtest&lt;-phoneme[[&quot;test&quot;]];gtest&lt;-phoneme[[&quot;classtest&quot;]] dataf&lt;-data.frame(glearn);ldat=ldata(&quot;df&quot;=dataf,&quot;x&quot;=mlearn);newdat&lt;-list(&quot;x&quot;=mtest) a1&lt;-classif.glm(glearn~x, data = ldat) a2&lt;-classif.glm(glearn~x, data = ldat,type=&quot;majority&quot;) a3&lt;-classif.glm(glearn~x, data = ldat,type=&quot;majority&quot;,weights=c(rep(4,len=40),rep(1,50))) # mean(predict(a1,newdat)==gtest);mean(predict(a2,newdat)==gtest);mean(predict(a3,newdat)==gtest) References References Cuesta-Albertos, Juan A, Manuel Febrero-Bande, and Manuel Oviedo de la Fuente. 2017. Test 26 (1): 119–42. Ferraty, Frédéric, and Philippe Vieu. 2003. “Curves Discrimination: A Nonparametric Functional Approach.” Comput. Statist. Data Anal. 44 (1): 161–73. http://www.sciencedirect.com/science/article/pii/S016794730300032X. Li, Jun, Juan A Cuesta-Albertos, and Regina Y Liu. 2012. “\\(DD\\)–Classifier: Nonparametric Classification Procedure Based on \\(DD\\)–Plot.” J. Amer. Statist. Assoc. 107 (498): 737–53. http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2012.688462. Wood, Simon N. 2004. “Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models.” Journal of the American Statistical Association 99 (467): 673–86. "],["variable-selection.html", "Chapter 4 Variable Selection 4.1 Functiondal regression with points of impact 4.2 Variable selection in functional regression 4.3 Binary classification example 4.4 Hyperspectral images example 4.5 Optimum Multiscale Selection in 3D Point Cloud Classification (Oviedo-de la Fuente et al. 2021) 4.6 Model Comparison", " Chapter 4 Variable Selection Chapter under construction 4.1 Functiondal regression with points of impact 4.1.1 State of Art Nonparametric variable selection approach (NOVAS). NOVAS is quite expensive from a computational perspective, Frédéric Ferraty, Hall, and Vieu (2010). A wavelet-based weighted LASSO functional linear (FWLASSO). FWLASSO requires transforming the original variables and assuming a linear model, Zhao, Chen, and Ogden (2015). Berrendero, Cuevas, and Torrecilla (2016) use the Maxima-hunting proposal to choose the most relevant design points in functional classification setting. 4.1.2 Local maxima distance correlation approach (LMDC), (Ordóñez et al. 2018) In this work we study the utility of distance correlation G. J. Székely, Rizzo, and Bakirov (2007) as an intrinsic method for variable selection. Neither projection nor transformation of the variables is needed. Moreover, it is unnecessary to assume an a priori regression model. LMDC approach consists in calculating the local maxima of the distance correlation along the curve. 4.1.3 LMDC Algorithm: LMDC.select() function Calculate de distance correlation (DC) \\(R(t) = \\left \\lbrace R(X(t_j),Y) \\right\\rbrace {_{j=1}^N}\\), from the data \\(\\left \\lbrace X_i (t_j),Y_i \\right\\rbrace _{i=1}^n\\). Calculate the LM of the \\(\\hat{\\mathcal{R}} (t)\\). Only the significant local maxima for a default level of significance are selected. Denoting the arguments values (argvals) of the local maxima a \\(\\tilde t_1,\\tilde t_2,\\ldots,\\tilde t_{\\tilde{N}}\\) (\\(\\tilde{N}&lt;N\\)), we ordered them from highest to lowest values of DC, that is \\(\\hat{\\mathcal{R}}(\\tilde t_1) \\geq \\hat{ \\mathcal{R}}(\\tilde t_2) &gt;\\ldots \\geq \\hat {\\mathcal{R}}(\\tilde t_{\\tilde{N}})\\) library(fda.usc) pred2gsam &lt;- fda.usc.devel:::pred2gsam predict.classif &lt;- fda.usc.devel:::predict.classif predict.fregre.gsam &lt;- fda.usc.devel:::predict.fregre.gsam data(tecator) X.d2&lt;-fdata.deriv(tecator[[&quot;absorp.fdata&quot;]], nderiv = 2) colnames(X.d2[[&quot;data&quot;]])&lt;-paste0(&quot;X&quot;,round(X.d2[[&quot;argvals&quot;]])) dat &lt;- data.frame(&quot;y&quot;=tecator[[&quot;y&quot;]][[&quot;Fat&quot;]],X.d2[[&quot;data&quot;]] ) tol&lt;-.2 dc.raw &lt;- LMDC.select(&quot;y&quot;,data = dat, tol = tol,pvalue = 0.05, plot=F) # Preselected impact points covar&lt;-names(dat)[-1][dc.raw[[&quot;maxLocal&quot;]]] covar ## [1] &quot;X933&quot; &quot;X1046&quot; &quot;X907&quot; &quot;X886&quot; &quot;X896&quot; &quot;X1010&quot; &quot;X1020&quot; &quot;X1030&quot; &quot;X945&quot; ## [10] &quot;X876&quot; &quot;X915&quot; &quot;X862&quot; &quot;X993&quot; length(covar) ## [1] 13 4.1.4 LMDC Algorithm: LMDC.regre() function (Optionally) Check if the relationship between the reponse and the predictor variables is linear: \\(H_0:\\,Y=\\big&lt;X,\\beta\\big&gt;+\\epsilon\\), versus a general alternative using a test of linearity proposed in Garcı́a-Portugués, González-Manteiga, and Febrero-Bande (2014). ftest&lt;-flm.test(dat[,-1], dat[,&quot;y&quot;], verbose=F,plot.it=F) ftest ## ## PCvM test for the functional linear model using optimal PLS basis ## representation ## ## data: Y=&lt;X,b&gt;+e ## PCvM statistic = 216.48, p-value &lt; 2.2e-16 ftest[[&quot;p.value&quot;]] ## [1] 0 Fit a regression model to the response of interest \\(Y\\) using the vector of covariates \\(X(\\tilde{t})=\\{X(\\tilde t_1), \\ldots, X(\\tilde{t}_{\\tilde{N}})\\}\\). A linear model will be used if the null hypothesis is not rejected and a nonparametric (e.g. generalized additive model) model otherwise. (Optionally) Once the type model has been selected, we propose to Apply a forward stepwise regression method to determine the significant covariates, taking advantage of the fact that the local maxima have been ordered. This means we start with a model with the first covariate (the one with the highest value of distance correlation), and the rest of the ordered covariates are added to the model in turn. This substantially reduces the computing time. if (ftest$p.value &gt; 0.05) { # Linear relationship, step-wise lm is recommended out &lt;- LMDC.regre(y = &quot;y&quot;, covar = covar, data = dat, pvalue=.05, method =&quot;lm&quot;) } else {# Non-Linear relationship, step-wise gam is recommended out &lt;- LMDC.regre(y = &quot;y&quot;, covar = covar, data = dat,pvalue=.05, method =&quot;gam&quot;)} out ## $model ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(X933, k = 4) + s(X1046, k = 4) + s(X907, k = 4) + s(X886, ## k = 4) + s(X896, k = 4) + s(X1010, k = 4) + s(X1020, k = 4) + ## s(X1030, k = 4) + s(X945, k = 4) + s(X876, k = 4) + s(X915, ## k = 4) + s(X993, k = 4) ## ## Estimated degrees of freedom: ## 3.00 2.55 2.83 2.00 1.00 1.00 2.94 ## 2.92 2.81 2.57 1.00 2.85 total = 28.48 ## ## GCV score: 0.4403176 ## ## $xvar ## [1] &quot;X933&quot; &quot;X1046&quot; &quot;X907&quot; &quot;X886&quot; &quot;X896&quot; &quot;X1010&quot; &quot;X1020&quot; &quot;X1030&quot; &quot;X945&quot; ## [10] &quot;X876&quot; &quot;X915&quot; &quot;X993&quot; ## ## $pred ## NULL ## ## $edf ## [1] 28.48142 ## ## $nvar ## [1] 12 Differences in mean square prediction error between linear (usign lm model) and non-linear (usign gam model) model out &lt;- LMDC.regre(y = &quot;y&quot;, covar = covar, data = dat[1:165,],newdata=dat[166:215,], pvalue=.05, method =&quot;lm&quot;) mean((out$pred-dat$y[166:215])^2) ## [1] 11.75474 out &lt;- LMDC.regre(y = &quot;y&quot;, covar = covar, data = dat[1:165,],newdata=dat[166:215,], pvalue=.05, method =&quot;gam&quot;) mean((out$pred-dat$y[166:215])^2) ## [1] 1.148573 Binary classification example (Impact point selection, model estimation and prediction) data(tecator) X.d2&lt;-fdata.deriv(tecator[[&quot;absorp.fdata&quot;]], nderiv = 2) colnames(X.d2[[&quot;data&quot;]])&lt;-paste0(&quot;X&quot;,round(X.d2[[&quot;argvals&quot;]])) y2groups &lt;- ifelse(tecator[[&quot;y&quot;]][[&quot;Fat&quot;]]&lt;12,0,1) dat &lt;- data.frame(&quot;y2groups&quot;=y2groups,X.d2[[&quot;data&quot;]] ) tol&lt;-.1 dc.raw &lt;- LMDC.select(&quot;y2groups&quot;,data = dat, tol = tol,pvalue = 0.05, plot=F) # Preselected impact points covar&lt;-names(dat)[-1][dc.raw[[&quot;maxLocal&quot;]]] covar ## [1] &quot;X945&quot; &quot;X905&quot; &quot;X933&quot; &quot;X886&quot; &quot;X1020&quot; &quot;X876&quot; &quot;X1030&quot; &quot;X896&quot; &quot;X1046&quot; ## [10] &quot;X862&quot; &quot;X1010&quot; &quot;X915&quot; &quot;X965&quot; length(covar) ## [1] 13 # GLM model (using binomial family), other multivariate model can be used ind &lt;- 1:129 ldata&lt;-list(&quot;df&quot;=dat[ind,]) form.glm&lt;-formula(paste0(&quot;y2groups~&quot;,paste0(covar,collapse=&quot;+&quot;))) out.glm &lt;- classif.glm(form.glm, data = ldata) summary(out.glm) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## 0 1 ## 1 1 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 0 1 ## 0 58 0 ## 1 0 71 ## ## -Probability of correct classification: 1 # Prediction newldata&lt;-list(&quot;df&quot;=dat[-ind,]) pred.glm&lt;-predict(out.glm,newldata) # Confusion matrix table(newldata$df$y2groups,pred.glm) ## pred.glm ## 0 1 ## 0 40 0 ## 1 3 43 4.2 Variable selection in functional regression Febrero-Bande, González-Manteiga, and Oviedo de la Fuente (2019) consider the problem of variable selection in regression models in the case of functional variables that may be mixed with other type of variables (scalar, multivariate, directional, etc.). Our proposal begins with a simple null model and sequentially selects a new variable to be incorporated into the model based on the use of distance correlation proposed by (G. J. Székely, Rizzo, and Bakirov 2007). For the sake of simplicity, this paper only uses additive models. \\[ Y_i=\\alpha+\\sum_{j=1}^Jf_j({X_i^{(j)}})+\\varepsilon_i,\\quad i=1,\\ldots,N \\] The proposed algorithm may assess the type of contribution (linear, non linear, …) of each variable. The algorithm has shown quite promising results when applied to simulations and real data sets. 4.2.1 State of Art Stepwise regression, Akaike (1973). The main idea is to use some diagnostic tools, directly derived from the linear model, to evaluate the contribution of a new covariate and decide whether it should be included in the model. The final subset is usually constructed using: the forward and/or the backward selection. Feature Selection using LASSO. The work by Tibshirani, 1996 proposing the LASSO estimator includes a \\(l_1\\)-type constraint for the coefficient vector \\(\\beta\\). Several examples following the same line but using penalties or constraints such as: LARS (Efron et al. (2004)) and COSSO (Lin, Zhang, et al. (2006)). Each methods is based on a specific model, all the covariates must be included in the model at the same time and for functional data problems, the previous steps that commonly include variable standardization. Berrendero, Cuevas, and Torrecilla (2016) use the Minimum Redundance Maximum Relevance (mRMR) procedure to choose the most relevant design points in functional classification setting. A pure feature selection methods where the covariate is selected without a model. This is the approach employed in minimum Redundancy Maximum Relevance (mRMR), (Peng, Long, and Ding (2005)) where a new candidate covariate must have a great relevancy with the response while maintaining a lower redundancy with the covariates already selected in the model. he main advantage of this approach is that it is an incremental rule but the measures for redundancy and relevancy must be chosen in function of the regression model applied to ensure good predictive results in the final model. Berrendero, Cuevas, and Torrecilla (2018) used the Reproducing Kernel Hilbert Space (RKHS) for variable selection in FLM. Boosting, see F. Ferraty and Vieu (2009) in a functional data context. Boosting selects at each step the best covariate/model with respect to the unexplained part of the response. The final prediction is constructed as a combination of the different steps. Partial distance correlation (PDC): used in Yenigün and Rizzo (2015) for VS in multivariate linear models, a definition of PDC among \\(X\\) and \\(Y\\) given \\(Z\\) was introduced based on computing the distance correlation among the residuals of two models: \\(Y\\) respect to \\(Z\\) and \\(X\\) respect to \\(Z\\). PDC is constructed under linear relationship assumptions among variables. Its implementation only uses the distance matrices among elements of \\(X\\), \\(Y\\) and \\(Z\\) (variables should have a similar scale). Specifically, \\(Z\\) (the variables already in the model) could be a mix of functional, scalar or multivariate variables where an appropriate distance using all of them must be hard to compute. Even restricting ourselves to the scalar case, those variables should have a similar scale. 4.2.2 Algorithm All the previous solutions are not completely satisfactory in a functional data framework, specially when the number of possible covariates can be arbitrarily large. We are interested in an automatic regression procedure capable of dealing with a large number of covariates of different nature, possibly very closely related to one another. The key of the whole procedure is the extensive use of the DC that presents two important advantages: the choice of the variate is made without considering a model and it is possible to compute this quantity for variates of different nature as it is only computed from distances. The distance correlation (DC) is computed among the residuals of the current model with each candidate. Taking into account that the residuals have the same nature as the response variable, the DC can always be computed at each step. Our proposal is presented is a very general way, we have restricted ourselves to additive models that offer a balanced compromise between predictive ability and simplicity. The obtained results are quite promising in scenarios where no competitors are available because no other procedure can deal with variates of different nature in a homogeneous way. The procedure was applied to a real problem related with the Iberian Energy Market (Price and Demand) where the number of possible covariates is really big. The algorithm was able to find synthetic regression models offering interesting insights about the relationship among the response and the covariates. The final selected models mix functional, scalar and categorical information. Our algorithm can be formalized as follows: Let \\(Y\\) the response and \\(S=\\{X^1,\\ldots,X^p\\}\\) the set of all possible predictors. Set \\(\\hat{Y}=\\bar{Y}\\), and let \\(M^{(0)}=\\emptyset\\) the initial set of the variates included in the model. Set \\(i=0\\). Compute the residuals of the current model: \\(\\hat{\\varepsilon}=Y-\\hat{Y}\\). Choose \\(X^j\\in S\\) such that: 1) \\(\\mathcal{R}\\{\\hat{\\varepsilon},X^j\\}\\ge \\mathcal{R}\\{\\hat{\\varepsilon},X^k\\}, \\forall k\\ne j\\in S\\) and 2) the null hypothesis for the test of independence among \\(\\left\\{X^j\\right\\}\\) and \\(\\hat{\\varepsilon}\\) is rejected. IF NOT, END. Update the sets \\(M\\) and \\(S\\): \\(M^{(i+1)}=M^{(i)}\\cup\\{X^j\\}\\), and \\(S=S\\backslash\\{X^j\\}\\). Compute the new model for \\(Y\\) using \\(M^{(i+1)}\\) choosing the best contribution of the new covariate. Typically, there will be a catalog of all possible ways of constructing correct models with the variates in \\(M^{(i+1)}\\) fixing the contributions of the variates in \\(M^{(i)}\\) and adding the new one. Analyze the contribution of \\(X^j\\) in the new model respect to the current: IF this contribution is not relevant (typically comparing with the current model) THEN \\(M^{(i+1)}=M^{(i+1)}\\backslash\\{X^j\\}\\) and the current model remains unalterable ELSE the new model becomes the current model and provides new predictions (\\(\\hat{Y}\\)). Along the paper we have employed an additive model: \\(\\hat{Y}=\\bar{Y}+\\sum_{m\\in M}\\hat{f}_m\\left(X^{(m)}\\right)\\) where at each step \\(\\hat{f}_m\\) could be linear or nonlinear. Update the number of iterations: \\(i=i+1\\) and go to 3 END. The current model is the final model with the variates included in \\(M^{(i)}\\). \\(S\\) is either the empty set or contains those variables that accept the null hypothesis of the test of independence respect to the residuals of the current model. 4.3 Binary classification example Nt=250 Np=250 nB=100 Nvar &lt;- 50 set.seed(1) Xdat &lt;- mariachi(Nt,Np,Nvar) Xtrain &lt;- Xdat$Xtrain Xtest &lt;- Xdat$Xtest ytrain &lt;- Xtrain$grupo ytest &lt;- Xtest$grupo pairs(Xtrain[,2:6],col=ytrain) mdat &lt;- Xtrain[,1:Nvar] newmdat &lt;- Xtest[,1:Nvar] names(mdat) ## [1] &quot;grupo&quot; &quot;X1&quot; &quot;X2&quot; &quot;Z1&quot; &quot;Z2&quot; &quot;Z3&quot; &quot;Z4&quot; &quot;Z5&quot; &quot;Z6&quot; ## [10] &quot;Z7&quot; &quot;Z8&quot; &quot;Z9&quot; &quot;Z10&quot; &quot;Z11&quot; &quot;Z12&quot; &quot;Z13&quot; &quot;Z14&quot; &quot;Z15&quot; ## [19] &quot;Z16&quot; &quot;Z17&quot; &quot;Z18&quot; &quot;Z19&quot; &quot;Z20&quot; &quot;Z21&quot; &quot;Z22&quot; &quot;Z23&quot; &quot;Z24&quot; ## [28] &quot;Z25&quot; &quot;Z26&quot; &quot;Z27&quot; &quot;Z28&quot; &quot;Z29&quot; &quot;Z30&quot; &quot;Z31&quot; &quot;Z32&quot; &quot;Z33&quot; ## [37] &quot;Z34&quot; &quot;Z35&quot; &quot;Z36&quot; &quot;Z37&quot; &quot;Z38&quot; &quot;Z39&quot; &quot;Z40&quot; &quot;Z41&quot; &quot;Z42&quot; ## [46] &quot;Z43&quot; &quot;Z44&quot; &quot;Z45&quot; &quot;Z46&quot; &quot;Z47&quot; meas &lt;- &quot;accuracy&quot; library(randomForest) rf1 &lt;- randomForest(grupo~.,data=mdat) varImpPlot(rf1) pred.rf1 &lt;- predict(rf1,newmdat,measure=meas) cat2meas(ytest,pred.rf1) ## accuracy ## 0.672 table(ytest,pred.rf1) ## pred.rf1 ## ytest 1 2 ## 1 95 14 ## 2 68 73 rf.vs &lt;- fda.usc.devel:::classif.ML.vs(ldata(mdat),&quot;grupo&quot;,classif=&quot;classif.randomForest&quot;) summary(rf.vs) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## [1] 0.9185185 0.8869565 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 124 11 ## 2 13 102 ## ## -Probability of correct classification: 0.904 rf.vs$i.predictor ## X1 X2 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10 Z11 Z12 Z13 Z14 Z15 Z16 Z17 Z18 ## 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Z19 Z20 Z21 Z22 Z23 Z24 Z25 Z26 Z27 Z28 Z29 Z30 Z31 Z32 Z33 Z34 Z35 Z36 Z37 Z38 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Z39 Z40 Z41 Z42 Z43 Z44 Z45 Z46 Z47 ## 0 0 0 0 0 0 0 0 0 rf.vs$ipredictor ## [1] &quot;X2&quot; &quot;X1&quot; head(round(rf.vs$dcor,3)) ## X1 X2 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10 Z11 Z12 Z13 Z14 Z15 Z16 Z17 ## 1 0.016 0.018 0 0 0 0 0 0 0 0 0 0 0.01 0 0 0.010 0 0 0 ## 2 0.032 0.000 0 0 0 0 0 0 0 0 0 0 0.00 0 0 0.000 0 0 0 ## 3 0.000 0.000 0 0 0 0 0 0 0 0 0 0 0.00 0 0 0.011 0 0 0 ## 4 0.000 0.000 0 0 0 0 0 0 0 0 0 0 0.00 0 0 0.011 0 0 0 ## 5 0.000 0.000 0 0 0 0 0 0 0 0 0 0 0.00 0 0 0.000 0 0 0 ## 6 0.000 0.000 0 0 0 0 0 0 0 0 0 0 0.00 0 0 0.000 0 0 0 ## Z18 Z19 Z20 Z21 Z22 Z23 Z24 Z25 Z26 Z27 Z28 Z29 Z30 Z31 Z32 Z33 Z34 Z35 ## 1 0.015 0 0 0 0.013 0 0 0 0 0 0 0 0 0.00 0 0 0 0 ## 2 0.000 0 0 0 0.000 0 0 0 0 0 0 0 0 0.01 0 0 0 0 ## 3 0.000 0 0 0 0.000 0 0 0 0 0 0 0 0 0.00 0 0 0 0 ## 4 0.000 0 0 0 0.000 0 0 0 0 0 0 0 0 0.00 0 0 0 0 ## 5 0.000 0 0 0 0.000 0 0 0 0 0 0 0 0 0.00 0 0 0 0 ## 6 0.000 0 0 0 0.000 0 0 0 0 0 0 0 0 0.00 0 0 0 0 ## Z36 Z37 Z38 Z39 Z40 Z41 Z42 Z43 Z44 Z45 Z46 Z47 ## 1 0 0.000 0 0 0 0 0 0 0 0 0 0 ## 2 0 0.000 0 0 0 0 0 0 0 0 0 0 ## 3 0 0.016 0 0 0 0 0 0 0 0 0 0 ## 4 0 0.000 0 0 0 0 0 0 0 0 0 0 ## 5 0 0.000 0 0 0 0 0 0 0 0 0 0 ## 6 0 0.000 0 0 0 0 0 0 0 0 0 0 pred.rf.vs &lt;- predict(rf.vs,ldata(newmdat)) cat2meas(ytest,pred.rf.vs) ## accuracy ## 0.896 table(ytest,pred.rf.vs) ## pred.rf.vs ## ytest 1 2 ## 1 103 6 ## 2 20 121 predictors &lt;- names(mdat)[-1] form &lt;- formula(paste0(&quot;grupo~&quot;,paste0(&quot;s(&quot;,predictors,&quot;)&quot;,collapse=&quot;+&quot;))) form ## grupo ~ s(X1) + s(X2) + s(Z1) + s(Z2) + s(Z3) + s(Z4) + s(Z5) + ## s(Z6) + s(Z7) + s(Z8) + s(Z9) + s(Z10) + s(Z11) + s(Z12) + ## s(Z13) + s(Z14) + s(Z15) + s(Z16) + s(Z17) + s(Z18) + s(Z19) + ## s(Z20) + s(Z21) + s(Z22) + s(Z23) + s(Z24) + s(Z25) + s(Z26) + ## s(Z27) + s(Z28) + s(Z29) + s(Z30) + s(Z31) + s(Z32) + s(Z33) + ## s(Z34) + s(Z35) + s(Z36) + s(Z37) + s(Z38) + s(Z39) + s(Z40) + ## s(Z41) + s(Z42) + s(Z43) + s(Z44) + s(Z45) + s(Z46) + s(Z47) ldat &lt;- ldata(mdat) # gsam &lt;- classif.gsam(form,data=ldat) # Error in gam(formula = as.formula(pf), data = XX, family = family) : Model has more coefficients than data form &lt;- formula(paste0(&quot;grupo~&quot;,paste0(&quot;s(&quot;,predictors,&quot;,k=5)&quot;,collapse=&quot;+&quot;))) ldat &lt;- ldata(mdat) gsam &lt;- classif.gsam(form,data=ldat) pred.gsam &lt;- predict(gsam,ldata(newmdat)) cat2meas(ytest,pred.gsam) ## accuracy ## 0.896 table(ytest,pred.gsam) ## pred.gsam ## ytest 1 2 ## 1 97 12 ## 2 14 127 gsam.vs &lt;- classif.gsam.vs(ldata(mdat),&quot;grupo&quot;) summary(gsam.vs) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## 1 2 ## 1 1 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 135 0 ## 2 0 115 ## ## -Probability of correct classification: 1 gsam.vs$i.predictor ## X1 X2 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10 Z11 Z12 Z13 Z14 Z15 Z16 Z17 Z18 ## 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Z19 Z20 Z21 Z22 Z23 Z24 Z25 Z26 Z27 Z28 Z29 Z30 Z31 Z32 Z33 Z34 Z35 Z36 Z37 Z38 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Z39 Z40 Z41 Z42 Z43 Z44 Z45 Z46 Z47 ## 0 0 0 0 0 0 0 0 0 gsam.vs$ipredictor ## [1] &quot;X2&quot; &quot;X1&quot; pred.gsam.vs &lt;- predict(gsam.vs,ldata(newmdat)) cat2meas(ytest,pred.gsam.vs) ## accuracy ## 0.984 table(ytest,pred.gsam.vs) ## pred.gsam.vs ## ytest 1 2 ## 1 106 3 ## 2 1 140 4.4 Hyperspectral images example During the last years, the use of hyperspectral sensors has been extended to a great variety of applications such as discrimination among different land cover classes in remote sensing images. 4.4.1 Hyperspectral Data set: Pavia University This is a remote sensing image obtained by the 103-band ROSIS sensor from the University of Pavia (Pavia Univ.), with a spatial dimension of 610 × 340 pixels 4.4.2 Hyperspectral images In the University of Pavia image, 300 pixels for each class were chosen for training, and the rest were used as a test set. 1 2 3 4 5 6 7 8 9 6631 18649 2099 3064 1345 5029 1330 3682 947 [1] “Training” 1 2 3 4 5 6 7 8 9 300 300 300 300 300 300 300 300 300 [1] “Test” yy 1 2 3 4 5 6 7 8 9 797 2308 244 324 119 589 117 424 78 names&lt;-c(“Asphalt”,“Meadows”,“Gravel”,“Trees”,“Metal sheets”,“Bare Soil”,“Bitumen”,“Bricks”,“Shadows”) ``` r # scalar covariates names(ltest$df); [1] &quot;y&quot; &quot;icol&quot; &quot;irow&quot; &quot;z0&quot; &quot;z0.1&quot; &quot;z0.2&quot; &quot;z0.29&quot; &quot;z0.39&quot; &quot;z0.49&quot; &quot;z0.59&quot; &quot;z0.69&quot; &quot;z0.78&quot; [13] &quot;z0.88&quot; &quot;z0.98&quot; # functional covariates names(ltest)[-1] [1] &quot;x&quot; &quot;x.d1&quot; gsam1 &lt;- classif.gsam(y ~ icol+irow ,data=ltrain2) There were 18 warnings (use warnings() to see them) summary(gsam1) - SUMMARY - -Probability of correct classification by group (prob.classification): 1 2 3 4 5 6 7 8 9 0.00 0.47 0.62 0.00 0.99 0.92 0.72 0.34 0.01 -Confusion matrix between the theoretical groups (by rows) and estimated groups (by column) 1 2 3 4 5 6 7 8 9 1 0 12 11 0 23 43 0 9 2 2 0 47 11 0 23 19 0 0 0 3 0 0 62 0 0 0 0 38 0 4 0 19 14 0 29 27 0 9 2 5 1 0 0 0 99 0 0 0 0 6 2 0 0 6 0 92 0 0 0 7 11 0 0 12 0 0 72 0 5 8 0 20 21 0 15 0 4 34 6 9 2 7 38 14 14 11 7 6 1 -Probability of correct classification: 0.4522 pred.gsam1 &lt;- predict(gsam1,ltest) cat2meas(ytest,pred.gsam1) [1] 0.439 table(ytest,pred.gsam1) pred.gsam1 ytest 1 2 3 4 5 6 7 8 9 1 0 61 52 0 121 174 0 22 16 2 0 639 186 0 321 168 16 2 0 3 0 0 92 0 0 0 0 51 0 4 0 38 17 0 74 53 3 21 8 5 1 0 0 0 103 0 0 0 0 6 5 0 0 37 0 325 0 0 0 7 12 0 0 16 0 0 68 0 1 8 0 49 39 0 24 0 9 89 14 9 6 5 11 6 11 10 14 9 1 gsam1 &lt;- classif.gsam(y ~ s(icol) + s(irow) ,data=ltrain2) summary(gsam1) - SUMMARY - -Probability of correct classification by group (prob.classification): 1 2 3 4 5 6 7 8 9 0.00 0.47 0.62 0.00 0.99 0.92 0.72 0.34 0.01 -Confusion matrix between the theoretical groups (by rows) and estimated groups (by column) 1 2 3 4 5 6 7 8 9 1 0 12 11 0 23 43 0 9 2 2 0 47 11 0 23 19 0 0 0 3 0 0 62 0 0 0 0 38 0 4 0 19 14 0 29 27 0 9 2 5 1 0 0 0 99 0 0 0 0 6 2 0 0 6 0 92 0 0 0 7 11 0 0 12 0 0 72 0 5 8 0 20 21 0 15 0 4 34 6 9 2 7 38 14 14 11 7 6 1 -Probability of correct classification: 0.4522 pred.gsam1 &lt;- predict(gsam1,ltest) cat2meas(ytest,pred.gsam1) [1] 0.439 table(ytest,pred.gsam1) pred.gsam1 ytest 1 2 3 4 5 6 7 8 9 1 0 61 52 0 121 174 0 22 16 2 0 639 186 0 321 168 16 2 0 3 0 0 92 0 0 0 0 51 0 4 0 38 17 0 74 53 3 21 8 5 1 0 0 0 103 0 0 0 0 6 5 0 0 37 0 325 0 0 0 7 12 0 0 16 0 0 68 0 1 8 0 49 39 0 24 0 9 89 14 9 6 5 11 6 11 10 14 9 1 gsam1 &lt;- classif.gsam(y ~ s(x) ,data=ltrain2) summary(gsam1) - SUMMARY - -Probability of correct classification by group (prob.classification): 1 2 3 4 5 6 7 8 9 0.62 0.78 0.64 0.97 1.00 0.25 0.82 0.71 1.00 -Confusion matrix between the theoretical groups (by rows) and estimated groups (by column) 1 2 3 4 5 6 7 8 9 1 62 0 4 0 0 2 27 5 0 2 0 78 3 13 0 6 0 0 0 3 2 1 64 0 0 0 7 26 0 4 0 2 0 97 0 1 0 0 0 5 0 0 0 0 100 0 0 0 0 6 0 56 1 2 0 25 0 16 0 7 15 0 3 0 0 0 82 0 0 8 1 1 12 0 0 6 9 71 0 9 0 0 0 0 0 0 0 0 100 -Probability of correct classification: 0.7544 pred.gsam1 &lt;- predict(gsam1,ltest) cat2meas(ytest,pred.gsam1) [1] 0.7016667 table(ytest,pred.gsam1) pred.gsam1 ytest 1 2 3 4 5 6 7 8 9 1 281 2 36 0 2 3 111 11 0 2 0 1001 27 186 0 116 0 2 0 3 4 1 103 0 0 0 7 28 0 4 0 7 0 207 0 0 0 0 0 5 0 0 0 0 104 0 0 0 0 6 1 215 11 5 0 90 0 45 0 7 8 1 1 0 0 0 87 0 0 8 2 3 32 0 0 11 17 159 0 9 0 0 0 0 0 0 0 0 73 gsam1 &lt;- classif.gsam(y ~ s(icol) + s(irow) + s(x) ,data=ltrain2) summary(gsam1) - SUMMARY - -Probability of correct classification by group (prob.classification): 1 2 3 4 5 6 7 8 9 0.49 0.80 0.75 0.94 1.00 0.78 0.84 0.74 1.00 -Confusion matrix between the theoretical groups (by rows) and estimated groups (by column) 1 2 3 4 5 6 7 8 9 1 49 0 10 0 0 7 30 4 0 2 0 80 0 10 0 10 0 0 0 3 0 1 75 0 0 0 0 24 0 4 0 4 0 94 0 2 0 0 0 5 0 0 0 0 100 0 0 0 0 6 1 16 0 2 0 78 0 3 0 7 16 0 0 0 0 0 84 0 0 8 0 2 20 0 0 0 4 74 0 9 0 0 0 0 0 0 0 0 100 -Probability of correct classification: 0.8156 pred.gsam1 &lt;- predict(gsam1,ltest) cat2meas(ytest,pred.gsam1) [1] 0.7766667 table(ytest,pred.gsam1) pred.gsam1 ytest 1 2 3 4 5 6 7 8 9 1 236 1 46 0 2 43 115 3 0 2 0 1026 1 192 0 112 0 1 0 3 0 1 121 0 0 0 1 20 0 4 0 6 0 207 0 1 0 0 0 5 0 0 0 0 104 0 0 0 0 6 2 44 0 5 0 308 0 8 0 7 6 1 0 0 0 0 90 0 0 8 4 3 36 0 0 0 16 165 0 9 0 0 0 0 0 0 0 0 73 gsam.vs &lt;- classif.gsam.vs(ltrain2,&quot;y&quot;) gsam.vs$i.predictor icol irow z0 z0.1 z0.2 z0.29 z0.39 z0.49 z0.59 z0.69 z0.78 z0.88 z0.98 x x.d1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 gsam.vs$ipredictor [1] &quot;x&quot; &quot;z0.69&quot; &quot;icol&quot; &quot;x.d1&quot; &quot;irow&quot; round(gsam.vs$dcor,2) icol irow z0 z0.1 z0.2 z0.29 z0.39 z0.49 z0.59 z0.69 z0.78 z0.88 z0.98 x x.d1 1 0.20 0.18 0.33 0.43 0.44 0.44 0.42 0.37 0.36 0.38 0.42 0.43 0.43 0.58 0.43 2 0.07 0.06 0.04 0.06 0.07 0.07 0.07 0.06 0.06 0.08 0.06 0.06 0.06 0.00 0.05 3 0.07 0.06 0.04 0.06 0.06 0.07 0.06 0.06 0.06 0.00 0.05 0.05 0.06 0.00 0.05 4 0.00 0.03 0.02 0.04 0.04 0.04 0.04 0.03 0.03 0.00 0.04 0.04 0.04 0.00 0.04 5 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 summary(gsam.vs) - SUMMARY - -Probability of correct classification by group (prob.classification): 1 2 3 4 5 6 7 8 9 0.72 0.84 0.82 0.94 1.00 0.86 0.88 0.73 1.00 -Confusion matrix between the theoretical groups (by rows) and estimated groups (by column) 1 2 3 4 5 6 7 8 9 1 72 0 5 0 0 3 15 5 0 2 0 84 0 7 0 8 0 1 0 3 0 1 82 0 0 0 1 16 0 4 0 4 0 94 0 2 0 0 0 5 0 0 0 0 100 0 0 0 0 6 0 9 0 1 0 86 0 4 0 7 12 0 0 0 0 0 88 0 0 8 2 0 23 0 0 1 1 73 0 9 0 0 0 0 0 0 0 0 100 -Probability of correct classification: 0.8656 pred.gsam.vs &lt;- predict(gsam.vs,ltest) cat2meas(ytest,pred.gsam.vs) [1] 0.8106667 table(ytest,pred.gsam.vs) pred.gsam.vs ytest 1 2 3 4 5 6 7 8 9 1 290 1 38 0 0 31 79 7 0 2 3 1096 1 105 0 118 0 9 0 3 0 1 117 0 0 0 2 23 0 4 0 9 0 201 0 4 0 0 0 5 0 0 0 0 104 0 0 0 0 6 1 47 0 2 0 307 0 10 0 7 16 0 0 0 0 0 80 1 0 8 8 3 43 0 0 1 5 164 0 9 0 0 0 0 0 0 0 0 73 4.4.3 Pravia Univ. Results Accuracy average over 100 runs. Method \\(x\\)-pos., \\(y\\)-pos. \\(X(t)\\) \\(X(t)\\), \\(x\\)-pos., \\(y\\)-pos. (F)GLM\\(+\\)MaxProb 0.4626 0.5898 0.7802 (F)GLM\\(+\\)MajVot 0.4600 0.6926 0.9022 (F)GAM\\(+\\)MaxProb 0.7070 0.7384 0.9768 (F)GAM \\(+\\)MajVot 0.7662 0.7824 0.9846 4.4.4 Functional data example data(tecator) y=tecator$y$Fat # Potential functional covariates x=tecator$absorp.fdata x1&lt;-fdata.deriv(x) x2&lt;-fdata.deriv(x,nderiv=2) # Potential factor covariates xcat0&lt;-cut(rnorm(length(y)),4) xcat1&lt;-cut(tecator$y$Protein,4) xcat2&lt;-cut(tecator$y$Water,4) ind &lt;- 1:129 # 3 functionals (x,x1,x2), 3 factors (xcat0, xcat1, xcat2) # and 100 potential scalars covariates (impact poitns of x1) dat &lt;- data.frame(&quot;Fat&quot;=y, x1$data, xcat1, xcat2) ldat &lt;- list(&quot;df&quot;=dat[ind,],&quot;x&quot;=x[ind,],&quot;x1&quot;=x1[ind,],&quot;x2&quot;=x2[ind,]) # Time consuming res.gam1&lt;-fregre.gsam.vs(data=ldat,y=&quot;Fat&quot;) summary(res.gam1$model) ## Fat x2.PC1 x2.PC2 x2.PC3 ## Min. : 0.90 Min. :-0.0033724 Min. :-3.094e-03 Min. :-0.0032994 ## 1st Qu.: 7.70 1st Qu.:-0.0023729 1st Qu.:-7.134e-04 1st Qu.:-0.0001902 ## Median :14.60 Median :-0.0009539 Median :-2.468e-05 Median : 0.0001020 ## Mean :18.24 Mean : 0.0000000 Mean : 0.000e+00 Mean : 0.0000000 ## 3rd Qu.:27.80 3rd Qu.: 0.0017247 3rd Qu.: 6.696e-04 3rd Qu.: 0.0002731 ## Max. :49.10 Max. : 0.0095969 Max. : 6.243e-03 Max. : 0.0034894 ## x2.PC4 ## Min. :-1.076e-03 ## 1st Qu.:-1.076e-04 ## Median : 2.944e-05 ## Mean : 0.000e+00 ## 3rd Qu.: 1.294e-04 ## Max. : 1.125e-03 # Prediction like fregre.gsam() newldat &lt;- list(&quot;df&quot;=dat[-ind,],&quot;x&quot;=x[-ind,],&quot;x1&quot;=x1[-ind,],&quot;x2&quot;=x2[-ind,]) pred.gam1&lt;-predict(res.gam1,newldat) plot(dat[-ind,&quot;Fat&quot;],pred.gam1) 4.5 Optimum Multiscale Selection in 3D Point Cloud Classification (Oviedo-de la Fuente et al. 2021) Supervised classification of 3D point clouds using machine learning algorithms and handcrafted local features as covariates frequently depends on the size of the neighborhood (scale) around each point used to determine those features. It is therefore crucial to estimate the scale or scales providing the best classification results. In this work, we propose three methods to estimate said scales, all of them based on calculating the maximum values of the distance correlation (DC) functions between the features and the label assigned to each point. The performance of the methods was tested using simulated data, and the method presenting the best results was applied to a benchmark data set for point cloud classification. This method consists of detecting the local maximums of DC functions previously smoothed to avoid choosing scales that are very close to each other. Five different classifiers were used: linear discriminant analysis, support vector machines, random forest, multinomial logistic regression and multilayer perceptron neural network. The results obtained were compared with those from other strategies available in the literature, being favorable to our approach. Figure 4.1: Urban scene Oviedo-de la Fuente, M.; Cabo, C.; Ordóñez, C.; Roca-Pardiñas, J. A Distance Correlation Approach for Optimum Multiscale Selection in 3D Point Cloud Classification. Mathematics 2021, 9, 1328. https://doi.org/10.3390/math9121328 Working paper Figure 4.2: Forest scene 4.6 Model Comparison Table collects the results of these fitted models. Recall that RMSE and MAE were calculated using 10-fold cross-validation. AIC and \\(R^2_{adj}\\) were computed using a model fitted with all available data. Results for mean prediction models Model RMSE \\(\\sigma_{\\mathrm{RMSE}}\\) MAE \\(\\sigma_{\\mathrm{MAE}}\\) AIC \\(R^2_{adj}\\) M1 40.83 7.28 24.81 4.01 20254 79.38% M2 57.87 35.23 25.28 4.99 19342 87.90% M3 34.95 6.36 22.34 3.03 18626 86.70% References Akaike, Htrotugu. 1973. “Maximum Likelihood Identification of Gaussian Autoregressive Moving Average Models.” Biometrika 60 (2): 255–65. Berrendero, José R, Antonio Cuevas, and Jos’e L Torrecilla. 2018. “On the Use of Reproducing Kernel Hilbert Spaces in Func2tional Classification.” Journal of the American Statistical Association 113: 1210–18. Berrendero, José R, Antonio Cuevas, and José L Torrecilla. 2016. “Variable Selection in Functional Data Classification: A Maxima-Hunting Proposal.” Statistica Sinica, 619–38. Efron, Bradley, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. 2004. “Least Angle Regression.” The Annals of Statistics 32 (2): 407–99. Febrero-Bande, Manuel, Wenceslao González-Manteiga, and Manuel Oviedo de la Fuente. 2019. “Variable Selection in Functional Additive Regression Models.” Computational Statistics 34 (2): 469–87. https://doi.org/10.1007/s00180-018-0844-5. Ferraty, Frédéric, P Hall, and Philippe Vieu. 2010. “Most-Predictive Design Points for Functional Data Predictors.” Biometrika 97 (4): 807–24. Ferraty, F., and P. Vieu. 2009. “Additive Prediction and Boosting for Functional Data.” Computational Statistics &amp; Data Analysis 53 (4): 1400–1413. http://www.sciencedirect.com/science/article/pii/S0167947308005628. Garcı́a-Portugués, Eduardo, Wenceslao González-Manteiga, and Manuel Febrero-Bande. 2014. “A Goodness-of-Fit Test for the Functional Linear Model with Scalar Response.” Journal of Computational and Graphical Statistics 23 (3): 761–78. Lin, Yi, Hao Helen Zhang, et al. 2006. “Component Selection and Smoothing in Multivariate Nonparametric Regression.” The Annals of Statistics 34 (5): 2272–97. Ordóñez, Celestino, Manuel Oviedo de la Fuente, Javier Roca-Pardiñas, and José Ramón Rodrı́guez-Pérez. 2018. “Determining Optimum Wavelengths for Leaf Water Content Estimation from Reflectance: A Distance Correlation Approach.” Chemometrics and Intelligent Laboratory Systems 173: 41–50. Oviedo-de la Fuente, Manuel, Carlos Cabo, Celestino Ordóñez, and Javier Roca-Pardiñas. 2021. “A Distance Correlation Approach for Optimum Multiscale Selection in 3D Point Cloud Classification.” Mathematics 9 (12): 1328. Peng, Hanchuan, Fuhui Long, and Chris Ding. 2005. “Feature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, no. 8: 1226–38. Székely, G. J., M. L. Rizzo, and N. K. Bakirov. 2007. “Measuring and Testing Dependence by Correlation of Distances.” The Annals of Statistics 35 (6): 2769–94. http://projecteuclid.org/euclid.aos/1201012979. Yenigün, C Deniz, and Maria L Rizzo. 2015. “Variable Selection in Regression Using Maximal Correlation and Distance Correlation.” Journal of Statistical Computation and Simulation 85 (8): 1692–1705. Zhao, Yihong, Huaihou Chen, and R Todd Ogden. 2015. “Wavelet-Based Weighted LASSO and Screening Approaches in Functional Linear Regression.” Journal of Computational and Graphical Statistics 24 (3): 655–75. "],["conclusions-and-future-work.html", "Chapter 5 Conclusions and future work 5.1 Conclusions 5.2 Code: R scripts and notebooks 5.3 Future work 5.4 Future work: Wells-Riley equation References", " Chapter 5 Conclusions and future work 5.1 Conclusions We have observed that models improving the simple model (which uses the current hour \\(CO_2\\) curve as its only predictor) can be built. It is also to mention that, except for the wind speed, none of the ambient conditions variables (temperature, humidity and pressure) were selected by the variable selection algorithm and that their inclusion in the model doesn’t seem to improve the model’s results. Having obtaining as good results as we had, we conclude that this models can be utilized to make real time predictions with good accuracy. 5.2 Code: R scripts and notebooks influx.R a function to download data from our servers and convert it to functional data. funcionesUtiles.R auxiliary functions used to process the data and models (lagging fdata objects, performing CV,…). ARH.R Has code for the ARH(1) model adapted from Christian Alvarez Pelaez master thesis ``Series de tiempo de datos funcionales’’. plotlyFunctions.R functions used to create good-looking plots of the data. AnÃ¡lisisExploratorio.Rmd Conducts an exploratory data analysis on the EBAU data. regresionMediaEBAU.Rmd models for predicting the hourly mean concentration level of \\(CO_2\\) using the EBAU data. regresionValoresPuntualesEBAU.Rmd models for predicting the concentration level of \\(CO_2\\) at the \\(15^{th}\\), \\(30^{th}\\) and \\(45^{th}\\) minute using the EBAU data. regresionMaxEBAU.Rmd odels for predicting the maximum level of \\(CO_2\\). respuestaFuncional.Rmd Implements models with functional response (fregre.basis.fr and ARH(1)). datosFIC.Rmd exploratory data analysis comparing the new data to the preexisting and testing some formulated models on this data. 5.3 Future work Window training: real-time predictions, it would be convenient to train a model using pairs of curves selected at different minutes. Design of experiment (factorial design): type of classes (lectures vs laboratories), occupancy ratio, faculty, campus,… Fourier transforms give us better performance in our predictive models, we could consider other function transformations to make models. Functional response models Functional Time Series such as ARH(p), SARIMAHX, among others. 5.4 Future work: Wells-Riley equation A good measure to quantify the risk of SARS-CoV-2 infection would be the directly estimate the probability of infection. With the Wells-Riley equation (riley1978airborne?). Let \\(X\\) be a ``Number of airborne virus infections in a period of time’’ \\[ X \\sim \\mathrm{Poisson}\\left(\\dfrac{Ipqt}{Q}\\right) \\] \\(D\\): Number of disease cases, \\(S\\): Number of susceptibles, \\(I\\): Number of infectors, \\(p\\): Breathing rate per person (\\(m^3/s\\)), \\(q\\): Quantum generation rate by an infected person (\\(quanta/s\\)), \\(Q\\): Outdoor air supply rate (\\(m^3/s)\\). How many infectors are in the room? What is the exhalation rate per person? References "],["functional-clustering.html", "Chapter 6 Functional Clustering 6.1 K-Means Clustering for functional data, Hartigan and Wong (1979) References Acknowledgements", " Chapter 6 Functional Clustering bla bla bla nuevas herramientas 6.1 K-Means Clustering for functional data, Hartigan and Wong (1979) Perform k-means clustering on functional data. The method searches the locations around which are grouped data (for a predetermined number of groups). + If ncl=NULL, randomizes the initial centers, ncl=2 using kmeans.center.ini function. + If ncl is an integer, indicating the number of groups to classify, are selected ncl initial centers using kmeans.center.ini function. + If ncl is a vector of integers, indicating the position of the initial centers with length(ncl) equal to number of groups. + If ncl is a fdata class objecct, ncl are the initial centers curves with nrow(ncl) number of groups. The fucntion return a list with: + cluster: Indexes of groups assigned. + centers: Curves centers. data(phoneme) mlearn&lt;-phoneme$learn[1:150,] ylearn&lt;-as.numeric(phoneme$classlearn[1:150]) # Unsupervised classification kmeans.assig.groups &lt;- fda.usc:::kmeans.assig.groups kmeans.center.ini &lt;- fda.usc:::kmeans.center.ini kmeans.centers.update&lt;-fda.usc:::kmeans.centers.update out.fd1=fda.usc:::kmeans.fd(mlearn,ncl=c(1,51,101),draw=TRUE) table(out.fd1$cluster,ylearn) ## ylearn ## 1 2 3 ## 1 50 10 3 ## 2 0 40 1 ## 3 0 0 46 # Time consuming # out.fd2=kmeans.fd(mlearn,ncl=3,draw=FALSE,par.ini=list(method=&quot;exact&quot;)) # Different Depth function # ind=c(17,77,126) # out.fd3=kmeans.fd(mlearn,ncl=mlearn[ind,],draw=FALSE, # dfunc=func.trim.FM,par.dfunc=list(trim=0.1)) References Acknowledgements This research/work is part of the grants PID2020-113578RB-I00 and PID2023-147127OB-I00 “ERDF/EU”, funded by MCIN/AEI/10.13039/501100011033/. It has also been supported by the Xunta de Galicia (Grupos de Referencia Competitiva ED431C-2024/14) and by CITIC as a center accredited for excellence within the Galician University System and a member of the CIGUS Network, receives subsidies from the Department of Education, Science, Universities, and Vocational Training of the Xunta de Galicia. Additionally, it is co-financed by the EU through the FEDER Galicia 2021-27 operational program (Ref. ED431G 2023/01). Akaike, Htrotugu. 1973. “Maximum Likelihood Identification of Gaussian Autoregressive Moving Average Models.” Biometrika 60 (2): 255–65. Aneiros-Pérez, Germán, and Philippe Vieu. 2006. “Semi-Functional Partial Linear Regression.” Statist. Probab. Lett. 76 (11): 1102–10. Berrendero, José R, Antonio Cuevas, and Jos’e L Torrecilla. 2018. “On the Use of Reproducing Kernel Hilbert Spaces in Func2tional Classification.” Journal of the American Statistical Association 113: 1210–18. Berrendero, José R, Antonio Cuevas, and José L Torrecilla. 2016. “Variable Selection in Functional Data Classification: A Maxima-Hunting Proposal.” Statistica Sinica, 619–38. Cardot, Hervé, Christophe Crambes, and Pascal Sarda. 2005. “Quantile Regression When the Covariates Are Functions.” Nonparametric Statistics 17 (7): 841–56. Cardot, Hervé, Frédéric Ferraty, and Pascal Sarda. 1999. “Functional Linear Model.” Statist. Probab. Lett. 45 (1): 11–22. Carmack, Patrick S, Jeffrey S Spence, and William R Schucany. 2012. “Generalised Correlated Cross-Validation.” Journal of Nonparametric Statistics 24 (2): 269–82. Chiou, Jeng-Min, Hans-Georg Muller, Jane-Ling Wang, et al. 2004. “Functional Response Models.” Statistica Sinica 14 (3): 675–94. Cuesta-Albertos, Juan A, Manuel Febrero-Bande, and Manuel Oviedo de la Fuente. 2017. Test 26 (1): 119–42. Cuesta-Albertos, Juan, and Alicia Nieto-Reyes. 2008. “The Random Tukey Depth.” Computational Statistics and Data Analysis 52 (11): 4979–88. Cuevas, Antonio, Manuel Febrero, and Ricardo Fraiman. 2007. “Robust Estimation and Classification for Functional Data via Projection-Based Depth Notions.” Comput. Statist. 22 (3): 481–96. http://link.springer.com/article/10.1007/s00180-007-0053-0. Efron, Bradley, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. 2004. “Least Angle Regression.” The Annals of Statistics 32 (2): 407–99. Faraway, Julian J. 1997. “Regression Analysis for a Functional Response.” Technometrics 39 (3): 254–61. Febrero-Bande, Manuel, Pedro Galeano, and Wenceslao González-Manteiga. 2008. “Outlier Detection in Functional Data by Depth Measures, with Application to Identify Abnormal \\({\\rm NO}_x\\) Levels.” Environmetrics 19 (4): 331–45. ———. 2010. “Measures of Influence for the Functional Linear Model with Scalar Response.” J. Multivariate Anal. 101 (2): 327–39. Febrero-Bande, Manuel, and Wenceslao González-Manteiga. 2013. “Generalized Additive Models for Functional Data.” Test 22 (2): 278–92. http://dx.doi.org/10.1007/s11749-012-0308-0. Febrero-Bande, Manuel, Wenceslao González-Manteiga, and Manuel Oviedo de la Fuente. 2019. “Variable Selection in Functional Additive Regression Models.” Computational Statistics 34 (2): 469–87. https://doi.org/10.1007/s00180-018-0844-5. Febrero-Bande, Manuel, and M Oviedo de la Fuente. 2012. “Statistical Computing in Functional Data Analysis: The R Package fda.usc.” J. Statist. Software 51 (4): 1–28. Ferraty, Frédéric, Aldo Goia, Ernesto Salinelli, and Philippe Vieu. 2013. “Functional Projection Pursuit Regression.” Test 22 (2): 293–320. Ferraty, Frédéric, P Hall, and Philippe Vieu. 2010. “Most-Predictive Design Points for Functional Data Predictors.” Biometrika 97 (4): 807–24. Ferraty, Frédéric, Juhyun Park, and Philippe Vieu. 2011. “Estimation of a Functional Single Index Model.” In Recent Advances in Functional Data Analysis and Related Topics, 111–16. Springer. Ferraty, Frédéric, Ingrid Van Keilegom, and Philippe Vieu. 2012. “Regression When Both Response and Predictor Are Functions.” Journal of Multivariate Analysis 109: 10–28. Ferraty, Frédéric, and Philippe Vieu. 2003. “Curves Discrimination: A Nonparametric Functional Approach.” Comput. Statist. Data Anal. 44 (1): 161–73. http://www.sciencedirect.com/science/article/pii/S016794730300032X. ———. 2006. Nonparametric Functional Data Analysis. Springer Series in Statistics. New York: Springer-Verlag. Ferraty, F., and P. Vieu. 2009. “Additive Prediction and Boosting for Functional Data.” Computational Statistics &amp; Data Analysis 53 (4): 1400–1413. http://www.sciencedirect.com/science/article/pii/S0167947308005628. Fraiman, Ricardo, and Graciela Muniz. 2001. “Trimmed Means for Functional Data.” Test 10 (2): 419–40. Garcı́a-Portugués, Eduardo, Wenceslao González-Manteiga, and Manuel Febrero-Bande. 2014. “A Goodness-of-Fit Test for the Functional Linear Model with Scalar Response.” Journal of Computational and Graphical Statistics 23 (3): 761–78. Hartigan, John A, and Manchek A Wong. 1979. “Algorithm AS 136: A k-Means Clustering Algorithm.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 28 (1): 100–108. Kato, Kengo et al. 2012. “Estimation in Functional Linear Quantile Regression.” The Annals of Statistics 40 (6): 3108–36. Krämer, Nicole, and Masashi Sugiyama. 2011. “The Degrees of Freedom of Partial Least Squares Regression.” Journal of the American Statistical Association 106 (494): 697–705. Li, Jun, Juan A Cuesta-Albertos, and Regina Y Liu. 2012. “\\(DD\\)–Classifier: Nonparametric Classification Procedure Based on \\(DD\\)–Plot.” J. Amer. Statist. Assoc. 107 (498): 737–53. http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2012.688462. Lin, Yi, Hao Helen Zhang, et al. 2006. “Component Selection and Smoothing in Multivariate Nonparametric Regression.” The Annals of Statistics 34 (5): 2272–97. López-Pintado, Satan, and Juan Romo. 2009. “On the Concept of Depth for Functional Data.” Journal of the American Statistical Association 104: 718–34. Muller, Hans-Georg et al. 2008. “Functional Modeling of Longitudinal Data.” In Longitudinal Data Analysis, 237–66. Chapman; Hall/CRC. Müller, Hans-Georg, and Ulrich Stadtmüller. 2005. “Generalized Functional Linear Models.” Annals of Statistics, 774–805. Müller, Hans-Georg, and Fang Yao. 2012. “Functional Additive Models.” Journal of the American Statistical Association. Ordóñez, Celestino, Manuel Oviedo de la Fuente, Javier Roca-Pardiñas, and José Ramón Rodrı́guez-Pérez. 2018. “Determining Optimum Wavelengths for Leaf Water Content Estimation from Reflectance: A Distance Correlation Approach.” Chemometrics and Intelligent Laboratory Systems 173: 41–50. Oviedo de la Fuente, Manuel, Manuel Febrero-Bande, Marı́a Pilar Muñoz, and Àngela Domı́nguez. 2018. “Predicting Seasonal Influenza Transmission Using Functional Regression Models with Temporal Dependence.” PloS One 13 (4): e0194250. Oviedo-de la Fuente, Manuel, Carlos Cabo, Celestino Ordóñez, and Javier Roca-Pardiñas. 2021. “A Distance Correlation Approach for Optimum Multiscale Selection in 3D Point Cloud Classification.” Mathematics 9 (12): 1328. Peng, Hanchuan, Fuhui Long, and Chris Ding. 2005. “Feature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, no. 8: 1226–38. Preda, C., and G. Saporta. 2005. “PLS Regression on a Stochastic Process.” Comput. Statist. Data Anal. 48 (1): 149–58. Ramsay, J. O., and B. W. Silverman. 2005a. Functional Data Analysis. Springer. ———. 2005b. Functional Data Analysis. Second. Springer Series in Statistics. New York: Springer-Verlag. Székely, G. J., M. L. Rizzo, and N. K. Bakirov. 2007. “Measuring and Testing Dependence by Correlation of Distances.” The Annals of Statistics 35 (6): 2769–94. http://projecteuclid.org/euclid.aos/1201012979. Székely, Gábor J, and Maria L Rizzo. 2013. “The Distance Correlation \\(t\\)-Test of Independence in High Dimension.” J. Multivariate Anal. 117: 193–213. Wood, Simon N. 2004. “Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models.” Journal of the American Statistical Association 99 (467): 673–86. Yenigün, C Deniz, and Maria L Rizzo. 2015. “Variable Selection in Regression Using Maximal Correlation and Distance Correlation.” Journal of Statistical Computation and Simulation 85 (8): 1692–1705. Zhao, Yihong, Huaihou Chen, and R Todd Ogden. 2015. “Wavelet-Based Weighted LASSO and Screening Approaches in Functional Linear Regression.” Journal of Computational and Graphical Statistics 24 (3): 655–75. Zivot, Eric, and Jiahui Wang. 2007. Modeling Financial Time Series with s-Plus. Vol. 191. Springer Science &amp; Business Media. References Hartigan, John A, and Manchek A Wong. 1979. “Algorithm AS 136: A k-Means Clustering Algorithm.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 28 (1): 100–108. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
