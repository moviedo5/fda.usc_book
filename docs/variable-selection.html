<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Variable Selection | Functional Data Analysis using fda.usc and fda.clust packages</title>
  <meta name="description" content="Functional Data Analysis, Regression, Classification and Clustering using fda.usc and fda.clust packages" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Variable Selection | Functional Data Analysis using fda.usc and fda.clust packages" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Functional Data Analysis, Regression, Classification and Clustering using fda.usc and fda.clust packages" />
  <meta name="github-repo" content="moviedo5/bookdown_fda_clust" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Variable Selection | Functional Data Analysis using fda.usc and fda.clust packages" />
  
  <meta name="twitter:description" content="Functional Data Analysis, Regression, Classification and Clustering using fda.usc and fda.clust packages" />
  

<meta name="author" content="Manuel Oviedo de la Fuente (Universidade of A Coruña, CITIC, MODES RG) and Manuel Febrero-Bande (Universidade de Santaigo de Compostela, MODESTYA RG)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="functional-supervised-classification.html"/>
<link rel="next" href="functional-clustering.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Escritura de libros con bookdown</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#cran-task-view"><i class="fa fa-check"></i>CRAN Task View</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#functional-data-analysis-in-r"><i class="fa fa-check"></i>Functional Data Analysis in R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#installation"><i class="fa fa-check"></i>Installation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#quick-start"><i class="fa fa-check"></i>Quick Start</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="definition.html"><a href="definition.html"><i class="fa fa-check"></i><b>1</b> Functional Data: Definition, Representation and Manipulation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="definition.html"><a href="definition.html#some-definitions-of-functional-data"><i class="fa fa-check"></i><b>1.1</b> Some definitions of Functional Data</a></li>
<li class="chapter" data-level="1.2" data-path="definition.html"><a href="definition.html#in-fda.usc-the-data-are-curves"><i class="fa fa-check"></i><b>1.2</b> In fda.usc: ``The data are curves’’</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="definition.html"><a href="definition.html#definition-of-fdata-class-in-r"><i class="fa fa-check"></i><b>1.2.1</b> Definition of –fdata– class in R</a></li>
<li class="chapter" data-level="1.2.2" data-path="definition.html"><a href="definition.html#some-utilities-of-fda.usc-package"><i class="fa fa-check"></i><b>1.2.2</b> Some utilities of fda.usc package</a></li>
<li class="chapter" data-level="1.2.3" data-path="definition.html"><a href="definition.html#definition-of-ldata-class-in-r"><i class="fa fa-check"></i><b>1.2.3</b> Definition of –ldata– class in R</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="definition.html"><a href="definition.html#resume-by-smoothing"><i class="fa fa-check"></i><b>1.3</b> Resume by smoothing</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="definition.html"><a href="definition.html#derivatives"><i class="fa fa-check"></i><b>1.3.1</b> Derivatives</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="definition.html"><a href="definition.html#semi-metric-as-classification-rule"><i class="fa fa-check"></i><b>1.4</b> Semi-metric as classification rule</a></li>
<li class="chapter" data-level="1.5" data-path="definition.html"><a href="definition.html#correlation-distances-szekely2007"><i class="fa fa-check"></i><b>1.5</b> Correlation Distances <span class="citation">(G. J. Székely, Rizzo, and Bakirov 2007)</span></a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="definition.html"><a href="definition.html#depth-for-functional-data"><i class="fa fa-check"></i><b>1.5.1</b> Depth for functional data</a></li>
<li class="chapter" data-level="1.5.2" data-path="definition.html"><a href="definition.html#depth-and-distances-for-multivariate-functional-data-cuesta2017hbox"><i class="fa fa-check"></i><b>1.5.2</b> Depth (and distances) for multivariate functional data <span class="citation">(J. A. Cuesta-Albertos, Febrero-Bande, and Oviedo de la Fuente 2017)</span></a></li>
<li class="chapter" data-level="1.5.3" data-path="definition.html"><a href="definition.html#outliers-detection"><i class="fa fa-check"></i><b>1.5.3</b> Outliers detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>2</b> Functional Regression Model</a>
<ul>
<li class="chapter" data-level="2.1" data-path="regression.html"><a href="regression.html#functional-linear-model-flr-with-basis-representation"><i class="fa fa-check"></i><b>2.1</b> Functional linear model (FLR) with basis representation</a></li>
<li class="chapter" data-level="2.2" data-path="regression.html"><a href="regression.html#flm-with-functional-and-non-functional-covariates"><i class="fa fa-check"></i><b>2.2</b> FLM with functional and non functional covariates</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="regression.html"><a href="regression.html#predict-method-for-functional-regression-model"><i class="fa fa-check"></i><b>2.2.1</b> Predict method for functional regression model</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="regression.html"><a href="regression.html#other-procedures"><i class="fa fa-check"></i><b>2.3</b> Other procedures</a></li>
<li class="chapter" data-level="2.4" data-path="regression.html"><a href="regression.html#non-linear-model-fv2006"><i class="fa fa-check"></i><b>2.4</b> Non Linear Model <span class="citation">(Frédéric Ferraty and Vieu 2006)</span></a></li>
<li class="chapter" data-level="2.5" data-path="regression.html"><a href="regression.html#semi-linear-model-aneiros2005"><i class="fa fa-check"></i><b>2.5</b> Semi Linear Model <span class="citation">(Aneiros-Pérez and Vieu 2006)</span></a></li>
<li class="chapter" data-level="2.6" data-path="regression.html"><a href="regression.html#generalized-linear-models-muller2005generalized"><i class="fa fa-check"></i><b>2.6</b> Generalized Linear Models <span class="citation">(Müller and Stadtmüller 2005)</span></a></li>
<li class="chapter" data-level="2.7" data-path="regression.html"><a href="regression.html#generalized-functional-additive-model"><i class="fa fa-check"></i><b>2.7</b> Generalized Functional Additive Model</a></li>
<li class="chapter" data-level="2.8" data-path="regression.html"><a href="regression.html#functional-gls-model"><i class="fa fa-check"></i><b>2.8</b> Functional GLS model</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="regression.html"><a href="regression.html#dependent-data-example"><i class="fa fa-check"></i><b>2.8.1</b> Dependent data example,</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="regression.html"><a href="regression.html#functional-response-model"><i class="fa fa-check"></i><b>2.9</b> Functional Response Model</a></li>
<li class="chapter" data-level="2.10" data-path="regression.html"><a href="regression.html#other-models"><i class="fa fa-check"></i><b>2.10</b> Other Models:</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="functional-supervised-classification.html"><a href="functional-supervised-classification.html"><i class="fa fa-check"></i><b>3</b> Functional Supervised Classification</a>
<ul>
<li class="chapter" data-level="3.1" data-path="functional-supervised-classification.html"><a href="functional-supervised-classification.html#logistic-regression-model-glm-classif.glm"><i class="fa fa-check"></i><b>3.1</b> Logistic Regression Model (GLM): <code>classif.glm</code></a></li>
<li class="chapter" data-level="3.2" data-path="functional-supervised-classification.html"><a href="functional-supervised-classification.html#generalized-additive-models-gam-classif.gsam-and-classif.gkam"><i class="fa fa-check"></i><b>3.2</b> Generalized Additive Models (GAM): <code>classif.gsam</code> and <code>classif.gkam</code></a></li>
<li class="chapter" data-level="3.3" data-path="functional-supervised-classification.html"><a href="functional-supervised-classification.html#nonparametric-classification-methods-classif.knn-and-classif.np-ferraty2003"><i class="fa fa-check"></i><b>3.3</b> Nonparametric classification methods: <code>classif.knn</code> and <code>classif.np</code> <span class="citation">(Frédéric Ferraty and Vieu 2003)</span></a></li>
<li class="chapter" data-level="3.4" data-path="functional-supervised-classification.html"><a href="functional-supervised-classification.html#maximum-depth-classif.depth-li2012"><i class="fa fa-check"></i><b>3.4</b> Maximum depth: <code>classif.depth</code> <span class="citation">(Li, Cuesta-Albertos, and Liu 2012)</span></a></li>
<li class="chapter" data-level="3.5" data-path="functional-supervised-classification.html"><a href="functional-supervised-classification.html#the-ddgclassifier-classif.dd-cuesta2017hbox"><i class="fa fa-check"></i><b>3.5</b> The DD<span class="math inline">\(^G\)</span>–classifier <code>classif.DD</code> <span class="citation">(J. A. Cuesta-Albertos, Febrero-Bande, and Oviedo de la Fuente 2017)</span></a></li>
<li class="chapter" data-level="3.6" data-path="functional-supervised-classification.html"><a href="functional-supervised-classification.html#classifiers-adapted-from-multivariate-framework"><i class="fa fa-check"></i><b>3.6</b> Classifiers adapted from Multivariate Framework</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="variable-selection.html"><a href="variable-selection.html"><i class="fa fa-check"></i><b>4</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="4.1" data-path="variable-selection.html"><a href="variable-selection.html#functiondal-regression-with-points-of-impact"><i class="fa fa-check"></i><b>4.1</b> Functiondal regression with points of impact</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="variable-selection.html"><a href="variable-selection.html#state-of-art"><i class="fa fa-check"></i><b>4.1.1</b> State of Art</a></li>
<li class="chapter" data-level="4.1.2" data-path="variable-selection.html"><a href="variable-selection.html#local-maxima-distance-correlation-approach-lmdc-ordonez2018"><i class="fa fa-check"></i><b>4.1.2</b> Local maxima distance correlation approach (LMDC), <span class="citation">(Ordóñez et al. 2018)</span></a></li>
<li class="chapter" data-level="4.1.3" data-path="variable-selection.html"><a href="variable-selection.html#lmdc-algorithm-lmdc.select-function"><i class="fa fa-check"></i><b>4.1.3</b> LMDC Algorithm: <code>LMDC.select()</code> function</a></li>
<li class="chapter" data-level="4.1.4" data-path="variable-selection.html"><a href="variable-selection.html#lmdc-algorithm-lmdc.regre-function"><i class="fa fa-check"></i><b>4.1.4</b> LMDC Algorithm: <code>LMDC.regre()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="variable-selection.html"><a href="variable-selection.html#variable-selection-in-functional-regression"><i class="fa fa-check"></i><b>4.2</b> Variable selection in functional regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="variable-selection.html"><a href="variable-selection.html#state-of-art-1"><i class="fa fa-check"></i><b>4.2.1</b> State of Art</a></li>
<li class="chapter" data-level="4.2.2" data-path="variable-selection.html"><a href="variable-selection.html#algorithm"><i class="fa fa-check"></i><b>4.2.2</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="variable-selection.html"><a href="variable-selection.html#binary-classification-example"><i class="fa fa-check"></i><b>4.3</b> Binary classification example</a></li>
<li class="chapter" data-level="4.4" data-path="variable-selection.html"><a href="variable-selection.html#hyperspectral-images-example"><i class="fa fa-check"></i><b>4.4</b> Hyperspectral images example</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="variable-selection.html"><a href="variable-selection.html#hyperspectral-data-set-pavia-university"><i class="fa fa-check"></i><b>4.4.1</b> Hyperspectral Data set: Pavia University</a></li>
<li class="chapter" data-level="4.4.2" data-path="variable-selection.html"><a href="variable-selection.html#hyperspectral-images"><i class="fa fa-check"></i><b>4.4.2</b> Hyperspectral images</a></li>
<li class="chapter" data-level="4.4.3" data-path="variable-selection.html"><a href="variable-selection.html#pravia-univ.-results"><i class="fa fa-check"></i><b>4.4.3</b> Pravia Univ. Results</a></li>
<li class="chapter" data-level="4.4.4" data-path="variable-selection.html"><a href="variable-selection.html#functional-data-example"><i class="fa fa-check"></i><b>4.4.4</b> Functional data example</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="variable-selection.html"><a href="variable-selection.html#optimum-multiscale-selection-in-3d-point-cloud-classification-oviedo2021distance"><i class="fa fa-check"></i><b>4.5</b> Optimum Multiscale Selection in 3D Point Cloud Classification <span class="citation">(Oviedo-de la Fuente et al. 2021)</span></a></li>
<li class="chapter" data-level="4.6" data-path="variable-selection.html"><a href="variable-selection.html#model-comparison"><i class="fa fa-check"></i><b>4.6</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="functional-clustering.html"><a href="functional-clustering.html"><i class="fa fa-check"></i><b>5</b> Functional Clustering</a>
<ul>
<li class="chapter" data-level="5.1" data-path="functional-clustering.html"><a href="functional-clustering.html#r-packages-for-clustering"><i class="fa fa-check"></i><b>5.1</b> R Packages for Clustering</a></li>
<li class="chapter" data-level="5.2" data-path="functional-clustering.html"><a href="functional-clustering.html#r-packages-for-clustering-functional-data"><i class="fa fa-check"></i><b>5.2</b> R Packages for Clustering Functional Data</a></li>
<li class="chapter" data-level="5.3" data-path="functional-clustering.html"><a href="functional-clustering.html#k-means-clustering-for-functional-data"><i class="fa fa-check"></i><b>5.3</b> K-Means Clustering for Functional Data</a></li>
<li class="chapter" data-level="5.4" data-path="functional-clustering.html"><a href="functional-clustering.html#functional-data-clustering-with-dbscan-fdbscan"><i class="fa fa-check"></i><b>5.4</b> Functional Data Clustering with DBSCAN (<code>fdbscan</code>)</a></li>
<li class="chapter" data-level="5.5" data-path="functional-clustering.html"><a href="functional-clustering.html#functional-data-clustering-with-mean-shift-fmeanshift"><i class="fa fa-check"></i><b>5.5</b> Functional Data Clustering with Mean Shift (<code>fmeanshift</code>)</a></li>
<li class="chapter" data-level="5.6" data-path="functional-clustering.html"><a href="functional-clustering.html#multivariate-functional-data-clustering"><i class="fa fa-check"></i><b>5.6</b> Multivariate Functional Data Clustering</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="functional-clustering.html"><a href="functional-clustering.html#hierarchical-clustering-with-mfhclust"><i class="fa fa-check"></i><b>5.6.1</b> Hierarchical Clustering with <code>mfhclust()</code></a></li>
<li class="chapter" data-level="5.6.2" data-path="functional-clustering.html"><a href="functional-clustering.html#k-means-clustering-with-mfkmeans"><i class="fa fa-check"></i><b>5.6.2</b> K-means Clustering with mfkmeans()</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="functional-clustering.html"><a href="functional-clustering.html#cluster-validation-measures"><i class="fa fa-check"></i><b>5.7</b> Cluster Validation Measures</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="functional-clustering.html"><a href="functional-clustering.html#silhouette-index"><i class="fa fa-check"></i><b>5.7.1</b> Silhouette Index</a></li>
<li class="chapter" data-level="5.7.2" data-path="functional-clustering.html"><a href="functional-clustering.html#dunn-index"><i class="fa fa-check"></i><b>5.7.2</b> Dunn Index</a></li>
<li class="chapter" data-level="5.7.3" data-path="functional-clustering.html"><a href="functional-clustering.html#davies-bouldin-index"><i class="fa fa-check"></i><b>5.7.3</b> Davies-Bouldin Index</a></li>
<li class="chapter" data-level="5.7.4" data-path="functional-clustering.html"><a href="functional-clustering.html#calinski-harabasz-index"><i class="fa fa-check"></i><b>5.7.4</b> Calinski-Harabasz Index</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="functional-clustering.html"><a href="functional-clustering.html#conclusions-and-future-work"><i class="fa fa-check"></i>Conclusions and Future Work</a></li>
<li class="chapter" data-level="" data-path="functional-clustering.html"><a href="functional-clustering.html#chapter-references"><i class="fa fa-check"></i>Chapter references</a></li>
<li class="chapter" data-level="" data-path="functional-clustering.html"><a href="functional-clustering.html#funding-and-financial-support"><i class="fa fa-check"></i>Funding and Financial Support</a></li>
<li class="chapter" data-level="" data-path="functional-clustering.html"><a href="functional-clustering.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Functional Data Analysis using fda.usc and fda.clust packages</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variable-selection" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Variable Selection<a href="variable-selection.html#variable-selection" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- 
---
output: html_document
bibliography: biblio.bib
---
%\VignetteEngine{knitr::knitr} 
%\VignetteIndexEntry{}
#    Functional Data Analysis using fda.usc <img src="Rlogo.jpg" width="75"  /> package 
<a id="top"></a>
-->
<div id="functiondal-regression-with-points-of-impact" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Functiondal regression with points of impact<a href="variable-selection.html#functiondal-regression-with-points-of-impact" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="state-of-art" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> State of Art<a href="variable-selection.html#state-of-art" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Nonparametric variable selection approach (NOVAS). NOVAS is quite expensive from a computational perspective, <span class="citation">Frédéric Ferraty, Hall, and Vieu (<a href="#ref-ferraty2010most">2010</a>)</span>.</p></li>
<li><p>A wavelet-based weighted LASSO functional linear (FWLASSO).
FWLASSO requires transforming the original variables and assuming a linear model, <span class="citation">Zhao, Chen, and Ogden (<a href="#ref-Zhao2015">2015</a>)</span>.</p></li>
<li><p><span class="citation">Berrendero, Cuevas, and Torrecilla (<a href="#ref-berrendero2016variable">2016</a>)</span> use the Maxima-hunting proposal to choose the most relevant design points in functional classification setting.</p></li>
</ul>
</div>
<div id="local-maxima-distance-correlation-approach-lmdc-ordonez2018" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Local maxima distance correlation approach (LMDC), <span class="citation">(<a href="#ref-Ordonez2018">Ordóñez et al. 2018</a>)</span><a href="variable-selection.html#local-maxima-distance-correlation-approach-lmdc-ordonez2018" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>In this work we study the utility of distance correlation <span class="citation">G. J. Székely, Rizzo, and Bakirov (<a href="#ref-Szekely2007">2007</a>)</span> as an intrinsic method for variable selection.</p></li>
<li><p>Neither projection nor transformation of the variables is needed. Moreover, it is unnecessary to assume an a priori regression model.</p></li>
<li><p>LMDC approach consists in calculating the local maxima of the distance correlation along the curve.</p></li>
</ul>
</div>
<div id="lmdc-algorithm-lmdc.select-function" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> LMDC Algorithm: <code>LMDC.select()</code> function<a href="variable-selection.html#lmdc-algorithm-lmdc.select-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>Calculate de distance correlation (DC) <span class="math inline">\(R(t) = \left \lbrace  R(X(t_j),Y)  \right\rbrace {_{j=1}^N}\)</span>, from the data <span class="math inline">\(\left \lbrace X_i (t_j),Y_i \right\rbrace _{i=1}^n\)</span>.</p></li>
<li><p>Calculate the LM of the <span class="math inline">\(\hat{\mathcal{R}} (t)\)</span>.
Only the significant local maxima for a default level of significance are selected.
Denoting the arguments values (argvals) of the local maxima a <span class="math inline">\(\tilde t_1,\tilde t_2,\ldots,\tilde t_{\tilde{N}}\)</span> (<span class="math inline">\(\tilde{N}&lt;N\)</span>), we ordered them from highest to lowest values of DC, that is <span class="math inline">\(\hat{\mathcal{R}}(\tilde t_1) \geq
\hat{ \mathcal{R}}(\tilde t_2) &gt;\ldots \geq    \hat {\mathcal{R}}(\tilde t_{\tilde{N}})\)</span></p></li>
</ol>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="variable-selection.html#cb189-1" tabindex="-1"></a><span class="fu">library</span>(fda.usc)</span>
<span id="cb189-2"><a href="variable-selection.html#cb189-2" tabindex="-1"></a>pred2gsam <span class="ot">&lt;-</span> fda.usc.devel<span class="sc">:::</span>pred2gsam </span>
<span id="cb189-3"><a href="variable-selection.html#cb189-3" tabindex="-1"></a>predict.classif <span class="ot">&lt;-</span>  fda.usc.devel<span class="sc">:::</span>predict.classif</span>
<span id="cb189-4"><a href="variable-selection.html#cb189-4" tabindex="-1"></a>predict.fregre.gsam  <span class="ot">&lt;-</span>  fda.usc.devel<span class="sc">:::</span>predict.fregre.gsam </span></code></pre></div>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="variable-selection.html#cb190-1" tabindex="-1"></a><span class="fu">data</span>(tecator)</span>
<span id="cb190-2"><a href="variable-selection.html#cb190-2" tabindex="-1"></a>X.d2<span class="ot">&lt;-</span><span class="fu">fdata.deriv</span>(tecator[[<span class="st">&quot;absorp.fdata&quot;</span>]],</span>
<span id="cb190-3"><a href="variable-selection.html#cb190-3" tabindex="-1"></a><span class="at">nderiv =</span> <span class="dv">2</span>)</span>
<span id="cb190-4"><a href="variable-selection.html#cb190-4" tabindex="-1"></a><span class="fu">colnames</span>(X.d2[[<span class="st">&quot;data&quot;</span>]])<span class="ot">&lt;-</span><span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="fu">round</span>(X.d2[[<span class="st">&quot;argvals&quot;</span>]]))</span>
<span id="cb190-5"><a href="variable-selection.html#cb190-5" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;y&quot;</span><span class="ot">=</span>tecator[[<span class="st">&quot;y&quot;</span>]][[<span class="st">&quot;Fat&quot;</span>]],X.d2[[<span class="st">&quot;data&quot;</span>]] )</span>
<span id="cb190-6"><a href="variable-selection.html#cb190-6" tabindex="-1"></a>tol<span class="ot">&lt;-</span>.<span class="dv">2</span></span>
<span id="cb190-7"><a href="variable-selection.html#cb190-7" tabindex="-1"></a>dc.raw <span class="ot">&lt;-</span> <span class="fu">LMDC.select</span>(<span class="st">&quot;y&quot;</span>,<span class="at">data =</span> dat, <span class="at">tol =</span> tol,<span class="at">pvalue =</span> <span class="fl">0.05</span>,</span>
<span id="cb190-8"><a href="variable-selection.html#cb190-8" tabindex="-1"></a><span class="at">plot=</span>F)</span>
<span id="cb190-9"><a href="variable-selection.html#cb190-9" tabindex="-1"></a><span class="co"># Preselected impact points </span></span>
<span id="cb190-10"><a href="variable-selection.html#cb190-10" tabindex="-1"></a>covar<span class="ot">&lt;-</span><span class="fu">names</span>(dat)[<span class="sc">-</span><span class="dv">1</span>][dc.raw[[<span class="st">&quot;maxLocal&quot;</span>]]]</span>
<span id="cb190-11"><a href="variable-selection.html#cb190-11" tabindex="-1"></a>covar</span></code></pre></div>
<pre><code>##  [1] &quot;X933&quot;  &quot;X1046&quot; &quot;X907&quot;  &quot;X886&quot;  &quot;X896&quot;  &quot;X1010&quot; &quot;X1020&quot; &quot;X1030&quot; &quot;X945&quot; 
## [10] &quot;X876&quot;  &quot;X915&quot;  &quot;X862&quot;  &quot;X993&quot;</code></pre>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="variable-selection.html#cb192-1" tabindex="-1"></a><span class="fu">length</span>(covar)</span></code></pre></div>
<pre><code>## [1] 13</code></pre>
</div>
<div id="lmdc-algorithm-lmdc.regre-function" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> LMDC Algorithm: <code>LMDC.regre()</code> function<a href="variable-selection.html#lmdc-algorithm-lmdc.regre-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol start="3" style="list-style-type: decimal">
<li>(Optionally) Check if the relationship between the reponse and the predictor variables is linear: <span class="math inline">\(H_0:\,Y=\big&lt;X,\beta\big&gt;+\epsilon\)</span>, versus a general alternative using a test of linearity proposed in <span class="citation">Garcı́a-Portugués, González-Manteiga, and Febrero-Bande (<a href="#ref-garcia2014goodness">2014</a>)</span>.</li>
</ol>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="variable-selection.html#cb194-1" tabindex="-1"></a>ftest<span class="ot">&lt;-</span><span class="fu">flm.test</span>(dat[,<span class="sc">-</span><span class="dv">1</span>], dat[,<span class="st">&quot;y&quot;</span>],</span>
<span id="cb194-2"><a href="variable-selection.html#cb194-2" tabindex="-1"></a><span class="at">verbose=</span>F,<span class="at">plot.it=</span>F)</span>
<span id="cb194-3"><a href="variable-selection.html#cb194-3" tabindex="-1"></a>ftest</span></code></pre></div>
<pre><code>## 
##  PCvM test for the functional linear model using optimal PLS basis
##  representation
## 
## data:  Y=&lt;X,b&gt;+e
## PCvM statistic = 216.66, p-value &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="variable-selection.html#cb196-1" tabindex="-1"></a>ftest[[<span class="st">&quot;p.value&quot;</span>]]</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<ol start="4" style="list-style-type: decimal">
<li><p>Fit a regression model to the response of interest <span class="math inline">\(Y\)</span> using the vector of covariates <span class="math inline">\(X(\tilde{t})=\{X(\tilde t_1), \ldots, X(\tilde{t}_{\tilde{N}})\}\)</span>. A linear model will be used if the null hypothesis is not rejected and a nonparametric (e.g. generalized additive model) model otherwise.</p></li>
<li><p>(Optionally) Once the type model has been selected, we propose to
Apply a forward stepwise regression method to determine the significant covariates, taking advantage of the fact that the local maxima have been ordered. This means we start with a model with the first covariate (the one with the highest value of distance correlation), and the rest of the ordered covariates are added to the model in turn. This substantially reduces the computing time.</p></li>
</ol>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="variable-selection.html#cb198-1" tabindex="-1"></a><span class="cf">if</span> (ftest<span class="sc">$</span>p.value <span class="sc">&gt;</span> <span class="fl">0.05</span>) { <span class="co"># Linear relationship, step-wise lm is recommended</span></span>
<span id="cb198-2"><a href="variable-selection.html#cb198-2" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">LMDC.regre</span>(<span class="at">y =</span> <span class="st">&quot;y&quot;</span>, <span class="at">covar =</span> covar, <span class="at">data =</span> dat, <span class="at">pvalue=</span>.<span class="dv">05</span>, <span class="at">method =</span><span class="st">&quot;lm&quot;</span>)</span>
<span id="cb198-3"><a href="variable-selection.html#cb198-3" tabindex="-1"></a>} <span class="cf">else</span> {<span class="co"># Non-Linear relationship, step-wise gam is recommended</span></span>
<span id="cb198-4"><a href="variable-selection.html#cb198-4" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">LMDC.regre</span>(<span class="at">y =</span> <span class="st">&quot;y&quot;</span>, <span class="at">covar =</span> covar, <span class="at">data =</span> dat,<span class="at">pvalue=</span>.<span class="dv">05</span>, <span class="at">method =</span><span class="st">&quot;gam&quot;</span>)}  </span>
<span id="cb198-5"><a href="variable-selection.html#cb198-5" tabindex="-1"></a>out</span></code></pre></div>
<pre><code>## $model
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## y ~ s(X933, k = 4) + s(X1046, k = 4) + s(X907, k = 4) + s(X886, 
##     k = 4) + s(X896, k = 4) + s(X1010, k = 4) + s(X1020, k = 4) + 
##     s(X1030, k = 4) + s(X945, k = 4) + s(X876, k = 4) + s(X915, 
##     k = 4) + s(X993, k = 4)
## 
## Estimated degrees of freedom:
## 3.00 2.55 2.83 2.00 1.00 1.00 2.94 
## 2.92 2.81 2.57 1.00 2.85  total = 28.48 
## 
## GCV score: 0.4403176     
## 
## $xvar
##  [1] &quot;X933&quot;  &quot;X1046&quot; &quot;X907&quot;  &quot;X886&quot;  &quot;X896&quot;  &quot;X1010&quot; &quot;X1020&quot; &quot;X1030&quot; &quot;X945&quot; 
## [10] &quot;X876&quot;  &quot;X915&quot;  &quot;X993&quot; 
## 
## $pred
## NULL
## 
## $edf
## [1] 28.48142
## 
## $nvar
## [1] 12</code></pre>
<p>Differences in mean square prediction error between linear (usign <code>lm</code> model) and non-linear (usign <code>gam</code> model) model</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="variable-selection.html#cb200-1" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">LMDC.regre</span>(<span class="at">y =</span> <span class="st">&quot;y&quot;</span>, <span class="at">covar =</span> covar, <span class="at">data =</span> dat[<span class="dv">1</span><span class="sc">:</span><span class="dv">165</span>,],<span class="at">newdata=</span>dat[<span class="dv">166</span><span class="sc">:</span><span class="dv">215</span>,], <span class="at">pvalue=</span>.<span class="dv">05</span>, <span class="at">method =</span><span class="st">&quot;lm&quot;</span>)</span>
<span id="cb200-2"><a href="variable-selection.html#cb200-2" tabindex="-1"></a><span class="fu">mean</span>((out<span class="sc">$</span>pred<span class="sc">-</span>dat<span class="sc">$</span>y[<span class="dv">166</span><span class="sc">:</span><span class="dv">215</span>])<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 11.75474</code></pre>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="variable-selection.html#cb202-1" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">LMDC.regre</span>(<span class="at">y =</span> <span class="st">&quot;y&quot;</span>, <span class="at">covar =</span> covar, <span class="at">data =</span> dat[<span class="dv">1</span><span class="sc">:</span><span class="dv">165</span>,],<span class="at">newdata=</span>dat[<span class="dv">166</span><span class="sc">:</span><span class="dv">215</span>,], <span class="at">pvalue=</span>.<span class="dv">05</span>, <span class="at">method =</span><span class="st">&quot;gam&quot;</span>)</span>
<span id="cb202-2"><a href="variable-selection.html#cb202-2" tabindex="-1"></a><span class="fu">mean</span>((out<span class="sc">$</span>pred<span class="sc">-</span>dat<span class="sc">$</span>y[<span class="dv">166</span><span class="sc">:</span><span class="dv">215</span>])<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1.148573</code></pre>
<blockquote>
<p>Binary classification example (Impact point selection, model estimation and prediction)</p>
</blockquote>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="variable-selection.html#cb204-1" tabindex="-1"></a><span class="fu">data</span>(tecator)</span>
<span id="cb204-2"><a href="variable-selection.html#cb204-2" tabindex="-1"></a>X.d2<span class="ot">&lt;-</span><span class="fu">fdata.deriv</span>(tecator[[<span class="st">&quot;absorp.fdata&quot;</span>]],</span>
<span id="cb204-3"><a href="variable-selection.html#cb204-3" tabindex="-1"></a><span class="at">nderiv =</span> <span class="dv">2</span>)</span>
<span id="cb204-4"><a href="variable-selection.html#cb204-4" tabindex="-1"></a><span class="fu">colnames</span>(X.d2[[<span class="st">&quot;data&quot;</span>]])<span class="ot">&lt;-</span><span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="fu">round</span>(X.d2[[<span class="st">&quot;argvals&quot;</span>]]))</span>
<span id="cb204-5"><a href="variable-selection.html#cb204-5" tabindex="-1"></a>y2groups <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(tecator[[<span class="st">&quot;y&quot;</span>]][[<span class="st">&quot;Fat&quot;</span>]]<span class="sc">&lt;</span><span class="dv">12</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb204-6"><a href="variable-selection.html#cb204-6" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;y2groups&quot;</span><span class="ot">=</span>y2groups,X.d2[[<span class="st">&quot;data&quot;</span>]] )</span>
<span id="cb204-7"><a href="variable-selection.html#cb204-7" tabindex="-1"></a>tol<span class="ot">&lt;-</span>.<span class="dv">1</span></span>
<span id="cb204-8"><a href="variable-selection.html#cb204-8" tabindex="-1"></a>dc.raw <span class="ot">&lt;-</span> <span class="fu">LMDC.select</span>(<span class="st">&quot;y2groups&quot;</span>,<span class="at">data =</span> dat, <span class="at">tol =</span> tol,<span class="at">pvalue =</span> <span class="fl">0.05</span>,</span>
<span id="cb204-9"><a href="variable-selection.html#cb204-9" tabindex="-1"></a><span class="at">plot=</span>F)</span>
<span id="cb204-10"><a href="variable-selection.html#cb204-10" tabindex="-1"></a><span class="co"># Preselected impact points </span></span>
<span id="cb204-11"><a href="variable-selection.html#cb204-11" tabindex="-1"></a>covar<span class="ot">&lt;-</span><span class="fu">names</span>(dat)[<span class="sc">-</span><span class="dv">1</span>][dc.raw[[<span class="st">&quot;maxLocal&quot;</span>]]]</span>
<span id="cb204-12"><a href="variable-selection.html#cb204-12" tabindex="-1"></a>covar</span></code></pre></div>
<pre><code>##  [1] &quot;X945&quot;  &quot;X905&quot;  &quot;X933&quot;  &quot;X886&quot;  &quot;X1020&quot; &quot;X876&quot;  &quot;X1030&quot; &quot;X896&quot;  &quot;X1046&quot;
## [10] &quot;X862&quot;  &quot;X1010&quot; &quot;X915&quot;  &quot;X965&quot;</code></pre>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="variable-selection.html#cb206-1" tabindex="-1"></a><span class="fu">length</span>(covar)</span></code></pre></div>
<pre><code>## [1] 13</code></pre>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="variable-selection.html#cb208-1" tabindex="-1"></a><span class="co"># GLM model (using binomial family), other multivariate model can be used</span></span>
<span id="cb208-2"><a href="variable-selection.html#cb208-2" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">129</span></span>
<span id="cb208-3"><a href="variable-selection.html#cb208-3" tabindex="-1"></a>ldata<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="st">&quot;df&quot;</span><span class="ot">=</span>dat[ind,])</span>
<span id="cb208-4"><a href="variable-selection.html#cb208-4" tabindex="-1"></a>form.glm<span class="ot">&lt;-</span><span class="fu">formula</span>(<span class="fu">paste0</span>(<span class="st">&quot;y2groups~&quot;</span>,<span class="fu">paste0</span>(covar,<span class="at">collapse=</span><span class="st">&quot;+&quot;</span>)))</span>
<span id="cb208-5"><a href="variable-selection.html#cb208-5" tabindex="-1"></a>out.glm  <span class="ot">&lt;-</span> <span class="fu">classif.glm</span>(form.glm, <span class="at">data =</span> ldata)</span>
<span id="cb208-6"><a href="variable-selection.html#cb208-6" tabindex="-1"></a><span class="fu">summary</span>(out.glm)</span></code></pre></div>
<pre><code>##      - SUMMARY - 
## 
## -Probability of correct classification by group (prob.classification):
## 0 1 
## 1 1 
## 
## -Confusion matrix between the theoretical groups (by rows)
##   and estimated groups (by column) 
##    
##      0  1
##   0 58  0
##   1  0 71
## 
## -Probability of correct classification:  1</code></pre>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="variable-selection.html#cb210-1" tabindex="-1"></a><span class="co"># Prediction</span></span>
<span id="cb210-2"><a href="variable-selection.html#cb210-2" tabindex="-1"></a>newldata<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="st">&quot;df&quot;</span><span class="ot">=</span>dat[<span class="sc">-</span>ind,])</span>
<span id="cb210-3"><a href="variable-selection.html#cb210-3" tabindex="-1"></a>pred.glm<span class="ot">&lt;-</span><span class="fu">predict</span>(out.glm,newldata)</span>
<span id="cb210-4"><a href="variable-selection.html#cb210-4" tabindex="-1"></a></span>
<span id="cb210-5"><a href="variable-selection.html#cb210-5" tabindex="-1"></a><span class="co"># Confusion matrix</span></span>
<span id="cb210-6"><a href="variable-selection.html#cb210-6" tabindex="-1"></a><span class="fu">table</span>(newldata<span class="sc">$</span>df<span class="sc">$</span>y2groups,pred.glm)</span></code></pre></div>
<pre><code>##    pred.glm
##      0  1
##   0 40  0
##   1  3 43</code></pre>
</div>
</div>
<div id="variable-selection-in-functional-regression" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Variable selection in functional regression<a href="variable-selection.html#variable-selection-in-functional-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="citation">Febrero-Bande, González-Manteiga, and Oviedo de la Fuente (<a href="#ref-Febrero-Bande2019">2019</a>)</span> consider the problem of variable selection in regression models in the case of functional variables that may be mixed with other type of variables (scalar, multivariate, directional, etc.).</p>
<p>Our proposal begins with a simple null model and sequentially selects a new variable to be incorporated into the model based on the use of distance correlation proposed by <span class="citation">(<a href="#ref-Szekely2007">G. J. Székely, Rizzo, and Bakirov 2007</a>)</span>. For the sake of simplicity, this paper only uses additive models.</p>
<p><span class="math display">\[
Y_i=\alpha+\sum_{j=1}^Jf_j({X_i^{(j)}})+\varepsilon_i,\quad i=1,\ldots,N
\]</span></p>
<p>The proposed algorithm may assess the type of contribution (linear, non linear, …) of each variable. The algorithm has shown quite promising results when applied to simulations and real data sets.</p>
<div id="state-of-art-1" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> State of Art<a href="variable-selection.html#state-of-art-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Stepwise regression, <span class="citation">Akaike (<a href="#ref-Akaike1973">1973</a>)</span>. The main idea is to use some diagnostic tools, directly derived from the linear model, to evaluate the contribution of a new covariate and decide whether it should be included in the model. The final subset is usually constructed using: the forward and/or the backward selection.</p></li>
<li><p>Feature Selection using LASSO. The work by Tibshirani, 1996 proposing the LASSO estimator includes a <span class="math inline">\(l_1\)</span>-type constraint for the coefficient vector <span class="math inline">\(\beta\)</span>. Several examples following the same line but using penalties or constraints such as: LARS (<span class="citation">Efron et al. (<a href="#ref-efron2004least">2004</a>)</span>) and COSSO (<span class="citation">Lin, Zhang, et al. (<a href="#ref-lin2006component">2006</a>)</span>). Each methods is based on a specific model, all the covariates must be included in the model at the same time and for functional data problems, the previous steps that commonly include variable standardization.</p></li>
<li><p><span class="citation">Berrendero, Cuevas, and Torrecilla (<a href="#ref-berrendero2016variable">2016</a>)</span> use the Minimum Redundance Maximum Relevance (mRMR) procedure to choose the most relevant design points in functional classification setting.</p></li>
<li><p>A pure feature selection methods where the covariate is selected without a model. This is the approach employed in minimum Redundancy Maximum Relevance (mRMR), (<span class="citation">Peng, Long, and Ding (<a href="#ref-peng2005feature">2005</a>)</span>) where a new candidate covariate must have a great relevancy with the response while maintaining a lower redundancy with the covariates already selected in the model. he main advantage of this approach is that it is an incremental rule but the measures for redundancy and relevancy must be chosen in function of the regression model applied to ensure good predictive results in the final model. <span class="citation">Berrendero, Cuevas, and Torrecilla (<a href="#ref-berrendero2018">2018</a>)</span> used the Reproducing Kernel Hilbert Space (RKHS) for variable selection in FLM.</p></li>
<li><p>Boosting, see <span class="citation">F. Ferraty and Vieu (<a href="#ref-Ferraty2009">2009</a>)</span> in a functional data context. Boosting selects at each step the best covariate/model with respect to the unexplained part of the response. The final prediction is constructed as a combination of the different steps.</p></li>
<li><p>Partial distance correlation (PDC): used in <span class="citation">Yenigün and Rizzo (<a href="#ref-Yenigun2015">2015</a>)</span> for VS in multivariate linear models, a definition of PDC among <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span> was introduced based on computing the distance correlation among the residuals of two models: <span class="math inline">\(Y\)</span> respect to <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> respect to <span class="math inline">\(Z\)</span>. PDC is constructed under linear relationship assumptions among variables. Its implementation only uses the distance matrices among elements of <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> (variables should have a similar scale).</p></li>
</ul>
<p>Specifically, <span class="math inline">\(Z\)</span> (the variables already in the model) could be a mix of functional, scalar or multivariate variables where an appropriate distance using all of them must be hard to compute. Even restricting ourselves to the scalar case, those variables should have a similar scale.</p>
</div>
<div id="algorithm" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Algorithm<a href="variable-selection.html#algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>All the previous solutions are not completely satisfactory in a functional data framework, specially when the number of possible covariates can be arbitrarily large. We are interested in an automatic regression procedure capable of dealing with a large number of covariates of different nature, possibly very closely related to one another.</p>
<p>The key of the whole procedure is the extensive use of the DC that presents two important advantages: the choice of the variate is made without considering a model and it is possible to compute this quantity for variates of different nature as it is only computed from distances. The distance correlation (DC) is computed among the residuals of the current model with each candidate. Taking into account that the residuals have the same nature as the response variable, the DC can always be computed at each step.</p>
<p>Our proposal is presented is a very general way, we have restricted ourselves to additive models that offer a balanced compromise between predictive ability and simplicity. The obtained results are quite promising in scenarios where no competitors are available because no other procedure can deal with variates of different nature in a homogeneous way.</p>
<p>The procedure was applied to a real problem related with the Iberian Energy Market (Price and Demand) where the number of possible covariates is really big. The algorithm was able to find synthetic regression models offering interesting insights about the relationship among the response and the covariates. The final selected models mix functional, scalar and categorical information.</p>
<p>Our algorithm can be formalized as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(Y\)</span> the response and <span class="math inline">\(S=\{X^1,\ldots,X^p\}\)</span> the set of all possible predictors.</p></li>
<li><p>Set <span class="math inline">\(\hat{Y}=\bar{Y}\)</span>, and let <span class="math inline">\(M^{(0)}=\emptyset\)</span> the initial set of the variates included in the model. Set <span class="math inline">\(i=0\)</span>.</p></li>
<li><p>Compute the residuals of the current model: <span class="math inline">\(\hat{\varepsilon}=Y-\hat{Y}\)</span>.</p></li>
<li><p>Choose <span class="math inline">\(X^j\in S\)</span> such that: 1) <span class="math inline">\(\mathcal{R}\{\hat{\varepsilon},X^j\}\ge \mathcal{R}\{\hat{\varepsilon},X^k\}, \forall k\ne j\in S\)</span> and 2) the null hypothesis for the test of independence among <span class="math inline">\(\left\{X^j\right\}\)</span> and <span class="math inline">\(\hat{\varepsilon}\)</span> is rejected. IF NOT, END.</p></li>
<li><p>Update the sets <span class="math inline">\(M\)</span> and <span class="math inline">\(S\)</span>: <span class="math inline">\(M^{(i+1)}=M^{(i)}\cup\{X^j\}\)</span>, and <span class="math inline">\(S=S\backslash\{X^j\}\)</span>.</p></li>
<li><p>Compute the new model for <span class="math inline">\(Y\)</span> using <span class="math inline">\(M^{(i+1)}\)</span> choosing the best contribution of the new covariate. Typically, there will be a catalog of all possible ways of constructing correct models with the variates in <span class="math inline">\(M^{(i+1)}\)</span> fixing the contributions of the variates in <span class="math inline">\(M^{(i)}\)</span> and adding the new one.</p></li>
<li><p>Analyze the contribution of <span class="math inline">\(X^j\)</span> in the new model respect to the current:</p></li>
</ol>
<ul>
<li><p>IF this contribution is not relevant (typically comparing with the current model)
THEN <span class="math inline">\(M^{(i+1)}=M^{(i+1)}\backslash\{X^j\}\)</span> and the current model remains unalterable</p></li>
<li><p>ELSE the new model becomes the current model and provides new predictions (<span class="math inline">\(\hat{Y}\)</span>). Along the paper we have employed an additive model: <span class="math inline">\(\hat{Y}=\bar{Y}+\sum_{m\in M}\hat{f}_m\left(X^{(m)}\right)\)</span> where at each step <span class="math inline">\(\hat{f}_m\)</span> could be linear or nonlinear.</p></li>
</ul>
<ol start="8" style="list-style-type: decimal">
<li><p>Update the number of iterations: <span class="math inline">\(i=i+1\)</span> and go to 3</p></li>
<li><p>END.</p></li>
</ol>
<p>The current model is the final model with the variates included in <span class="math inline">\(M^{(i)}\)</span>.
<span class="math inline">\(S\)</span> is either the empty set or contains those variables that accept the null hypothesis of the test of independence respect to the residuals of the current model.</p>
</div>
</div>
<div id="binary-classification-example" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Binary classification example<a href="variable-selection.html#binary-classification-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="variable-selection.html#cb212-1" tabindex="-1"></a>Nt<span class="ot">=</span><span class="dv">250</span></span>
<span id="cb212-2"><a href="variable-selection.html#cb212-2" tabindex="-1"></a>Np<span class="ot">=</span><span class="dv">250</span></span>
<span id="cb212-3"><a href="variable-selection.html#cb212-3" tabindex="-1"></a>nB<span class="ot">=</span><span class="dv">100</span></span>
<span id="cb212-4"><a href="variable-selection.html#cb212-4" tabindex="-1"></a>Nvar <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb212-5"><a href="variable-selection.html#cb212-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb212-6"><a href="variable-selection.html#cb212-6" tabindex="-1"></a>Xdat <span class="ot">&lt;-</span> <span class="fu">mariachi</span>(Nt,Np,Nvar)</span>
<span id="cb212-7"><a href="variable-selection.html#cb212-7" tabindex="-1"></a>Xtrain <span class="ot">&lt;-</span> Xdat<span class="sc">$</span>Xtrain</span>
<span id="cb212-8"><a href="variable-selection.html#cb212-8" tabindex="-1"></a>Xtest <span class="ot">&lt;-</span> Xdat<span class="sc">$</span>Xtest</span>
<span id="cb212-9"><a href="variable-selection.html#cb212-9" tabindex="-1"></a>ytrain <span class="ot">&lt;-</span> Xtrain<span class="sc">$</span>grupo</span>
<span id="cb212-10"><a href="variable-selection.html#cb212-10" tabindex="-1"></a>ytest <span class="ot">&lt;-</span> Xtest<span class="sc">$</span>grupo</span>
<span id="cb212-11"><a href="variable-selection.html#cb212-11" tabindex="-1"></a><span class="fu">pairs</span>(Xtrain[,<span class="dv">2</span><span class="sc">:</span><span class="dv">6</span>],<span class="at">col=</span>ytrain)</span></code></pre></div>
<p><img src="bookdown_intro_files/figure-html/unnamed-chunk-80-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="variable-selection.html#cb213-1" tabindex="-1"></a>mdat <span class="ot">&lt;-</span> Xtrain[,<span class="dv">1</span><span class="sc">:</span>Nvar]</span>
<span id="cb213-2"><a href="variable-selection.html#cb213-2" tabindex="-1"></a>newmdat <span class="ot">&lt;-</span> Xtest[,<span class="dv">1</span><span class="sc">:</span>Nvar]</span>
<span id="cb213-3"><a href="variable-selection.html#cb213-3" tabindex="-1"></a><span class="fu">names</span>(mdat)</span></code></pre></div>
<pre><code>##  [1] &quot;grupo&quot; &quot;X1&quot;    &quot;X2&quot;    &quot;Z1&quot;    &quot;Z2&quot;    &quot;Z3&quot;    &quot;Z4&quot;    &quot;Z5&quot;    &quot;Z6&quot;   
## [10] &quot;Z7&quot;    &quot;Z8&quot;    &quot;Z9&quot;    &quot;Z10&quot;   &quot;Z11&quot;   &quot;Z12&quot;   &quot;Z13&quot;   &quot;Z14&quot;   &quot;Z15&quot;  
## [19] &quot;Z16&quot;   &quot;Z17&quot;   &quot;Z18&quot;   &quot;Z19&quot;   &quot;Z20&quot;   &quot;Z21&quot;   &quot;Z22&quot;   &quot;Z23&quot;   &quot;Z24&quot;  
## [28] &quot;Z25&quot;   &quot;Z26&quot;   &quot;Z27&quot;   &quot;Z28&quot;   &quot;Z29&quot;   &quot;Z30&quot;   &quot;Z31&quot;   &quot;Z32&quot;   &quot;Z33&quot;  
## [37] &quot;Z34&quot;   &quot;Z35&quot;   &quot;Z36&quot;   &quot;Z37&quot;   &quot;Z38&quot;   &quot;Z39&quot;   &quot;Z40&quot;   &quot;Z41&quot;   &quot;Z42&quot;  
## [46] &quot;Z43&quot;   &quot;Z44&quot;   &quot;Z45&quot;   &quot;Z46&quot;   &quot;Z47&quot;</code></pre>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="variable-selection.html#cb215-1" tabindex="-1"></a>meas <span class="ot">&lt;-</span> <span class="st">&quot;accuracy&quot;</span></span>
<span id="cb215-2"><a href="variable-selection.html#cb215-2" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb215-3"><a href="variable-selection.html#cb215-3" tabindex="-1"></a>rf1 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(grupo<span class="sc">~</span>.,<span class="at">data=</span>mdat)</span>
<span id="cb215-4"><a href="variable-selection.html#cb215-4" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf1)</span></code></pre></div>
<p><img src="bookdown_intro_files/figure-html/unnamed-chunk-82-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="variable-selection.html#cb216-1" tabindex="-1"></a>pred.rf1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf1,newmdat,<span class="at">measure=</span>meas)</span>
<span id="cb216-2"><a href="variable-selection.html#cb216-2" tabindex="-1"></a><span class="fu">cat2meas</span>(ytest,pred.rf1)</span></code></pre></div>
<pre><code>## accuracy 
##    0.672</code></pre>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="variable-selection.html#cb218-1" tabindex="-1"></a><span class="fu">table</span>(ytest,pred.rf1)</span></code></pre></div>
<pre><code>##      pred.rf1
## ytest  1  2
##     1 95 14
##     2 68 73</code></pre>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="variable-selection.html#cb220-1" tabindex="-1"></a>rf.vs <span class="ot">&lt;-</span> fda.usc.devel<span class="sc">:::</span><span class="fu">classif.ML.vs</span>(<span class="fu">ldata</span>(mdat),<span class="st">&quot;grupo&quot;</span>,<span class="at">classif=</span><span class="st">&quot;classif.randomForest&quot;</span>)</span>
<span id="cb220-2"><a href="variable-selection.html#cb220-2" tabindex="-1"></a><span class="fu">summary</span>(rf.vs)</span></code></pre></div>
<pre><code>##      - SUMMARY - 
## 
## -Probability of correct classification by group (prob.classification):
## [1] 0.9185185 0.8869565
## 
## -Confusion matrix between the theoretical groups (by rows)
##   and estimated groups (by column) 
##    
##       1   2
##   1 124  11
##   2  13 102
## 
## -Probability of correct classification:  0.904</code></pre>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="variable-selection.html#cb222-1" tabindex="-1"></a>rf.vs<span class="sc">$</span>i.predictor</span></code></pre></div>
<pre><code>##  X1  X2  Z1  Z2  Z3  Z4  Z5  Z6  Z7  Z8  Z9 Z10 Z11 Z12 Z13 Z14 Z15 Z16 Z17 Z18 
##   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## Z19 Z20 Z21 Z22 Z23 Z24 Z25 Z26 Z27 Z28 Z29 Z30 Z31 Z32 Z33 Z34 Z35 Z36 Z37 Z38 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## Z39 Z40 Z41 Z42 Z43 Z44 Z45 Z46 Z47 
##   0   0   0   0   0   0   0   0   0</code></pre>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="variable-selection.html#cb224-1" tabindex="-1"></a>rf.vs<span class="sc">$</span>ipredictor</span></code></pre></div>
<pre><code>## [1] &quot;X2&quot; &quot;X1&quot;</code></pre>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="variable-selection.html#cb226-1" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">round</span>(rf.vs<span class="sc">$</span>dcor,<span class="dv">3</span>))</span></code></pre></div>
<pre><code>##      X1    X2 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10  Z11 Z12 Z13   Z14 Z15 Z16 Z17
## 1 0.016 0.018  0  0  0  0  0  0  0  0  0   0 0.01   0   0 0.010   0   0   0
## 2 0.032 0.000  0  0  0  0  0  0  0  0  0   0 0.00   0   0 0.000   0   0   0
## 3 0.000 0.000  0  0  0  0  0  0  0  0  0   0 0.00   0   0 0.011   0   0   0
## 4 0.000 0.000  0  0  0  0  0  0  0  0  0   0 0.00   0   0 0.011   0   0   0
## 5 0.000 0.000  0  0  0  0  0  0  0  0  0   0 0.00   0   0 0.000   0   0   0
## 6 0.000 0.000  0  0  0  0  0  0  0  0  0   0 0.00   0   0 0.000   0   0   0
##     Z18 Z19 Z20 Z21   Z22 Z23 Z24 Z25 Z26 Z27 Z28 Z29 Z30  Z31 Z32 Z33 Z34 Z35
## 1 0.015   0   0   0 0.013   0   0   0   0   0   0   0   0 0.00   0   0   0   0
## 2 0.000   0   0   0 0.000   0   0   0   0   0   0   0   0 0.01   0   0   0   0
## 3 0.000   0   0   0 0.000   0   0   0   0   0   0   0   0 0.00   0   0   0   0
## 4 0.000   0   0   0 0.000   0   0   0   0   0   0   0   0 0.00   0   0   0   0
## 5 0.000   0   0   0 0.000   0   0   0   0   0   0   0   0 0.00   0   0   0   0
## 6 0.000   0   0   0 0.000   0   0   0   0   0   0   0   0 0.00   0   0   0   0
##   Z36   Z37 Z38 Z39 Z40 Z41 Z42 Z43 Z44 Z45 Z46 Z47
## 1   0 0.000   0   0   0   0   0   0   0   0   0   0
## 2   0 0.000   0   0   0   0   0   0   0   0   0   0
## 3   0 0.016   0   0   0   0   0   0   0   0   0   0
## 4   0 0.000   0   0   0   0   0   0   0   0   0   0
## 5   0 0.000   0   0   0   0   0   0   0   0   0   0
## 6   0 0.000   0   0   0   0   0   0   0   0   0   0</code></pre>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="variable-selection.html#cb228-1" tabindex="-1"></a>pred.rf.vs <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.vs,<span class="fu">ldata</span>(newmdat))</span>
<span id="cb228-2"><a href="variable-selection.html#cb228-2" tabindex="-1"></a><span class="fu">cat2meas</span>(ytest,pred.rf.vs)</span></code></pre></div>
<pre><code>## accuracy 
##    0.896</code></pre>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="variable-selection.html#cb230-1" tabindex="-1"></a><span class="fu">table</span>(ytest,pred.rf.vs)</span></code></pre></div>
<pre><code>##      pred.rf.vs
## ytest   1   2
##     1 103   6
##     2  20 121</code></pre>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="variable-selection.html#cb232-1" tabindex="-1"></a>predictors <span class="ot">&lt;-</span> <span class="fu">names</span>(mdat)[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb232-2"><a href="variable-selection.html#cb232-2" tabindex="-1"></a>form <span class="ot">&lt;-</span> <span class="fu">formula</span>(<span class="fu">paste0</span>(<span class="st">&quot;grupo~&quot;</span>,<span class="fu">paste0</span>(<span class="st">&quot;s(&quot;</span>,predictors,<span class="st">&quot;)&quot;</span>,<span class="at">collapse=</span><span class="st">&quot;+&quot;</span>)))</span>
<span id="cb232-3"><a href="variable-selection.html#cb232-3" tabindex="-1"></a>form</span></code></pre></div>
<pre><code>## grupo ~ s(X1) + s(X2) + s(Z1) + s(Z2) + s(Z3) + s(Z4) + s(Z5) + 
##     s(Z6) + s(Z7) + s(Z8) + s(Z9) + s(Z10) + s(Z11) + s(Z12) + 
##     s(Z13) + s(Z14) + s(Z15) + s(Z16) + s(Z17) + s(Z18) + s(Z19) + 
##     s(Z20) + s(Z21) + s(Z22) + s(Z23) + s(Z24) + s(Z25) + s(Z26) + 
##     s(Z27) + s(Z28) + s(Z29) + s(Z30) + s(Z31) + s(Z32) + s(Z33) + 
##     s(Z34) + s(Z35) + s(Z36) + s(Z37) + s(Z38) + s(Z39) + s(Z40) + 
##     s(Z41) + s(Z42) + s(Z43) + s(Z44) + s(Z45) + s(Z46) + s(Z47)</code></pre>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="variable-selection.html#cb234-1" tabindex="-1"></a>ldat <span class="ot">&lt;-</span> <span class="fu">ldata</span>(mdat)</span>
<span id="cb234-2"><a href="variable-selection.html#cb234-2" tabindex="-1"></a><span class="co"># gsam &lt;- classif.gsam(form,data=ldat)</span></span>
<span id="cb234-3"><a href="variable-selection.html#cb234-3" tabindex="-1"></a><span class="co"># Error in gam(formula = as.formula(pf), data = XX, family = family) : Model has more coefficients than data</span></span>
<span id="cb234-4"><a href="variable-selection.html#cb234-4" tabindex="-1"></a></span>
<span id="cb234-5"><a href="variable-selection.html#cb234-5" tabindex="-1"></a>form <span class="ot">&lt;-</span> <span class="fu">formula</span>(<span class="fu">paste0</span>(<span class="st">&quot;grupo~&quot;</span>,<span class="fu">paste0</span>(<span class="st">&quot;s(&quot;</span>,predictors,<span class="st">&quot;,k=5)&quot;</span>,<span class="at">collapse=</span><span class="st">&quot;+&quot;</span>)))</span>
<span id="cb234-6"><a href="variable-selection.html#cb234-6" tabindex="-1"></a>ldat <span class="ot">&lt;-</span> <span class="fu">ldata</span>(mdat)</span>
<span id="cb234-7"><a href="variable-selection.html#cb234-7" tabindex="-1"></a>gsam <span class="ot">&lt;-</span> <span class="fu">classif.gsam</span>(form,<span class="at">data=</span>ldat)</span>
<span id="cb234-8"><a href="variable-selection.html#cb234-8" tabindex="-1"></a>pred.gsam <span class="ot">&lt;-</span> <span class="fu">predict</span>(gsam,<span class="fu">ldata</span>(newmdat))</span>
<span id="cb234-9"><a href="variable-selection.html#cb234-9" tabindex="-1"></a><span class="fu">cat2meas</span>(ytest,pred.gsam)</span></code></pre></div>
<pre><code>## accuracy 
##    0.896</code></pre>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="variable-selection.html#cb236-1" tabindex="-1"></a><span class="fu">table</span>(ytest,pred.gsam)</span></code></pre></div>
<pre><code>##      pred.gsam
## ytest   1   2
##     1  97  12
##     2  14 127</code></pre>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="variable-selection.html#cb238-1" tabindex="-1"></a>gsam.vs <span class="ot">&lt;-</span> <span class="fu">classif.gsam.vs</span>(<span class="fu">ldata</span>(mdat),<span class="st">&quot;grupo&quot;</span>)</span>
<span id="cb238-2"><a href="variable-selection.html#cb238-2" tabindex="-1"></a><span class="fu">summary</span>(gsam.vs)</span></code></pre></div>
<pre><code>##      - SUMMARY - 
## 
## -Probability of correct classification by group (prob.classification):
## 1 2 
## 1 1 
## 
## -Confusion matrix between the theoretical groups (by rows)
##   and estimated groups (by column) 
##    
##       1   2
##   1 135   0
##   2   0 115
## 
## -Probability of correct classification:  1</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="variable-selection.html#cb240-1" tabindex="-1"></a>gsam.vs<span class="sc">$</span>i.predictor</span></code></pre></div>
<pre><code>##  X1  X2  Z1  Z2  Z3  Z4  Z5  Z6  Z7  Z8  Z9 Z10 Z11 Z12 Z13 Z14 Z15 Z16 Z17 Z18 
##   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## Z19 Z20 Z21 Z22 Z23 Z24 Z25 Z26 Z27 Z28 Z29 Z30 Z31 Z32 Z33 Z34 Z35 Z36 Z37 Z38 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## Z39 Z40 Z41 Z42 Z43 Z44 Z45 Z46 Z47 
##   0   0   0   0   0   0   0   0   0</code></pre>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="variable-selection.html#cb242-1" tabindex="-1"></a>gsam.vs<span class="sc">$</span>ipredictor</span></code></pre></div>
<pre><code>## [1] &quot;X2&quot; &quot;X1&quot;</code></pre>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="variable-selection.html#cb244-1" tabindex="-1"></a>pred.gsam.vs <span class="ot">&lt;-</span> <span class="fu">predict</span>(gsam.vs,<span class="fu">ldata</span>(newmdat))</span>
<span id="cb244-2"><a href="variable-selection.html#cb244-2" tabindex="-1"></a><span class="fu">cat2meas</span>(ytest,pred.gsam.vs)</span></code></pre></div>
<pre><code>## accuracy 
##    0.984</code></pre>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="variable-selection.html#cb246-1" tabindex="-1"></a><span class="fu">table</span>(ytest,pred.gsam.vs)</span></code></pre></div>
<pre><code>##      pred.gsam.vs
## ytest   1   2
##     1 106   3
##     2   1 140</code></pre>
</div>
<div id="hyperspectral-images-example" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Hyperspectral images example<a href="variable-selection.html#hyperspectral-images-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>During the last years, the use of hyperspectral sensors has been extended to a great variety of applications such as discrimination among different land cover classes in remote sensing images.</p>
<p><img src="img/hyper02.png" width="86%" style="display: block; margin: auto;" /></p>
<div id="hyperspectral-data-set-pavia-university" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Hyperspectral Data set: Pavia University<a href="variable-selection.html#hyperspectral-data-set-pavia-university" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This is a remote sensing image obtained by the 103-band ROSIS sensor from the University of Pavia (Pavia Univ.), with a spatial dimension of 610 × 340 pixels</p>
<p><img src="img/pavia02.png" width="88%" style="display: block; margin: auto;" /></p>
<p><img src="img/pavia03.png" width="66%" style="display: block; margin: auto;" /></p>
</div>
<div id="hyperspectral-images" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Hyperspectral images<a href="variable-selection.html#hyperspectral-images" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the University of Pavia image, 300 pixels for each class were chosen for training, and the rest were used as a test set.</p>
<blockquote>
<pre><code>1     2     3     4     5     6     7     8     9 </code></pre>
</blockquote>
<blockquote>
<p>6631 18649 2099 3064 1345 5029 1330 3682 947</p>
</blockquote>
<blockquote>
<p>[1] “Training”</p>
<p>1 2 3 4 5 6 7 8 9</p>
</blockquote>
<blockquote>
<p>300 300 300 300 300 300 300 300 300</p>
</blockquote>
<blockquote>
<p>[1] “Test”</p>
</blockquote>
<blockquote>
<p>yy</p>
</blockquote>
<blockquote>
<p>1 2 3 4 5 6 7 8 9</p>
</blockquote>
<blockquote>
<p>797 2308 244 324 119 589 117 424 78</p>
</blockquote>
<p>names&lt;-c(“Asphalt”,“Meadows”,“Gravel”,“Trees”,“Metal sheets”,“Bare Soil”,“Bitumen”,“Bricks”,“Shadows”)</p>
<pre><code>





``` r
# scalar covariates
names(ltest$df);
[1] &quot;y&quot;     &quot;icol&quot;  &quot;irow&quot;  &quot;z0&quot;    &quot;z0.1&quot;  &quot;z0.2&quot;  &quot;z0.29&quot; &quot;z0.39&quot; &quot;z0.49&quot; &quot;z0.59&quot; &quot;z0.69&quot; &quot;z0.78&quot;
[13] &quot;z0.88&quot; &quot;z0.98&quot;
# functional covariates 
names(ltest)[-1]
[1] &quot;x&quot;    &quot;x.d1&quot;
gsam1 &lt;- classif.gsam(y ~ icol+irow ,data=ltrain2)
There were 18 warnings (use warnings() to see them)
summary(gsam1)
     - SUMMARY - 

-Probability of correct classification by group (prob.classification):
   1    2    3    4    5    6    7    8    9 
0.00 0.47 0.62 0.00 0.99 0.92 0.72 0.34 0.01 

-Confusion matrix between the theoretical groups (by rows)
  and estimated groups (by column) 
   
     1  2  3  4  5  6  7  8  9
  1  0 12 11  0 23 43  0  9  2
  2  0 47 11  0 23 19  0  0  0
  3  0  0 62  0  0  0  0 38  0
  4  0 19 14  0 29 27  0  9  2
  5  1  0  0  0 99  0  0  0  0
  6  2  0  0  6  0 92  0  0  0
  7 11  0  0 12  0  0 72  0  5
  8  0 20 21  0 15  0  4 34  6
  9  2  7 38 14 14 11  7  6  1

-Probability of correct classification:  0.4522 

pred.gsam1 &lt;- predict(gsam1,ltest)
cat2meas(ytest,pred.gsam1)
[1] 0.439
table(ytest,pred.gsam1)
     pred.gsam1
ytest   1   2   3   4   5   6   7   8   9
    1   0  61  52   0 121 174   0  22  16
    2   0 639 186   0 321 168  16   2   0
    3   0   0  92   0   0   0   0  51   0
    4   0  38  17   0  74  53   3  21   8
    5   1   0   0   0 103   0   0   0   0
    6   5   0   0  37   0 325   0   0   0
    7  12   0   0  16   0   0  68   0   1
    8   0  49  39   0  24   0   9  89  14
    9   6   5  11   6  11  10  14   9   1

gsam1 &lt;- classif.gsam(y ~ s(icol) + s(irow) ,data=ltrain2)
summary(gsam1)
     - SUMMARY - 

-Probability of correct classification by group (prob.classification):
   1    2    3    4    5    6    7    8    9 
0.00 0.47 0.62 0.00 0.99 0.92 0.72 0.34 0.01 

-Confusion matrix between the theoretical groups (by rows)
  and estimated groups (by column) 
   
     1  2  3  4  5  6  7  8  9
  1  0 12 11  0 23 43  0  9  2
  2  0 47 11  0 23 19  0  0  0
  3  0  0 62  0  0  0  0 38  0
  4  0 19 14  0 29 27  0  9  2
  5  1  0  0  0 99  0  0  0  0
  6  2  0  0  6  0 92  0  0  0
  7 11  0  0 12  0  0 72  0  5
  8  0 20 21  0 15  0  4 34  6
  9  2  7 38 14 14 11  7  6  1

-Probability of correct classification:  0.4522 

pred.gsam1 &lt;- predict(gsam1,ltest)
cat2meas(ytest,pred.gsam1)
[1] 0.439
table(ytest,pred.gsam1)
     pred.gsam1
ytest   1   2   3   4   5   6   7   8   9
    1   0  61  52   0 121 174   0  22  16
    2   0 639 186   0 321 168  16   2   0
    3   0   0  92   0   0   0   0  51   0
    4   0  38  17   0  74  53   3  21   8
    5   1   0   0   0 103   0   0   0   0
    6   5   0   0  37   0 325   0   0   0
    7  12   0   0  16   0   0  68   0   1
    8   0  49  39   0  24   0   9  89  14
    9   6   5  11   6  11  10  14   9   1
 
gsam1 &lt;- classif.gsam(y ~ s(x) ,data=ltrain2)
summary(gsam1)
     - SUMMARY - 

-Probability of correct classification by group (prob.classification):
   1    2    3    4    5    6    7    8    9 
0.62 0.78 0.64 0.97 1.00 0.25 0.82 0.71 1.00 

-Confusion matrix between the theoretical groups (by rows)
  and estimated groups (by column) 
   
      1   2   3   4   5   6   7   8   9
  1  62   0   4   0   0   2  27   5   0
  2   0  78   3  13   0   6   0   0   0
  3   2   1  64   0   0   0   7  26   0
  4   0   2   0  97   0   1   0   0   0
  5   0   0   0   0 100   0   0   0   0
  6   0  56   1   2   0  25   0  16   0
  7  15   0   3   0   0   0  82   0   0
  8   1   1  12   0   0   6   9  71   0
  9   0   0   0   0   0   0   0   0 100

-Probability of correct classification:  0.7544 

pred.gsam1 &lt;- predict(gsam1,ltest)
cat2meas(ytest,pred.gsam1)
[1] 0.7016667
table(ytest,pred.gsam1)
     pred.gsam1
ytest    1    2    3    4    5    6    7    8    9
    1  281    2   36    0    2    3  111   11    0
    2    0 1001   27  186    0  116    0    2    0
    3    4    1  103    0    0    0    7   28    0
    4    0    7    0  207    0    0    0    0    0
    5    0    0    0    0  104    0    0    0    0
    6    1  215   11    5    0   90    0   45    0
    7    8    1    1    0    0    0   87    0    0
    8    2    3   32    0    0   11   17  159    0
    9    0    0    0    0    0    0    0    0   73

gsam1 &lt;- classif.gsam(y ~ s(icol) + s(irow) + s(x)  ,data=ltrain2)
summary(gsam1)
     - SUMMARY - 

-Probability of correct classification by group (prob.classification):
   1    2    3    4    5    6    7    8    9 
0.49 0.80 0.75 0.94 1.00 0.78 0.84 0.74 1.00 

-Confusion matrix between the theoretical groups (by rows)
  and estimated groups (by column) 
   
      1   2   3   4   5   6   7   8   9
  1  49   0  10   0   0   7  30   4   0
  2   0  80   0  10   0  10   0   0   0
  3   0   1  75   0   0   0   0  24   0
  4   0   4   0  94   0   2   0   0   0
  5   0   0   0   0 100   0   0   0   0
  6   1  16   0   2   0  78   0   3   0
  7  16   0   0   0   0   0  84   0   0
  8   0   2  20   0   0   0   4  74   0
  9   0   0   0   0   0   0   0   0 100

-Probability of correct classification:  0.8156 

pred.gsam1 &lt;- predict(gsam1,ltest)
cat2meas(ytest,pred.gsam1)
[1] 0.7766667
table(ytest,pred.gsam1)
     pred.gsam1
ytest    1    2    3    4    5    6    7    8    9
    1  236    1   46    0    2   43  115    3    0
    2    0 1026    1  192    0  112    0    1    0
    3    0    1  121    0    0    0    1   20    0
    4    0    6    0  207    0    1    0    0    0
    5    0    0    0    0  104    0    0    0    0
    6    2   44    0    5    0  308    0    8    0
    7    6    1    0    0    0    0   90    0    0
    8    4    3   36    0    0    0   16  165    0
    9    0    0    0    0    0    0    0    0   73

gsam.vs &lt;- classif.gsam.vs(ltrain2,&quot;y&quot;)

gsam.vs$i.predictor
 icol  irow    z0  z0.1  z0.2 z0.29 z0.39 z0.49 z0.59 z0.69 z0.78 z0.88 z0.98     x  x.d1 
    1     1     0     0     0     0     0     0     0     1     0     0     0     1     1 
gsam.vs$ipredictor
[1] &quot;x&quot;     &quot;z0.69&quot; &quot;icol&quot;  &quot;x.d1&quot; &quot;irow&quot;
round(gsam.vs$dcor,2)
   icol irow   z0 z0.1 z0.2 z0.29 z0.39 z0.49 z0.59 z0.69 z0.78 z0.88 z0.98    x x.d1
1  0.20 0.18 0.33 0.43 0.44  0.44  0.42  0.37  0.36  0.38  0.42  0.43  0.43 0.58 0.43
2  0.07 0.06 0.04 0.06 0.07  0.07  0.07  0.06  0.06  0.08  0.06  0.06  0.06 0.00 0.05
3  0.07 0.06 0.04 0.06 0.06  0.07  0.06  0.06  0.06  0.00  0.05  0.05  0.06 0.00 0.05
4  0.00 0.03 0.02 0.04 0.04  0.04  0.04  0.03  0.03  0.00  0.04  0.04  0.04 0.00 0.04
5  0.00 0.03 0.00 0.00 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 0.00 0.00
6  0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 0.00 0.00

summary(gsam.vs)
     - SUMMARY - 

-Probability of correct classification by group (prob.classification):
   1    2    3    4    5    6    7    8    9 
0.72 0.84 0.82 0.94 1.00 0.86 0.88 0.73 1.00 

-Confusion matrix between the theoretical groups (by rows)
  and estimated groups (by column) 
   
      1   2   3   4   5   6   7   8   9
  1  72   0   5   0   0   3  15   5   0
  2   0  84   0   7   0   8   0   1   0
  3   0   1  82   0   0   0   1  16   0
  4   0   4   0  94   0   2   0   0   0
  5   0   0   0   0 100   0   0   0   0
  6   0   9   0   1   0  86   0   4   0
  7  12   0   0   0   0   0  88   0   0
  8   2   0  23   0   0   1   1  73   0
  9   0   0   0   0   0   0   0   0 100

-Probability of correct classification:  0.8656 

pred.gsam.vs &lt;- predict(gsam.vs,ltest)
cat2meas(ytest,pred.gsam.vs)
[1] 0.8106667
table(ytest,pred.gsam.vs)
     pred.gsam.vs
ytest    1    2    3    4    5    6    7    8    9
    1  290    1   38    0    0   31   79    7    0
    2    3 1096    1  105    0  118    0    9    0
    3    0    1  117    0    0    0    2   23    0
    4    0    9    0  201    0    4    0    0    0
    5    0    0    0    0  104    0    0    0    0
    6    1   47    0    2    0  307    0   10    0
    7   16    0    0    0    0    0   80    1    0
    8    8    3   43    0    0    1    5  164    0
    9    0    0    0    0    0    0    0    0   73</code></pre>
</div>
<div id="pravia-univ.-results" class="section level3 hasAnchor" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Pravia Univ. Results<a href="variable-selection.html#pravia-univ.-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<caption>Accuracy average over 100 runs.</caption>
<thead>
<tr class="header">
<th align="right">Method</th>
<th align="center"><span class="math inline">\(x\)</span>-pos., <span class="math inline">\(y\)</span>-pos.</th>
<th align="center"><span class="math inline">\(X(t)\)</span></th>
<th align="left"><span class="math inline">\(X(t)\)</span>, <span class="math inline">\(x\)</span>-pos., <span class="math inline">\(y\)</span>-pos.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">(F)GLM<span class="math inline">\(+\)</span>MaxProb</td>
<td align="center">0.4626</td>
<td align="center">0.5898</td>
<td align="left">0.7802</td>
</tr>
<tr class="even">
<td align="right">(F)GLM<span class="math inline">\(+\)</span>MajVot</td>
<td align="center">0.4600</td>
<td align="center">0.6926</td>
<td align="left">0.9022</td>
</tr>
<tr class="odd">
<td align="right">(F)GAM<span class="math inline">\(+\)</span>MaxProb</td>
<td align="center">0.7070</td>
<td align="center">0.7384</td>
<td align="left">0.9768</td>
</tr>
<tr class="even">
<td align="right">(F)GAM <span class="math inline">\(+\)</span>MajVot</td>
<td align="center">0.7662</td>
<td align="center">0.7824</td>
<td align="left">0.9846</td>
</tr>
</tbody>
</table>
</div>
<div id="functional-data-example" class="section level3 hasAnchor" number="4.4.4">
<h3><span class="header-section-number">4.4.4</span> Functional data example<a href="variable-selection.html#functional-data-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="variable-selection.html#cb250-1" tabindex="-1"></a><span class="fu">data</span>(tecator)</span>
<span id="cb250-2"><a href="variable-selection.html#cb250-2" tabindex="-1"></a>y<span class="ot">=</span>tecator<span class="sc">$</span>y<span class="sc">$</span>Fat</span>
<span id="cb250-3"><a href="variable-selection.html#cb250-3" tabindex="-1"></a></span>
<span id="cb250-4"><a href="variable-selection.html#cb250-4" tabindex="-1"></a><span class="co"># Potential functional covariates</span></span>
<span id="cb250-5"><a href="variable-selection.html#cb250-5" tabindex="-1"></a>x<span class="ot">=</span>tecator<span class="sc">$</span>absorp.fdata</span>
<span id="cb250-6"><a href="variable-selection.html#cb250-6" tabindex="-1"></a>x1<span class="ot">&lt;-</span><span class="fu">fdata.deriv</span>(x)</span>
<span id="cb250-7"><a href="variable-selection.html#cb250-7" tabindex="-1"></a>x2<span class="ot">&lt;-</span><span class="fu">fdata.deriv</span>(x,<span class="at">nderiv=</span><span class="dv">2</span>)</span>
<span id="cb250-8"><a href="variable-selection.html#cb250-8" tabindex="-1"></a></span>
<span id="cb250-9"><a href="variable-selection.html#cb250-9" tabindex="-1"></a><span class="co"># Potential factor covariates</span></span>
<span id="cb250-10"><a href="variable-selection.html#cb250-10" tabindex="-1"></a>xcat0<span class="ot">&lt;-</span><span class="fu">cut</span>(<span class="fu">rnorm</span>(<span class="fu">length</span>(y)),<span class="dv">4</span>)</span>
<span id="cb250-11"><a href="variable-selection.html#cb250-11" tabindex="-1"></a>xcat1<span class="ot">&lt;-</span><span class="fu">cut</span>(tecator<span class="sc">$</span>y<span class="sc">$</span>Protein,<span class="dv">4</span>)</span>
<span id="cb250-12"><a href="variable-selection.html#cb250-12" tabindex="-1"></a>xcat2<span class="ot">&lt;-</span><span class="fu">cut</span>(tecator<span class="sc">$</span>y<span class="sc">$</span>Water,<span class="dv">4</span>)</span>
<span id="cb250-13"><a href="variable-selection.html#cb250-13" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">129</span></span>
<span id="cb250-14"><a href="variable-selection.html#cb250-14" tabindex="-1"></a></span>
<span id="cb250-15"><a href="variable-selection.html#cb250-15" tabindex="-1"></a><span class="co"># 3 functionals (x,x1,x2), 3 factors (xcat0, xcat1, xcat2)</span></span>
<span id="cb250-16"><a href="variable-selection.html#cb250-16" tabindex="-1"></a><span class="co"># and 100 potential scalars covariates (impact poitns of x1) </span></span>
<span id="cb250-17"><a href="variable-selection.html#cb250-17" tabindex="-1"></a>dat    <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;Fat&quot;</span><span class="ot">=</span>y, x1<span class="sc">$</span>data, xcat1, xcat2)</span>
<span id="cb250-18"><a href="variable-selection.html#cb250-18" tabindex="-1"></a>ldat <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="st">&quot;df&quot;</span><span class="ot">=</span>dat[ind,],<span class="st">&quot;x&quot;</span><span class="ot">=</span>x[ind,],<span class="st">&quot;x1&quot;</span><span class="ot">=</span>x1[ind,],<span class="st">&quot;x2&quot;</span><span class="ot">=</span>x2[ind,])</span>
<span id="cb250-19"><a href="variable-selection.html#cb250-19" tabindex="-1"></a></span>
<span id="cb250-20"><a href="variable-selection.html#cb250-20" tabindex="-1"></a><span class="co"># Time consuming</span></span>
<span id="cb250-21"><a href="variable-selection.html#cb250-21" tabindex="-1"></a>res.gam1<span class="ot">&lt;-</span><span class="fu">fregre.gsam.vs</span>(<span class="at">data=</span>ldat,<span class="at">y=</span><span class="st">&quot;Fat&quot;</span>)</span>
<span id="cb250-22"><a href="variable-selection.html#cb250-22" tabindex="-1"></a><span class="fu">summary</span>(res.gam1<span class="sc">$</span>model)</span></code></pre></div>
<pre><code>##       Fat            x2.PC1               x2.PC2               x2.PC3          
##  Min.   : 0.90   Min.   :-0.0033724   Min.   :-3.094e-03   Min.   :-0.0032994  
##  1st Qu.: 7.70   1st Qu.:-0.0023729   1st Qu.:-7.134e-04   1st Qu.:-0.0001902  
##  Median :14.60   Median :-0.0009539   Median :-2.468e-05   Median : 0.0001020  
##  Mean   :18.24   Mean   : 0.0000000   Mean   : 0.000e+00   Mean   : 0.0000000  
##  3rd Qu.:27.80   3rd Qu.: 0.0017247   3rd Qu.: 6.696e-04   3rd Qu.: 0.0002731  
##  Max.   :49.10   Max.   : 0.0095969   Max.   : 6.243e-03   Max.   : 0.0034894  
##      x2.PC4          
##  Min.   :-1.076e-03  
##  1st Qu.:-1.076e-04  
##  Median : 2.944e-05  
##  Mean   : 0.000e+00  
##  3rd Qu.: 1.294e-04  
##  Max.   : 1.125e-03</code></pre>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="variable-selection.html#cb252-1" tabindex="-1"></a><span class="co"># Prediction like fregre.gsam() </span></span>
<span id="cb252-2"><a href="variable-selection.html#cb252-2" tabindex="-1"></a>newldat <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="st">&quot;df&quot;</span><span class="ot">=</span>dat[<span class="sc">-</span>ind,],<span class="st">&quot;x&quot;</span><span class="ot">=</span>x[<span class="sc">-</span>ind,],<span class="st">&quot;x1&quot;</span><span class="ot">=</span>x1[<span class="sc">-</span>ind,],<span class="st">&quot;x2&quot;</span><span class="ot">=</span>x2[<span class="sc">-</span>ind,])</span>
<span id="cb252-3"><a href="variable-selection.html#cb252-3" tabindex="-1"></a>pred.gam1<span class="ot">&lt;-</span><span class="fu">predict</span>(res.gam1,newldat)</span>
<span id="cb252-4"><a href="variable-selection.html#cb252-4" tabindex="-1"></a><span class="fu">plot</span>(dat[<span class="sc">-</span>ind,<span class="st">&quot;Fat&quot;</span>],pred.gam1)</span></code></pre></div>
<p><img src="bookdown_intro_files/figure-html/unnamed-chunk-92-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="optimum-multiscale-selection-in-3d-point-cloud-classification-oviedo2021distance" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Optimum Multiscale Selection in 3D Point Cloud Classification <span class="citation">(<a href="#ref-oviedo2021distance">Oviedo-de la Fuente et al. 2021</a>)</span><a href="variable-selection.html#optimum-multiscale-selection-in-3d-point-cloud-classification-oviedo2021distance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Supervised classification of 3D point clouds using machine learning algorithms and handcrafted local features as covariates frequently depends on the size of the neighborhood (scale) around each point used to determine those features. It is therefore crucial to estimate the scale or scales providing the best classification results. In this work, we propose three methods to estimate said scales, all of them based on calculating the maximum values of the distance correlation (DC) functions between the features and the label assigned to each point. The performance of the methods was tested using simulated data, and the method presenting the best results was applied to a benchmark data set for point cloud classification. This method consists of detecting the local maximums of DC functions previously smoothed to avoid choosing scales that are very close to each other. Five different classifiers were used: linear discriminant analysis, support vector machines, random forest, multinomial logistic regression and multilayer perceptron neural network. The results obtained were compared with those from other strategies available in the literature, being favorable to our approach.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-94"></span>
<img src="img/workflow.jpg" alt="Urban scene" width="106%" />
<p class="caption">
Figure 4.1: Urban scene
</p>
</div>
<p>Oviedo-de la Fuente, M.; Cabo, C.; Ordóñez, C.; Roca-Pardiñas, J. A Distance Correlation Approach for Optimum Multiscale Selection in 3D Point Cloud Classification. Mathematics 2021, 9, 1328. <a href="https://doi.org/10.3390/math9121328" class="uri">https://doi.org/10.3390/math9121328</a></p>
<ul>
<li>Working paper</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-95"></span>
<img src="img/F3.jpg" alt="Forest scene" width="96%" />
<p class="caption">
Figure 4.2: Forest scene
</p>
</div>
</div>
<div id="model-comparison" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Model Comparison<a href="variable-selection.html#model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Table collects the results of these fitted models. Recall that RMSE and MAE were calculated using 10-fold cross-validation. AIC and <span class="math inline">\(R^2_{adj}\)</span> were computed using a model fitted with all available data.</p>
<table>
<caption>Results for mean prediction models</caption>
<colgroup>
<col width="6%" />
<col width="7%" />
<col width="31%" />
<col width="5%" />
<col width="30%" />
<col width="3%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">RMSE</th>
<th align="center"><span class="math inline">\(\sigma_{\mathrm{RMSE}}\)</span></th>
<th align="center">MAE</th>
<th align="center"><span class="math inline">\(\sigma_{\mathrm{MAE}}\)</span></th>
<th align="center">AIC</th>
<th align="center"><span class="math inline">\(R^2_{adj}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">M1</td>
<td align="center">40.83</td>
<td align="center">7.28</td>
<td align="center">24.81</td>
<td align="center">4.01</td>
<td align="center">20254</td>
<td align="center">79.38%</td>
</tr>
<tr class="even">
<td align="center">M2</td>
<td align="center">57.87</td>
<td align="center">35.23</td>
<td align="center">25.28</td>
<td align="center">4.99</td>
<td align="center">19342</td>
<td align="center">87.90%</td>
</tr>
<tr class="odd">
<td align="center">M3</td>
<td align="center">34.95</td>
<td align="center">6.36</td>
<td align="center">22.34</td>
<td align="center">3.03</td>
<td align="center">18626</td>
<td align="center">86.70%</td>
</tr>
</tbody>
</table>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-Akaike1973" class="csl-entry">
Akaike, Htrotugu. 1973. <span>“Maximum Likelihood Identification of Gaussian Autoregressive Moving Average Models.”</span> <em>Biometrika</em> 60 (2): 255–65.
</div>
<div id="ref-berrendero2018" class="csl-entry">
Berrendero, José R, Antonio Cuevas, and Jos’e L Torrecilla. 2018. <span>“On the Use of Reproducing Kernel Hilbert Spaces in Func2tional Classification.”</span> <em>Journal of the American Statistical Association</em> 113: 1210–18.
</div>
<div id="ref-berrendero2016variable" class="csl-entry">
Berrendero, José R, Antonio Cuevas, and José L Torrecilla. 2016. <span>“Variable Selection in Functional Data Classification: A Maxima-Hunting Proposal.”</span> <em>Statistica Sinica</em>, 619–38.
</div>
<div id="ref-efron2004least" class="csl-entry">
Efron, Bradley, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. 2004. <span>“Least Angle Regression.”</span> <em>The Annals of Statistics</em> 32 (2): 407–99.
</div>
<div id="ref-Febrero-Bande2019" class="csl-entry">
Febrero-Bande, Manuel, Wenceslao González-Manteiga, and Manuel Oviedo de la Fuente. 2019. <span>“Variable Selection in Functional Additive Regression Models.”</span> <em>Computational Statistics</em> 34 (2): 469–87. <a href="https://doi.org/10.1007/s00180-018-0844-5">https://doi.org/10.1007/s00180-018-0844-5</a>.
</div>
<div id="ref-ferraty2010most" class="csl-entry">
Ferraty, Frédéric, P Hall, and Philippe Vieu. 2010. <span>“Most-Predictive Design Points for Functional Data Predictors.”</span> <em>Biometrika</em> 97 (4): 807–24.
</div>
<div id="ref-Ferraty2009" class="csl-entry">
Ferraty, F., and P. Vieu. 2009. <span>“Additive Prediction and Boosting for Functional Data.”</span> <em>Computational Statistics &amp; Data Analysis</em> 53 (4): 1400–1413. <a href="http://www.sciencedirect.com/science/article/pii/S0167947308005628">http://www.sciencedirect.com/science/article/pii/S0167947308005628</a>.
</div>
<div id="ref-garcia2014goodness" class="csl-entry">
Garcı́a-Portugués, Eduardo, Wenceslao González-Manteiga, and Manuel Febrero-Bande. 2014. <span>“A Goodness-of-Fit Test for the Functional Linear Model with Scalar Response.”</span> <em>Journal of Computational and Graphical Statistics</em> 23 (3): 761–78.
</div>
<div id="ref-lin2006component" class="csl-entry">
Lin, Yi, Hao Helen Zhang, et al. 2006. <span>“Component Selection and Smoothing in Multivariate Nonparametric Regression.”</span> <em>The Annals of Statistics</em> 34 (5): 2272–97.
</div>
<div id="ref-Ordonez2018" class="csl-entry">
Ordóñez, Celestino, Manuel Oviedo de la Fuente, Javier Roca-Pardiñas, and José Ramón Rodrı́guez-Pérez. 2018. <span>“Determining Optimum Wavelengths for Leaf Water Content Estimation from Reflectance: A Distance Correlation Approach.”</span> <em>Chemometrics and Intelligent Laboratory Systems</em> 173: 41–50.
</div>
<div id="ref-oviedo2021distance" class="csl-entry">
Oviedo-de la Fuente, Manuel, Carlos Cabo, Celestino Ordóñez, and Javier Roca-Pardiñas. 2021. <span>“A Distance Correlation Approach for Optimum Multiscale Selection in 3D Point Cloud Classification.”</span> <em>Mathematics</em> 9 (12): 1328.
</div>
<div id="ref-peng2005feature" class="csl-entry">
Peng, Hanchuan, Fuhui Long, and Chris Ding. 2005. <span>“Feature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy.”</span> <em>IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</em>, no. 8: 1226–38.
</div>
<div id="ref-Szekely2007" class="csl-entry">
Székely, G. J., M. L. Rizzo, and N. K. Bakirov. 2007. <span>“Measuring and Testing Dependence by Correlation of Distances.”</span> <em>The Annals of Statistics</em> 35 (6): 2769–94. <a href="http://projecteuclid.org/euclid.aos/1201012979">http://projecteuclid.org/euclid.aos/1201012979</a>.
</div>
<div id="ref-Yenigun2015" class="csl-entry">
Yenigün, C Deniz, and Maria L Rizzo. 2015. <span>“Variable Selection in Regression Using Maximal Correlation and Distance Correlation.”</span> <em>Journal of Statistical Computation and Simulation</em> 85 (8): 1692–1705.
</div>
<div id="ref-Zhao2015" class="csl-entry">
Zhao, Yihong, Huaihou Chen, and R Todd Ogden. 2015. <span>“Wavelet-Based Weighted LASSO and Screening Approaches in Functional Linear Regression.”</span> <em>Journal of Computational and Graphical Statistics</em> 24 (3): 655–75.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="functional-supervised-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="functional-clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/moviedo5/bookdown_fda_usc/edit/master/04-vs.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown_intro.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
